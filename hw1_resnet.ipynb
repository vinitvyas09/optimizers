{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de59520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f8e3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.0+cu126\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.83 GB\n",
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Environment Check\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15672e0b",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "Standard Training:  \n",
    "  Forward: x -> a1 -> a2 -> a3 -> loss  \n",
    "           [store] [store] [store]  \n",
    "  Backward: uses stored a1, a2, a3  \n",
    "  Memory: O(n) activations  \n",
    "  \n",
    "With Checkpointing:  \n",
    "  Forward: x -> a1 -> a2 -> a3 -> loss  \n",
    "           [save]       [save]  \n",
    "  Backward: recompute a2 from a1, then use  \n",
    "  Memory: O(sqrt(n)) with optimal placement  \n",
    "  \n",
    "For 100 layers:  \n",
    "  Standard: 100 activations stored  \n",
    "  Checkpointed: ~10 activations stored (at checkpoint boundaries)  \n",
    "  Memory reduction: 10x  \n",
    "  \n",
    "Compute analysis:  \n",
    "  Without checkpointing: n forward + n backward = 2n  \n",
    "  With checkpointing:    n forward + n backward + n recompute = 3n  \n",
    "  Compute overhead: ~50% of total training time  \n",
    "\n",
    "KEY: The sqrt(n) rule - optimal checkpointing reduces memory from O(n) to O(sqrt(n))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f16913",
   "metadata": {},
   "source": [
    "# Optimizations by torch.compile()\n",
    "\n",
    "How torch.compile handles activations:\n",
    "\n",
    "1. TRACING\n",
    "   torch.compile traces forward AND backward into one graph.\n",
    "   This lets it see the whole picture.\n",
    "\n",
    "2. MIN-CUT PARTITIONING\n",
    "   The graph is split at the optimal points.\n",
    "   Algorithm minimizes: tensors crossing the cut\n",
    "   (These are the tensors saved for backward)\n",
    "\n",
    "3. AUTOMATIC RECOMPUTATION\n",
    "   Cheap ops (relu, add, mul) are recomputed automatically.\n",
    "   No user intervention needed.\n",
    "\n",
    "4. FUSION\n",
    "   Pointwise ops get fused into kernels.\n",
    "   Fused ops are fast to recompute.\n",
    "\n",
    "So looks like torch.compile gives us SOME memory savings for FREE, plus speed improvements from fusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e1cec",
   "metadata": {},
   "source": [
    "# Resnet50 activation checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0f5256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 Structure:\n",
      "------------------------------------------------------------\n",
      "conv1:  1 conv layer\n",
      "layer1: 3 Bottleneck blocks (64 -> 256 channels)\n",
      "layer2: 4 Bottleneck blocks (128 -> 512 channels)\n",
      "layer3: 6 Bottleneck blocks (256 -> 1024 channels)\n",
      "layer4: 3 Bottleneck blocks (512 -> 2048 channels)\n",
      "fc:     1 linear layer\n",
      "\n",
      "Total Bottleneck blocks: 16\n",
      "\n",
      "Each Bottleneck block has 3 conv layers + skip connection.\n",
      "Feature maps grow larger in early layers, then shrink spatially.\n",
      "layer3 often has the largest activation memory (1024 channels, moderate spatial size).\n"
     ]
    }
   ],
   "source": [
    "# 5.1 ResNet50 Architecture Overview\n",
    "\n",
    "try:\n",
    "    import torchvision.models as models\n",
    "except Exception as e:\n",
    "    models = None\n",
    "    print(\"torchvision is required for the ResNet50 case study.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "if models is not None:\n",
    "    # Load ResNet50 without downloading weights\n",
    "    try:\n",
    "        resnet50 = models.resnet50(weights=None)\n",
    "    except TypeError:\n",
    "        # Older torchvision\n",
    "        resnet50 = models.resnet50(pretrained=False)\n",
    "\n",
    "    print(\"ResNet50 Structure:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"conv1:  1 conv layer\")\n",
    "    print(f\"layer1: {len(resnet50.layer1)} Bottleneck blocks (64 -> 256 channels)\")\n",
    "    print(f\"layer2: {len(resnet50.layer2)} Bottleneck blocks (128 -> 512 channels)\")\n",
    "    print(f\"layer3: {len(resnet50.layer3)} Bottleneck blocks (256 -> 1024 channels)\")\n",
    "    print(f\"layer4: {len(resnet50.layer4)} Bottleneck blocks (512 -> 2048 channels)\")\n",
    "    print(f\"fc:     1 linear layer\")\n",
    "    print()\n",
    "    print(\n",
    "        f\"Total Bottleneck blocks: {len(resnet50.layer1) + len(resnet50.layer2) + len(resnet50.layer3) + len(resnet50.layer4)}\"\n",
    "    )\n",
    "    print()\n",
    "    print(\"Each Bottleneck block has 3 conv layers + skip connection.\")\n",
    "    print(\"Feature maps grow larger in early layers, then shrink spatially.\")\n",
    "    print(\"layer3 often has the largest activation memory (1024 channels, moderate spatial size).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981e6531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50Checkpointed created with 4 strategies:\n",
      "------------------------------------------------------------\n",
      "'none':       No checkpointing (baseline)\n",
      "'per_stage':  Checkpoint each of the 4 stages\n",
      "'per_block':  Checkpoint each of the 16 bottleneck blocks\n",
      "'aggressive': Only checkpoint layer3 and layer4 (best ROI)\n"
     ]
    }
   ],
   "source": [
    "# 5.2 ResNet50 with Checkpointing: Three Strategies\n",
    "\n",
    "try:\n",
    "    import torchvision.models as models\n",
    "except Exception as e:\n",
    "    models = None\n",
    "    ResNet50Checkpointed = None\n",
    "    print(\"torchvision is required for ResNet50Checkpointed.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "if models is not None:\n",
    "    class ResNet50Checkpointed(nn.Module):\n",
    "        \"\"\"ResNet50 with configurable checkpointing strategies.\"\"\"\n",
    "\n",
    "        def __init__(self, num_classes=1000, checkpoint_strategy='none'):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                checkpoint_strategy: 'none', 'per_stage', 'per_block', or 'aggressive'\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "\n",
    "            # Load base ResNet50 without downloading weights\n",
    "            try:\n",
    "                base = models.resnet50(weights=None)\n",
    "            except TypeError:\n",
    "                base = models.resnet50(pretrained=False)\n",
    "\n",
    "            # Copy all layers\n",
    "            self.conv1 = base.conv1\n",
    "            self.bn1 = base.bn1\n",
    "            self.relu = base.relu\n",
    "            self.maxpool = base.maxpool\n",
    "            self.layer1 = base.layer1\n",
    "            self.layer2 = base.layer2\n",
    "            self.layer3 = base.layer3\n",
    "            self.layer4 = base.layer4\n",
    "            self.avgpool = base.avgpool\n",
    "            self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "            self.checkpoint_strategy = checkpoint_strategy\n",
    "\n",
    "        def _forward_stage(self, stage, x):\n",
    "            \"\"\"Forward through a stage (layer1, layer2, etc.).\"\"\"\n",
    "            for block in stage:\n",
    "                x = block(x)\n",
    "            return x\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Stem\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "            # NOTE: ResNet blocks include BatchNorm, which updates running stats in train mode.\n",
    "            # Checkpointing recomputes forward in backward, which can update BN stats twice.\n",
    "            # For strict parity, consider freezing BN stats (eval for BN) or using GroupNorm.\n",
    "\n",
    "            if self.checkpoint_strategy == 'none':\n",
    "                x = self.layer1(x)\n",
    "                x = self.layer2(x)\n",
    "                x = self.layer3(x)\n",
    "                x = self.layer4(x)\n",
    "\n",
    "            elif self.checkpoint_strategy == 'per_stage':\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer1, t), x, use_reentrant=False)\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer2, t), x, use_reentrant=False)\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer3, t), x, use_reentrant=False)\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer4, t), x, use_reentrant=False)\n",
    "\n",
    "            elif self.checkpoint_strategy == 'per_block':\n",
    "                for block in self.layer1:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer2:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer3:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer4:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "\n",
    "            elif self.checkpoint_strategy == 'aggressive':\n",
    "                x = self.layer1(x)\n",
    "                x = self.layer2(x)\n",
    "                for block in self.layer3:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer4:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown checkpoint_strategy: {self.checkpoint_strategy}\")\n",
    "\n",
    "            # Head\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "    print(\"ResNet50Checkpointed created with 4 strategies:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"'none':       No checkpointing (baseline)\")\n",
    "    print(\"'per_stage':  Checkpoint each of the 4 stages\")\n",
    "    print(\"'per_block':  Checkpoint each of the 16 bottleneck blocks\")\n",
    "    print(\"'aggressive': Only checkpoint layer3 and layer4 (best ROI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b8319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 Checkpointing Comparison\n",
      "Batch size: 32, Image size: 224x224\n",
      "----------------------------------------------------------------------\n",
      "Strategy        Peak Memory (MB)     Time (ms)       Mem Savings    \n",
      "----------------------------------------------------------------------\n",
      "none            3033.27              276.69             0.0%\n",
      "aggressive      2627.26              316.66            13.4%\n",
      "per_stage       1765.28              368.39            41.8%\n",
      "per_block       1360.01              368.90            55.2%\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 5.3 Memory Comparison: ResNet50 Strategies\n",
    "\n",
    "def benchmark_resnet50(strategy, batch_size=32, image_size=224, num_iters=5):\n",
    "    \"\"\"Benchmark memory and time for a ResNet50 checkpointing strategy.\"\"\"\n",
    "    if ResNet50Checkpointed is None:\n",
    "        raise RuntimeError(\"ResNet50Checkpointed is not available (torchvision import likely failed).\")\n",
    "\n",
    "    model = ResNet50Checkpointed(num_classes=1000, checkpoint_strategy=strategy).cuda()\n",
    "    model.train()\n",
    "\n",
    "    x = torch.randn(batch_size, 3, image_size, image_size, device=\"cuda\")\n",
    "    target = torch.randint(0, 1000, (batch_size,), device=\"cuda\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(2):\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Measure memory + time\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(num_iters):\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed = (time.time() - start) / num_iters * 1000  # ms/iter\n",
    "    peak_mem = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
    "\n",
    "    del model, x, target\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return peak_mem, elapsed\n",
    "\n",
    "if torch.cuda.is_available() and ResNet50Checkpointed is not None:\n",
    "    batch_size = 32\n",
    "    image_size = 224\n",
    "\n",
    "    print(\"ResNet50 Checkpointing Comparison\")\n",
    "    print(f\"Batch size: {batch_size}, Image size: {image_size}x{image_size}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Strategy':<15} {'Peak Memory (MB)':<20} {'Time (ms)':<15} {'Mem Savings':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    strategies = ['none', 'aggressive', 'per_stage', 'per_block']\n",
    "    results = {}\n",
    "\n",
    "    for strategy in strategies:\n",
    "        mem, time_ms = benchmark_resnet50(strategy, batch_size, image_size)\n",
    "        results[strategy] = (mem, time_ms)\n",
    "\n",
    "    baseline_mem = results['none'][0]\n",
    "    for strategy in strategies:\n",
    "        mem, time_ms = results[strategy]\n",
    "        savings = (1 - mem / baseline_mem) * 100\n",
    "        print(f\"{strategy:<15} {mem:<20.2f} {time_ms:<15.2f} {savings:>6.1f}%\")\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "else:\n",
    "    print(\"[Run on GPU and ensure torchvision is installed for ResNet50 benchmarks]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c74bb5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding maximum batch size for ResNet50 (224x224 images)\n",
      "--------------------------------------------------\n",
      "none           : max batch = 177 (1.00x)\n",
      "aggressive     : max batch = 205 (1.16x)\n",
      "per_block      : max batch = 256 (1.45x)\n",
      "--------------------------------------------------\n",
      "\n",
      "With checkpointing, you can often fit larger batches.\n"
     ]
    }
   ],
   "source": [
    "# 5.4 Maximum Batch Size: ResNet50\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def find_max_batch_resnet50(strategy, start=16, max_batch=256):\n",
    "    \"\"\"Find maximum batch size before OOM for ResNet50 (CUDA only).\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is required for find_max_batch_resnet50().\")\n",
    "    if ResNet50Checkpointed is None:\n",
    "        raise RuntimeError(\"ResNet50Checkpointed is not available (torchvision import likely failed).\")\n",
    "\n",
    "    start = max(1, int(start))\n",
    "    max_batch = max(start, int(max_batch))\n",
    "\n",
    "    def can_run(batch_size):\n",
    "        model = None\n",
    "        x = None\n",
    "        target = None\n",
    "        out = None\n",
    "        loss = None\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            model = ResNet50Checkpointed(checkpoint_strategy=strategy).cuda()\n",
    "            model.train()\n",
    "            x = torch.randn(batch_size, 3, 224, 224, device=\"cuda\")\n",
    "            target = torch.randint(0, 1000, (batch_size,), device=\"cuda\")\n",
    "\n",
    "            out = model(x)\n",
    "            loss = nn.CrossEntropyLoss()(out, target)\n",
    "            loss.backward()\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                return False\n",
    "            raise\n",
    "        finally:\n",
    "            try:\n",
    "                if model is not None:\n",
    "                    model.zero_grad(set_to_none=True)\n",
    "            except Exception:\n",
    "                pass\n",
    "            del model, x, target, out, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Exponential search to find an upper bound, then binary search.\n",
    "    max_working = 0\n",
    "    batch = start\n",
    "    while batch <= max_batch and can_run(batch):\n",
    "        max_working = batch\n",
    "        batch *= 2\n",
    "\n",
    "    low = max_working + 1\n",
    "    high = min(batch, max_batch)\n",
    "\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        if can_run(mid):\n",
    "            max_working = mid\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "\n",
    "    return max_working\n",
    "\n",
    "if torch.cuda.is_available() and ResNet50Checkpointed is not None:\n",
    "    print(\"Finding maximum batch size for ResNet50 (224x224 images)\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    strategies = ['none', 'aggressive', 'per_block']\n",
    "    baseline = None\n",
    "\n",
    "    for strategy in strategies:\n",
    "        max_batch = find_max_batch_resnet50(strategy)\n",
    "        if baseline is None:\n",
    "            baseline = max_batch\n",
    "        improvement = (max_batch / baseline) if baseline else float('inf')\n",
    "        print(f\"{strategy:<15}: max batch = {max_batch:>3d} ({improvement:.2f}x)\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nWith checkpointing, you can often fit larger batches.\")\n",
    "else:\n",
    "    print(\"[Run on GPU and ensure torchvision is installed to find max batch sizes]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d156542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1592307309.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if (use_amp and torch.cuda.is_available()) else None\n",
      "/tmp/ipython-input-1592307309.py:55: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet50 with 'aggressive' checkpointing\n",
      "Batch size: 32, Mixed precision: True\n",
      "--------------------------------------------------\n",
      "Epoch 1/3: loss = 2.6300, peak memory = 1630.90 MB\n",
      "Epoch 2/3: loss = 1.9535, peak memory = 1630.90 MB\n",
      "Epoch 3/3: loss = 1.5271, peak memory = 1630.90 MB\n",
      "--------------------------------------------------\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# 5.5 Complete Training Loop: ResNet50 with Checkpointing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_resnet50_with_checkpointing(\n",
    "    checkpoint_strategy='aggressive',\n",
    "    batch_size=32,\n",
    "    num_epochs=3,\n",
    "    num_samples=256,  # Small for demo\n",
    "    use_amp=True,\n",
    "):\n",
    "    \"\"\"Complete training loop with checkpointing and mixed precision.\"\"\"\n",
    "\n",
    "    if ResNet50Checkpointed is None:\n",
    "        raise RuntimeError(\"ResNet50Checkpointed is not available (torchvision import likely failed).\")\n",
    "\n",
    "    # Create model\n",
    "    model = ResNet50Checkpointed(\n",
    "        num_classes=10,  # Simplified for demo\n",
    "        checkpoint_strategy=checkpoint_strategy,\n",
    "    ).cuda()\n",
    "\n",
    "    # Create synthetic dataset\n",
    "    X = torch.randn(num_samples, 3, 224, 224)\n",
    "    y = torch.randint(0, 10, (num_samples,))\n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler() if (use_amp and torch.cuda.is_available()) else None\n",
    "\n",
    "    print(f\"Training ResNet50 with '{checkpoint_strategy}' checkpointing\")\n",
    "    print(f\"Batch size: {batch_size}, Mixed precision: {use_amp and scaler is not None}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.cuda()\n",
    "            batch_y = batch_y.cuda()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            if scaler is not None:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(batch_x)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: loss = {avg_loss:.4f}, peak memory = {peak_mem:.2f} MB\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "if torch.cuda.is_available() and ResNet50Checkpointed is not None:\n",
    "    model = train_resnet50_with_checkpointing(\n",
    "        checkpoint_strategy='aggressive',\n",
    "        batch_size=32,\n",
    "        num_epochs=3,\n",
    "        use_amp=True,\n",
    "    )\n",
    "else:\n",
    "    print(\"[Run on GPU and ensure torchvision is installed for training demo]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
