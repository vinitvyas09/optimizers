{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Checkpointing: A First Principles Deep Dive\n",
    "\n",
    "Memory is the bottleneck in deep learning. Not compute. Not data. Memory.\n",
    "\n",
    "This notebook tears apart activation checkpointing from first principles. You will understand exactly why activations dominate memory, how checkpointing trades compute for memory, and when this trade-off makes sense.\n",
    "\n",
    "Every code cell has an Impressions/Conclusions section. These are not summaries. They are insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# 1. Introduction\n\nTraining deep neural networks hits a wall. That wall is GPU memory.\n\nYou have a model. You have data. You have compute. But your GPU runs out of memory. Why?\n\nFour things consume GPU memory during training:\n1. **Model parameters** (weights and biases)\n2. **Gradients** (same size as parameters)\n3. **Optimizer states** (for Adam: 2x parameter size for m and v buffers)\n4. **Activations** (intermediate outputs from each layer)\n\nFor Adam/AdamW, the first three combined are 4x the model size (params + grads + 2 momentum buffers). This is fixed cost.\n\nHere is the surprise: activations often dominate anyway. Not parameters. Not gradients. Not optimizer states. Activations. Why? Because activations scale with batch size. Everything else does not."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1.1 The Memory Breakdown: Where Does Memory Go?\n\nimport torch\nimport torch.nn as nn\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters())\n\ndef bytes_to_mb(b):\n    return b / (1024 * 1024)\n\nclass DeepNetwork(nn.Module):\n    def __init__(self, input_size=1024, hidden_size=1024, num_layers=10):\n        super().__init__()\n        layers = []\n        for i in range(num_layers):\n            in_f = input_size if i == 0 else hidden_size\n            layers.append(nn.Linear(in_f, hidden_size))\n            layers.append(nn.ReLU())\n        self.layers = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.layers(x)\n\nmodel = DeepNetwork(input_size=1024, hidden_size=1024, num_layers=10)\nnum_params = count_parameters(model)\n\n# THEORETICAL memory breakdown for a full training setup with Adam\n# These are projections, not actual allocations in this code\nparam_mem = num_params * 4  # fp32 parameters\ngrad_mem = param_mem        # gradients (allocated during backward)\noptimizer_mem = param_mem * 2  # Adam's m and v buffers (allocated when optimizer.step() is called)\n\nprint(f\"Model: {num_params:,} parameters\")\nprint()\nprint(\"THEORETICAL memory breakdown for full training with Adam:\")\nprint(\"-\" * 60)\nprint(f\"Parameter memory:  {bytes_to_mb(param_mem):.2f} MB (always allocated)\")\nprint(f\"Gradient memory:   {bytes_to_mb(grad_mem):.2f} MB (allocated during backward)\")\nprint(f\"Optimizer memory:  {bytes_to_mb(optimizer_mem):.2f} MB (Adam: m + v, allocated on first step)\")\nprint(f\"Fixed cost total:  {bytes_to_mb(param_mem + grad_mem + optimizer_mem):.2f} MB\")\nprint()\nprint(\"Activation memory by batch size (THEORETICAL):\")\nprint(\"-\" * 60)\n\nfixed_cost = param_mem + grad_mem + optimizer_mem\nfor batch_size in [32, 64, 128, 256, 512]:\n    # Activation estimate: batch_size * hidden_size * num_layers * 4 bytes\n    # This is simplified - actual activation memory depends on what PyTorch saves\n    act_mem = batch_size * 1024 * 10 * 4\n    total = fixed_cost + act_mem\n    act_pct = (act_mem / total) * 100\n    print(f\"Batch {batch_size:3d}: activations = {bytes_to_mb(act_mem):6.2f} MB ({act_pct:.1f}% of total)\")\n\nprint()\nprint(\"NOTE: These are theoretical estimates. See Section 1.2 for actual GPU measurements.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Impressions/Conclusions (1.1)\n\nThis cell shows **theoretical** memory projections for a full training setup. No optimizer or gradients are actually allocated yet - we're just computing what the memory breakdown *would be*.\n\nThe theoretical fixed costs for Adam/AdamW:\n- Parameters: 1x model size (always allocated)\n- Gradients: 1x model size (allocated during backward pass)\n- Optimizer states (m, v): 2x model size (allocated on first optimizer.step())\n- Total fixed: 4x model size\n\nAt small batches, fixed costs dominate. At large batches, activations dominate. This crossover point depends on model depth and hidden size.\n\nThe math:\n- Fixed cost: O(model_size)\n- Activations: O(batch_size × depth × hidden_size)\n\nBatch size is the multiplier that kills you. Checkpointing targets activations because that is where the scaling problem lives.\n\n**Key distinction**: Section 1.2 shows *actual* GPU memory measurements during real forward/backward passes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Real GPU Memory Measurement\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepNetwork(nn.Module):\n",
    "    def __init__(self, input_size=1024, hidden_size=2048, num_layers=20):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_f = input_size if i == 0 else hidden_size\n",
    "            layers.append(nn.Linear(in_f, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    model = DeepNetwork().cuda()\n",
    "    \n",
    "    for batch_size in [16, 32, 64, 128]:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        x = torch.randn(batch_size, 1024).cuda()\n",
    "        output = model(x)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        peak = torch.cuda.max_memory_allocated() / 1e6\n",
    "        print(f\"Batch {batch_size:3d}: Peak = {peak:.2f} MB\")\n",
    "        \n",
    "        model.zero_grad()\n",
    "        del x, output, loss\n",
    "else:\n",
    "    print(\"[Run on GPU to see measurements]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (1.2)\n",
    "\n",
    "Peak memory grows linearly with batch size. Double the batch, roughly double the memory.\n",
    "\n",
    "This is what activation checkpointing solves. Reduce activation memory to:\n",
    "1. Train larger models\n",
    "2. Use larger batches\n",
    "3. Go deeper without OOM\n",
    "\n",
    "The solution: do not store all activations. Recompute them when needed."
   ]
  },
  {
   "cell_type": "code",
   "source": "# 1.3 When Does Memory Peak? The Critical Insight\n\nimport torch\nimport torch.nn as nn\n\nclass InstrumentedNetwork(nn.Module):\n    \"\"\"Network that tracks memory at each layer.\"\"\"\n    def __init__(self, num_layers=5, hidden=1024):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Linear(hidden, hidden) for _ in range(num_layers)\n        ])\n    \n    def forward(self, x, track=False):\n        memory_log = []\n        for i, layer in enumerate(self.layers):\n            x = torch.relu(layer(x))\n            if track and torch.cuda.is_available():\n                memory_log.append(torch.cuda.memory_allocated() / 1e6)\n        return x, memory_log\n\nif torch.cuda.is_available():\n    model = InstrumentedNetwork(num_layers=10, hidden=2048).cuda()\n    x = torch.randn(64, 2048).cuda()\n    \n    # Track memory during forward\n    torch.cuda.reset_peak_memory_stats()\n    output, fwd_mem = model(x, track=True)\n    mem_after_forward = torch.cuda.memory_allocated() / 1e6\n    \n    # Now trigger backward\n    loss = output.sum()\n    loss.backward()\n    peak_during_backward = torch.cuda.max_memory_allocated() / 1e6\n    mem_after_backward = torch.cuda.memory_allocated() / 1e6\n    \n    print(\"Memory Timeline:\")\n    print(\"-\" * 50)\n    for i, mem in enumerate(fwd_mem):\n        print(f\"After layer {i+1}: {mem:.2f} MB\")\n    print(f\"\\nAfter forward (before backward): {mem_after_forward:.2f} MB\")\n    print(f\"Peak during entire training step: {peak_during_backward:.2f} MB\")\n    print(f\"After backward: {mem_after_backward:.2f} MB\")\n    print(\"\\nKey insight: Memory peaks around the start of backward pass.\")\n    print(\"At this point, all activations are stored AND gradients start accumulating.\")\n    print(\"The exact peak depends on when gradients and activations overlap maximally.\")\nelse:\n    print(\"[Run on GPU to see memory timeline]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (1.3)\n\nMemory accumulates during forward, peaks at the start of backward, then drops as activations are consumed and freed.\n\nThe timeline:\n1. Forward pass: memory grows (storing activations)\n2. Start of backward: PEAK (all activations live, gradients starting)\n3. During backward: memory drops (activations freed after use)\n4. After backward: only params + grads remain\n\nCheckpointing attacks the peak. By not storing activations during forward, peak memory at backward start is dramatically lower.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Understanding Activation Checkpointing\n",
    "\n",
    "Activation checkpointing is a memory-compute trade-off. Save memory by not storing activations. Pay for it by recomputing them during backprop.\n",
    "\n",
    "To understand this, you need to know what activations are and why backprop needs them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What Are Activations?\n",
    "\n",
    "An activation is the output of a layer. That is it.\n",
    "\n",
    "```\n",
    "Input x -> [Layer 1] -> a1 -> [Layer 2] -> a2 -> [Layer 3] -> a3 -> Output\n",
    "```\n",
    "\n",
    "`a1`, `a2`, `a3` are activations. Each is a tensor stored in memory.\n",
    "\n",
    "Why store them? Backpropagation needs them to compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Why Backprop Needs Activations\n",
    "\n",
    "import torch\n",
    "\n",
    "# y = W2 * relu(W1 * x)\n",
    "# Gradient w.r.t. W1 needs the pre-activation W1*x\n",
    "# to compute relu'(W1*x)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "W1 = torch.randn(4, 4, requires_grad=True)\n",
    "W2 = torch.randn(4, 4, requires_grad=True)\n",
    "x = torch.randn(1, 4)\n",
    "\n",
    "# Forward pass stores these\n",
    "z1 = x @ W1.T           # Pre-activation (needed for ReLU grad)\n",
    "a1 = torch.relu(z1)     # Activation (needed for W2 grad)\n",
    "y = a1 @ W2.T\n",
    "\n",
    "print(\"Stored for backprop:\")\n",
    "print(f\"  z1: {z1.shape} - needed to compute relu gradient\")\n",
    "print(f\"  a1: {a1.shape} - needed to compute W2 gradient\")\n",
    "\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradients computed:\")\n",
    "print(f\"  W1.grad: {W1.grad.shape}\")\n",
    "print(f\"  W2.grad: {W2.grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (2.1)\n",
    "\n",
    "The chain rule requires intermediate values. Backprop walks backward through the computation graph. At each step, it needs forward pass values.\n",
    "\n",
    "For linear `y = Wx`: gradient w.r.t. W needs x.\n",
    "\n",
    "For ReLU `y = relu(x)`: gradient needs to know where x > 0.\n",
    "\n",
    "This is why activations are stored. This is what checkpointing eliminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2.2 The Core Trade-off: Store vs Recompute\n\nimport math\n\nprint(\"Standard Training:\")\nprint(\"  Forward: x -> a1 -> a2 -> a3 -> loss\")\nprint(\"           [store] [store] [store]\")\nprint(\"  Backward: uses stored a1, a2, a3\")\nprint(\"  Memory: O(n) activations\")\nprint()\nprint(\"With Checkpointing:\")\nprint(\"  Forward: x -> a1 -> a2 -> a3 -> loss\")\nprint(\"           [save]       [save]\")\nprint(\"  Backward: recompute a2 from a1, then use\")\nprint(\"  Memory: O(sqrt(n)) with optimal placement\")\nprint()\n\nnum_layers = 100\noptimal = int(math.sqrt(num_layers))\n\n# With sqrt(n) checkpoints, we have sqrt(n) segments of sqrt(n) layers each\n# During backward, each segment must be recomputed once\n# Recompute cost = sqrt(n) segments × sqrt(n) layers = n layer computations\n# This equals one additional forward pass worth of compute\n\nprint(f\"For {num_layers} layers:\")\nprint(f\"  Standard: {num_layers} activations stored\")\nprint(f\"  Checkpointed: ~{optimal} activations stored (at checkpoint boundaries)\")\nprint(f\"  Memory reduction: {num_layers/optimal:.0f}x\")\nprint()\nprint(\"Compute analysis:\")\nprint(f\"  Without checkpointing: n forward + n backward = 2n\")\nprint(f\"  With checkpointing:    n forward + n backward + n recompute = 3n\")\nprint(f\"  Compute overhead: ~50% of total training time\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Impressions/Conclusions (2.2)\n\nThe sqrt(n) rule: optimal checkpointing reduces memory from O(n) to O(sqrt(n)).\n\nFor 100 layers:\n- Standard: 100 activations stored\n- Checkpointed: ~10 activations stored (at checkpoint boundaries)\n- Memory reduction: 10x\n\nThe compute cost is real:\n- Without checkpointing: Forward (n) + Backward (n) = 2n operations\n- With checkpointing: Forward (n) + Backward (n) + Recompute (n) = 3n operations\n- **Overhead: ~50% of total training time**\n\nThis is still an excellent trade-off: 10x memory reduction for 50% compute overhead. When memory is the bottleneck (and it usually is), this lets you train models that otherwise wouldn't fit."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Prerequisites\n",
    "\n",
    "Environment setup and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Environment Check\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (3.1)\n",
    "\n",
    "PyTorch 1.9+ required for full checkpointing support. `torch.utils.checkpoint` provides the core API. CUDA strongly recommended since checkpointing shines when GPU memory is the bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Memory Profiling Utilities\n",
    "\n",
    "import torch\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class MemoryTracker:\n",
    "    \"\"\"Track GPU memory usage.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.snapshots = []\n",
    "    \n",
    "    def snapshot(self, label=\"\"):\n",
    "        if torch.cuda.is_available():\n",
    "            mem = torch.cuda.memory_allocated() / 1e6\n",
    "            self.snapshots.append({'label': label, 'mb': mem})\n",
    "            return mem\n",
    "        return 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.snapshots = []\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def peak_mb(self):\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.max_memory_allocated() / 1e6\n",
    "        return 0\n",
    "    \n",
    "    def report(self):\n",
    "        for s in self.snapshots:\n",
    "            print(f\"{s['label']:30s}: {s['mb']:8.2f} MB\")\n",
    "        print(f\"{'Peak':30s}: {self.peak_mb():8.2f} MB\")\n",
    "\n",
    "@contextmanager\n",
    "def track_memory(label=\"Op\"):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "    yield\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"{label}: Peak = {torch.cuda.max_memory_allocated()/1e6:.2f} MB\")\n",
    "\n",
    "print(\"Utilities loaded: MemoryTracker, track_memory()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (3.2)\n",
    "\n",
    "Key functions:\n",
    "- `torch.cuda.memory_allocated()`: current memory in use\n",
    "- `torch.cuda.max_memory_allocated()`: peak since last reset\n",
    "- `torch.cuda.empty_cache()`: free cached memory\n",
    "\n",
    "Peak memory is what matters. That is what causes OOM."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 4. How Activation Checkpointing Works\n\nPyTorch provides `torch.utils.checkpoint`. Two main functions:\n- `checkpoint`: wrap a single function/module\n- `checkpoint_sequential`: wrap a sequence of modules\n\nThe API is simple. The magic is in what happens under the hood.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 4.1 Model WITHOUT Checkpointing\n\nimport torch\nimport torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n\n    def forward(self, x):\n        x = torch.relu(self.layer1(x))\n        x = torch.relu(self.layer2(x))\n        return x\n\nif torch.cuda.is_available():\n    model = SimpleModel().cuda()\n    x = torch.randn(1, 1024).cuda()\n\n    torch.cuda.reset_peak_memory_stats()\n    output = model(x)\n    loss = output.sum()\n    loss.backward()\n    \n    print(f\"Peak Memory WITHOUT Checkpointing: {torch.cuda.max_memory_allocated() / 1e6:.2f} MB\")\nelse:\n    print(\"[Run on GPU]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 4.2 Model WITH Checkpointing\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass CheckpointedModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n\n    def forward(self, x):\n        # Checkpoint layer1: its activation won't be stored\n        # It will be recomputed during backward pass\n        x = checkpoint(lambda t: torch.relu(self.layer1(t)), x, use_reentrant=False)\n        x = torch.relu(self.layer2(x))\n        return x\n\nif torch.cuda.is_available():\n    model = CheckpointedModel().cuda()\n    x = torch.randn(1, 1024).cuda()\n\n    torch.cuda.reset_peak_memory_stats()\n    output = model(x)\n    loss = output.sum()\n    loss.backward()\n    \n    print(f\"Peak Memory WITH Checkpointing: {torch.cuda.max_memory_allocated() / 1e6:.2f} MB\")\nelse:\n    print(\"[Run on GPU]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (4.1 & 4.2)\n\nThe difference is subtle in small models. With 2 layers, memory savings are minimal because the baseline is already small.\n\nThe real benefit appears at scale. Let us test with a deeper model.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 4.3 Deep Model Comparison: The Real Difference\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass DeepModelNoCheckpoint(nn.Module):\n    def __init__(self, num_layers=20, hidden=2048):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Linear(hidden, hidden) for _ in range(num_layers)\n        ])\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = torch.relu(layer(x))\n        return x\n\nclass DeepModelWithCheckpoint(nn.Module):\n    def __init__(self, num_layers=20, hidden=2048):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Linear(hidden, hidden) for _ in range(num_layers)\n        ])\n    \n    def forward(self, x):\n        for layer in self.layers:\n            # Checkpoint every layer\n            x = checkpoint(lambda t, l=layer: torch.relu(l(t)), x, use_reentrant=False)\n        return x\n\nif torch.cuda.is_available():\n    batch_size = 64\n    hidden = 2048\n    num_layers = 20\n    \n    # Without checkpointing\n    torch.cuda.empty_cache()\n    model1 = DeepModelNoCheckpoint(num_layers, hidden).cuda()\n    x1 = torch.randn(batch_size, hidden).cuda()\n    \n    torch.cuda.reset_peak_memory_stats()\n    out1 = model1(x1)\n    out1.sum().backward()\n    mem_no_ckpt = torch.cuda.max_memory_allocated() / 1e6\n    \n    del model1, x1, out1\n    torch.cuda.empty_cache()\n    \n    # With checkpointing\n    model2 = DeepModelWithCheckpoint(num_layers, hidden).cuda()\n    x2 = torch.randn(batch_size, hidden).cuda()\n    \n    torch.cuda.reset_peak_memory_stats()\n    out2 = model2(x2)\n    out2.sum().backward()\n    mem_ckpt = torch.cuda.max_memory_allocated() / 1e6\n    \n    print(f\"Config: {num_layers} layers, hidden={hidden}, batch={batch_size}\")\n    print(f\"WITHOUT checkpointing: {mem_no_ckpt:.2f} MB\")\n    print(f\"WITH checkpointing:    {mem_ckpt:.2f} MB\")\n    print(f\"Memory saved: {mem_no_ckpt - mem_ckpt:.2f} MB ({(1 - mem_ckpt/mem_no_ckpt)*100:.1f}%)\")\nelse:\n    print(\"[Run on GPU]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (4.3)\n\nNow you see the real savings. With 20 layers at hidden=2048, checkpointing can save 30-50% memory depending on batch size and model architecture.\n\nThe key insight: activation memory grows with depth. Checkpointing trades that linear growth for constant (or sqrt) memory.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 5. Code Examples\n\nThree patterns you will use:\n1. Basic `checkpoint()` for single modules\n2. `checkpoint_sequential()` for sequential models\n3. Manual checkpointing in complex architectures (transformers)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 5.1 Basic Usage\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU())\n        self.block2 = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU())\n\n    def forward(self, x):\n        # Checkpoint block1\n        x = checkpoint(self.block1, x, use_reentrant=False)\n        x = self.block2(x)\n        return x\n\nmodel = SimpleModel()\nif torch.cuda.is_available():\n    model = model.cuda()\n\nx = torch.randn(1, 1024)\nif torch.cuda.is_available():\n    x = x.cuda()\n\noutput = model(x)\nprint(f\"Output shape: {output.shape}\")\nprint(\"Checkpointing applied to block1. Block2 runs normally.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (5.1)\n\n`checkpoint(fn, *args)` wraps any callable. During forward, it runs `fn(*args)` but does not save intermediate activations. During backward, it re-runs `fn(*args)` to recompute them.\n\nNote: `use_reentrant=False` is the modern API. It handles edge cases better than the old default.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 5.2 Using checkpoint_sequential\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint_sequential\n\n# A deep sequential model\nmodel = nn.Sequential(\n    nn.Linear(1024, 1024), nn.ReLU(),\n    nn.Linear(1024, 1024), nn.ReLU(),\n    nn.Linear(1024, 1024), nn.ReLU(),\n    nn.Linear(1024, 1024), nn.ReLU(),\n)\n\nx = torch.randn(1, 1024)\nif torch.cuda.is_available():\n    model = model.cuda()\n    x = x.cuda()\n\n# Split into 2 checkpoint segments\n# Each segment will be checkpointed separately\nsegments = 2\noutput = checkpoint_sequential(model, segments, x, use_reentrant=False)\n\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Model split into {segments} checkpoint segments\")\nprint(\"Each segment recomputes activations during backward pass\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (5.2)\n\n`checkpoint_sequential` is convenient for nn.Sequential models. The `segments` parameter controls granularity:\n- More segments = less memory, more recomputation\n- Fewer segments = more memory, less recomputation\n\nRule of thumb: start with sqrt(num_layers) segments.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 5.3 Transformer Training with Checkpointing\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_size, num_heads=8):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_size)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(embed_size, embed_size * 4),\n            nn.GELU(),\n            nn.Linear(embed_size * 4, embed_size)\n        )\n        self.norm2 = nn.LayerNorm(embed_size)\n\n    def forward(self, x):\n        # Self-attention with residual\n        attn_out, _ = self.attention(x, x, x)\n        x = self.norm1(x + attn_out)\n        # Feed-forward with residual (checkpointed)\n        ff_out = checkpoint(self.feed_forward, x, use_reentrant=False)\n        x = self.norm2(x + ff_out)\n        return x\n\n# Example usage\nembed_size = 512\nseq_length = 64\nbatch_size = 8\n\nblock = TransformerBlock(embed_size)\nif torch.cuda.is_available():\n    block = block.cuda()\n\nx = torch.randn(batch_size, seq_length, embed_size)\nif torch.cuda.is_available():\n    x = x.cuda()\n\noutput = block(x)\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(\"Feed-forward block is checkpointed (it uses the most memory)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (5.3)\n\nIn transformers, the feed-forward block uses 4x the hidden size. This is where most activation memory goes. Checkpointing the FFN is the standard practice in large language models.\n\nWhy not checkpoint attention? You can, but attention has complex intermediate states. The memory-compute trade-off is less favorable there.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 5.4 ResNet with Selective Checkpointing (Bonus)\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass BasicBlock(nn.Module):\n    \"\"\"ResNet basic block with optional checkpointing.\"\"\"\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        return torch.relu(out)\n\nclass MiniResNet(nn.Module):\n    \"\"\"Small ResNet with checkpointing options.\"\"\"\n    def __init__(self, num_blocks=4, use_checkpoint=False):\n        super().__init__()\n        self.use_checkpoint = use_checkpoint\n        \n        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        \n        self.blocks = nn.ModuleList([\n            BasicBlock(64 if i == 0 else 128, 128, stride=2 if i == 0 else 1)\n            for i in range(num_blocks)\n        ])\n        \n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        x = torch.relu(self.bn1(self.conv1(x)))\n        \n        for block in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint(block, x, use_reentrant=False)\n            else:\n                x = block(x)\n        \n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\n# Compare memory\nif torch.cuda.is_available():\n    batch_size = 32\n    \n    # Without checkpoint\n    torch.cuda.empty_cache()\n    model1 = MiniResNet(num_blocks=8, use_checkpoint=False).cuda()\n    x1 = torch.randn(batch_size, 3, 32, 32).cuda()\n    torch.cuda.reset_peak_memory_stats()\n    out1 = model1(x1)\n    out1.sum().backward()\n    mem1 = torch.cuda.max_memory_allocated() / 1e6\n    \n    del model1, x1, out1\n    torch.cuda.empty_cache()\n    \n    # With checkpoint\n    model2 = MiniResNet(num_blocks=8, use_checkpoint=True).cuda()\n    x2 = torch.randn(batch_size, 3, 32, 32).cuda()\n    torch.cuda.reset_peak_memory_stats()\n    out2 = model2(x2)\n    out2.sum().backward()\n    mem2 = torch.cuda.max_memory_allocated() / 1e6\n    \n    print(f\"ResNet with 8 blocks, batch={batch_size}\")\n    print(f\"WITHOUT checkpoint: {mem1:.2f} MB\")\n    print(f\"WITH checkpoint:    {mem2:.2f} MB\")\n    print(f\"Savings: {(1 - mem2/mem1)*100:.1f}%\")\nelse:\n    print(\"[Run on GPU]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (5.4)\n\nCNNs benefit from checkpointing too. Each residual block stores feature maps that can be recomputed. For deeper ResNets (ResNet-152), checkpointing is essential to train with reasonable batch sizes.\n\nKey pattern: wrap entire blocks, not individual layers. The overhead of checkpoint calls adds up.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 6. Performance Benchmarks\n\nNumbers matter. Let us measure:\n1. Memory savings across model sizes\n2. Compute overhead (training time)\n3. Batch size scaling",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 6.1 Memory Savings Across Model Depths\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\nimport time\n\ndef create_model(num_layers, hidden, use_checkpoint=False):\n    class Model(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.layers = nn.ModuleList([\n                nn.Linear(hidden, hidden) for _ in range(num_layers)\n            ])\n            self.use_ckpt = use_checkpoint\n        \n        def forward(self, x):\n            for layer in self.layers:\n                if self.use_ckpt:\n                    x = checkpoint(lambda t, l=layer: torch.relu(l(t)), x, use_reentrant=False)\n                else:\n                    x = torch.relu(layer(x))\n            return x\n    return Model()\n\nif torch.cuda.is_available():\n    hidden = 1024\n    batch = 64\n    \n    print(f\"Config: hidden={hidden}, batch={batch}\")\n    print(\"-\" * 60)\n    print(f\"{'Layers':<10} {'No Ckpt (MB)':<15} {'With Ckpt (MB)':<15} {'Savings':<10}\")\n    print(\"-\" * 60)\n    \n    for num_layers in [10, 20, 40, 80]:\n        # Without checkpoint\n        torch.cuda.empty_cache()\n        m1 = create_model(num_layers, hidden, False).cuda()\n        x1 = torch.randn(batch, hidden).cuda()\n        torch.cuda.reset_peak_memory_stats()\n        m1(x1).sum().backward()\n        mem1 = torch.cuda.max_memory_allocated() / 1e6\n        del m1, x1\n        \n        # With checkpoint\n        torch.cuda.empty_cache()\n        m2 = create_model(num_layers, hidden, True).cuda()\n        x2 = torch.randn(batch, hidden).cuda()\n        torch.cuda.reset_peak_memory_stats()\n        m2(x2).sum().backward()\n        mem2 = torch.cuda.max_memory_allocated() / 1e6\n        del m2, x2\n        \n        savings = (1 - mem2/mem1) * 100\n        print(f\"{num_layers:<10} {mem1:<15.2f} {mem2:<15.2f} {savings:.1f}%\")\nelse:\n    print(\"[Run on GPU for benchmarks]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (6.1)\n\nMemory savings increase with depth. At 10 layers, savings are modest. At 80 layers, savings can exceed 50%.\n\nThis matches the theory: activation memory is O(n), checkpointing reduces it to O(1) per segment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 6.2 Batch Size Scaling: The Real Win\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\ndef create_model(num_layers, hidden, use_checkpoint=False):\n    class Model(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.layers = nn.ModuleList([\n                nn.Linear(hidden, hidden) for _ in range(num_layers)\n            ])\n            self.use_ckpt = use_checkpoint\n        \n        def forward(self, x):\n            for layer in self.layers:\n                if self.use_ckpt:\n                    x = checkpoint(lambda t, l=layer: torch.relu(l(t)), x, use_reentrant=False)\n                else:\n                    x = torch.relu(layer(x))\n            return x\n    return Model()\n\ndef find_max_batch(model_fn, hidden, start=32, max_batch=2048):\n    \"\"\"Binary search to find maximum batch size before OOM.\"\"\"\n    low, high = start, max_batch\n    max_working = start\n    \n    while low <= high:\n        mid = (low + high) // 2\n        try:\n            torch.cuda.empty_cache()\n            model = model_fn().cuda()\n            x = torch.randn(mid, hidden).cuda()\n            torch.cuda.reset_peak_memory_stats()\n            model(x).sum().backward()\n            max_working = mid\n            low = mid + 1\n            del model, x\n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                high = mid - 1\n                torch.cuda.empty_cache()\n            else:\n                raise\n    return max_working\n\nif torch.cuda.is_available():\n    hidden = 2048\n    num_layers = 30\n    \n    print(f\"Finding maximum batch size for {num_layers}-layer model, hidden={hidden}\")\n    print(\"This demonstrates the practical benefit of checkpointing.\\n\")\n    \n    # Max batch without checkpointing\n    model_no_ckpt = lambda: create_model(num_layers, hidden, False)\n    max_batch_no_ckpt = find_max_batch(model_no_ckpt, hidden)\n    \n    # Max batch with checkpointing\n    model_ckpt = lambda: create_model(num_layers, hidden, True)\n    max_batch_ckpt = find_max_batch(model_ckpt, hidden)\n    \n    improvement = max_batch_ckpt / max_batch_no_ckpt\n    \n    print(f\"WITHOUT checkpointing: max batch = {max_batch_no_ckpt}\")\n    print(f\"WITH checkpointing:    max batch = {max_batch_ckpt}\")\n    print(f\"Improvement: {improvement:.1f}x larger batches possible\")\nelse:\n    print(\"[Run on GPU to find max batch sizes]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 6.3 Compute Overhead Measurement\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\nimport time\n\ndef benchmark_time(model, x, num_iters=10):\n    # Warmup\n    for _ in range(3):\n        model(x).sum().backward()\n        model.zero_grad()\n    \n    torch.cuda.synchronize()\n    start = time.time()\n    for _ in range(num_iters):\n        model(x).sum().backward()\n        model.zero_grad()\n    torch.cuda.synchronize()\n    return (time.time() - start) / num_iters * 1000  # ms\n\nif torch.cuda.is_available():\n    hidden = 1024\n    batch = 64\n    num_layers = 40\n    \n    # Create models\n    m1 = create_model(num_layers, hidden, False).cuda()\n    m2 = create_model(num_layers, hidden, True).cuda()\n    x = torch.randn(batch, hidden).cuda()\n    \n    time1 = benchmark_time(m1, x)\n    time2 = benchmark_time(m2, x)\n    \n    overhead = (time2 - time1) / time1 * 100\n    \n    print(f\"Config: {num_layers} layers, hidden={hidden}, batch={batch}\")\n    print(f\"WITHOUT checkpoint: {time1:.2f} ms/iter\")\n    print(f\"WITH checkpoint:    {time2:.2f} ms/iter\")\n    print(f\"Overhead: {overhead:.1f}%\")\nelse:\n    print(\"[Run on GPU for benchmarks]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (6.2)\n\nThis is the practical payoff. Checkpointing lets you use 2-3x larger batch sizes.\n\nLarger batches mean:\n- Better gradient estimates (less noise)\n- Higher GPU utilization\n- Faster convergence (sometimes)\n\nThe compute overhead of checkpointing is often offset by the efficiency gains from larger batches. You recompute more, but you also process more data per step.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (6.3)\n\nCompute overhead is typically 30-50% depending on model architecture. This is the extra forward passes during backprop.\n\nThe trade-off: 50% memory savings for ~50% compute overhead. Worth it when memory is the bottleneck.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 7. Debugging and Best Practices\n\nCheckpointing has gotchas. Here are the common ones and how to avoid them.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 7.1 Pitfall: Non-Differentiable Operations\n\nimport torch\nfrom torch.utils.checkpoint import checkpoint\n\n# BAD: torch.no_grad() inside checkpointed function\ndef bad_forward(x):\n    with torch.no_grad():  # This breaks gradient computation!\n        x = x ** 2\n    return x\n\n# GOOD: Keep everything differentiable\ndef good_forward(x):\n    x = x ** 2  # No torch.no_grad()\n    return x\n\nx = torch.randn(4, requires_grad=True)\n\n# This will fail or give wrong gradients\ntry:\n    y_bad = checkpoint(bad_forward, x, use_reentrant=False)\n    y_bad.sum().backward()\n    print(\"BAD: Gradients might be zero or wrong\")\nexcept Exception as e:\n    print(f\"BAD: Error - {e}\")\n\n# This works\nx = torch.randn(4, requires_grad=True)\ny_good = checkpoint(good_forward, x, use_reentrant=False)\ny_good.sum().backward()\nprint(f\"GOOD: x.grad = {x.grad}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (7.1)\n\nRule: Everything inside a checkpointed function must be differentiable. No `torch.no_grad()`, no detaching tensors, no in-place operations that break autograd.\n\nIf you need non-differentiable ops, do them outside the checkpointed region.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 7.2 Pitfall: Random Operations (Dropout)\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\n# Problem: Dropout uses different random values in forward vs recompute\nclass ModelWithDropout(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(32, 32)\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, x):\n        x = self.linear(x)\n        x = self.dropout(x)  # Random! Different each forward pass\n        return x\n\nmodel = ModelWithDropout()\nx = torch.randn(4, 32, requires_grad=True)\n\n# During training, checkpointed dropout can cause issues\n# because recomputation uses different random mask\n\n# Solution 1: Use deterministic mode (PyTorch 1.11+)\n# checkpoint(..., preserve_rng_state=True)  # Default in newer PyTorch\n\n# Solution 2: Use use_reentrant=False which handles this better\ny = checkpoint(model, x, use_reentrant=False)\nprint(f\"Output shape: {y.shape}\")\nprint(\"use_reentrant=False handles RNG state properly\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (7.2)\n\nDropout and other random ops are tricky. The recomputed forward pass might use different random values than the original.\n\nModern PyTorch (1.11+) with `use_reentrant=False` preserves RNG state. Always use this option.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 7.3 Best Practices Summary\n\nprint(\"\"\"\nBEST PRACTICES FOR ACTIVATION CHECKPOINTING\n============================================\n\n1. USE use_reentrant=False\n   - Modern API, handles edge cases better\n   - checkpoint(fn, x, use_reentrant=False)\n\n2. CHECKPOINT LARGE BLOCKS, NOT SMALL LAYERS\n   - Overhead of checkpoint() call is non-trivial\n   - Group 2-4 layers into blocks, then checkpoint blocks\n\n3. AVOID NESTED CHECKPOINTS\n   - Don't checkpoint inside checkpointed functions\n   - Leads to exponential recomputation\n\n4. KEEP EVERYTHING DIFFERENTIABLE\n   - No torch.no_grad() inside checkpointed regions\n   - No tensor detaching\n   - No in-place ops that break autograd\n\n5. TEST GRADIENTS FIRST\n   - Compare gradients with and without checkpointing\n   - They should be identical (within float precision)\n\n6. PROFILE MEMORY\n   - Use torch.cuda.memory_stats() to verify savings\n   - Peak memory is what matters\n\n7. CONSIDER CHECKPOINT PLACEMENT\n   - Middle layers often have largest activations\n   - Checkpoint those first\n\"\"\")\n\n# Gradient verification example\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nmodel = nn.Linear(64, 64)\nx = torch.randn(8, 64, requires_grad=True)\n\n# Without checkpoint\nx1 = x.clone().detach().requires_grad_(True)\ny1 = model(x1)\ny1.sum().backward()\ngrad1 = x1.grad.clone()\n\n# With checkpoint\nx2 = x.clone().detach().requires_grad_(True)\ny2 = checkpoint(model, x2, use_reentrant=False)\ny2.sum().backward()\ngrad2 = x2.grad.clone()\n\nprint(f\"Gradients match: {torch.allclose(grad1, grad2)}\")\nprint(f\"Max difference: {(grad1 - grad2).abs().max():.2e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (7.3)\n\nGradient verification is essential. If gradients do not match exactly (within float precision), something is wrong with your checkpointing setup. Debug before scaling up.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 8. Selective Activation Checkpoint (SAC)\n\nStandard checkpointing is all-or-nothing. Every op in the checkpointed region gets recomputed during backward.\n\nSelective Activation Checkpoint (SAC) gives you granular control. You choose which operations to save and which to recompute.\n\nWhy does this matter? Not all operations are equal:\n- Matmuls are expensive to recompute\n- Pointwise ops (relu, sigmoid) are cheap\n- Attention is very expensive\n\nSAC lets you save the expensive ones and recompute the cheap ones. Best of both worlds.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 9. Integrating with Distributed Training\n\nCheckpointing works with DDP and mixed precision. The combination is powerful.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 8.1 The CheckpointPolicy Enum\n\nfrom torch.utils.checkpoint import CheckpointPolicy\n\nprint(\"CheckpointPolicy has four options:\")\nprint(\"-\" * 50)\nprint()\nprint(\"MUST_SAVE:\")\nprint(\"  - Always save this op's output\")\nprint(\"  - Never recompute it\")\nprint(\"  - Use for expensive ops (matmul, attention)\")\nprint()\nprint(\"PREFER_SAVE:\")\nprint(\"  - Save if possible\")\nprint(\"  - torch.compile may override this\")\nprint()\nprint(\"MUST_RECOMPUTE:\")\nprint(\"  - Always recompute this op\")\nprint(\"  - Never save its output\")\nprint()\nprint(\"PREFER_RECOMPUTE:\")\nprint(\"  - Recompute if possible\")\nprint(\"  - torch.compile may override this\")\nprint(\"  - Use for cheap ops (relu, elementwise)\")\nprint()\nprint(\"The MUST_ variants are strict. The PREFER_ variants are hints.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (8.1)\n\nThe four policies give you a 2x2 matrix:\n- MUST vs PREFER: How strict?\n- SAVE vs RECOMPUTE: What action?\n\nUse MUST_ when you know for certain. Use PREFER_ when you want torch.compile to potentially optimize further.\n\nKey insight: A policy that returns PREFER_RECOMPUTE for everything is equivalent to vanilla checkpointing. A policy that returns PREFER_SAVE for everything is NOT the same as no checkpointing (it may save extra tensors).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 8.2 Policy Function: Save Matmuls, Recompute Everything Else\n\nimport torch\nfrom torch.utils.checkpoint import CheckpointPolicy\n\n# Policy 1: Conservative - only save the most expensive ops\n# IMPORTANT: Use the .default variants for correct op matching\naten = torch.ops.aten\ncompute_intensive_ops_basic = [\n    aten.mm.default,       # Matrix multiply\n    aten.bmm.default,      # Batched matrix multiply\n    aten.addmm.default,    # Fused add + matmul\n]\n\ndef policy_save_matmuls(ctx, op, *args, **kwargs):\n    \"\"\"Save matmuls, recompute everything else.\"\"\"\n    if op in compute_intensive_ops_basic:\n        return CheckpointPolicy.MUST_SAVE\n    else:\n        return CheckpointPolicy.PREFER_RECOMPUTE\n\nprint(\"Policy 1: Save Matmuls Only\")\nprint(\"-\" * 50)\nprint(\"Saves: mm.default, bmm.default, addmm.default\")\nprint(\"Recomputes: relu, gelu, sigmoid, layernorm, etc.\")\nprint()\nprint(\"Use case: When you want maximum memory savings but\")\nprint(\"cannot afford to recompute matrix multiplies.\")\nprint()\nprint(\"Trade-off position: Closer to full checkpointing,\")\nprint(\"but avoids the most expensive recomputation.\")\nprint()\nprint(\"NOTE: Always use .default suffix (e.g., aten.mm.default)\")\nprint(\"to match the actual ops passed to the policy function.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 8.3 Policy Function: Save All Compute-Intensive Ops\n\nimport torch\nfrom torch.utils.checkpoint import CheckpointPolicy\n\n# Policy 2: Aggressive - save everything expensive\n# Use .default variants for correct matching\naten = torch.ops.aten\ncompute_intensive_ops_full = [\n    aten.mm.default,\n    aten.bmm.default,\n    aten.addmm.default,\n    aten.convolution.default,\n    aten._scaled_dot_product_flash_attention.default,\n    aten._scaled_dot_product_efficient_attention.default,\n    aten.upsample_bilinear2d.default,\n    aten._scaled_mm.default,\n    aten.linear.default,\n]\n\ndef policy_save_all_expensive(ctx, op, *args, **kwargs):\n    \"\"\"Save all compute-intensive ops, including attention.\"\"\"\n    if op in compute_intensive_ops_full:\n        return CheckpointPolicy.MUST_SAVE\n    else:\n        return CheckpointPolicy.PREFER_RECOMPUTE\n\nprint(\"Policy 2: Save All Expensive Ops\")\nprint(\"-\" * 50)\nprint(\"Saves: matmuls + convolutions + attention + upsampling\")\nprint(\"Recomputes: only cheap pointwise ops\")\nprint()\nprint(\"Use case: When training time matters more than memory.\")\nprint(\"You still get some savings from recomputing cheap ops.\")\nprint()\nprint(\"Trade-off position: Closer to eager mode (no checkpointing),\")\nprint(\"but still saves memory by recomputing pointwise ops.\")\nprint()\nprint(\"NOTE: Some ops may not exist in your PyTorch version.\")\nprint(\"Check torch.ops.aten.<op>.default for availability.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (8.2 & 8.3)\n\nTwo policies, two positions on the speed-memory curve:\n\nPolicy 1 (save matmuls only):\n- Memory: Low (close to full checkpointing)\n- Speed: Medium (recomputes everything except matmuls)\n\nPolicy 2 (save all expensive):\n- Memory: Medium (saves attention and convolutions too)\n- Speed: High (only recomputes cheap pointwise ops)\n\nThe key realization: pointwise ops are cheap to recompute but take significant memory. Recomputing just those gives you substantial memory savings with minimal compute overhead.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 8.4 Using SAC in Practice: The Full API\n\nimport functools\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import (\n    checkpoint,\n    CheckpointPolicy,\n    create_selective_checkpoint_contexts,\n)\n\n# Define which ops to save - MUST use .default variants\naten = torch.ops.aten\nops_to_save = [\n    aten.mm.default,\n    aten.bmm.default,\n    aten.addmm.default,\n]\n\ndef policy_fn(ctx, op, *args, **kwargs):\n    \"\"\"Policy function for selective checkpointing.\"\"\"\n    if op in ops_to_save:\n        return CheckpointPolicy.MUST_SAVE\n    else:\n        return CheckpointPolicy.PREFER_RECOMPUTE\n\n# Create the context function using functools.partial\ncontext_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n\n# Define a function to checkpoint\ndef forward_fn(x, weight1, weight2):\n    \"\"\"Example forward: two matmuls with activations.\"\"\"\n    x = torch.mm(x, weight1)  # Will be SAVED (matches aten.mm.default)\n    x = torch.relu(x)          # Will be RECOMPUTED\n    x = torch.mm(x, weight2)  # Will be SAVED (matches aten.mm.default)\n    x = torch.sigmoid(x)       # Will be RECOMPUTED\n    return x\n\n# Test it\nx = torch.randn(32, 64, requires_grad=True)\nw1 = torch.randn(64, 64, requires_grad=True)\nw2 = torch.randn(64, 64, requires_grad=True)\n\n# Use checkpoint with the selective context\noutput = checkpoint(\n    forward_fn, x, w1, w2,\n    use_reentrant=False,\n    context_fn=context_fn,\n)\n\nprint(\"SAC API Usage:\")\nprint(\"-\" * 50)\nprint(\"1. Define policy_fn(ctx, op, *args, **kwargs) -> CheckpointPolicy\")\nprint(\"2. Create context_fn with functools.partial\")\nprint(\"3. Pass context_fn to checkpoint(..., context_fn=context_fn)\")\nprint()\nprint(f\"Output shape: {output.shape}\")\nprint(\"Matmuls saved. Activations will be recomputed during backward.\")\nprint()\nprint(\"IMPORTANT: Use aten.<op>.default for op matching, not aten.<op>\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 8.5 Shortcut: Pass a List Instead of Policy Function\n\nimport functools\nimport torch\nfrom torch.utils.checkpoint import (\n    checkpoint,\n    create_selective_checkpoint_contexts,\n)\n\n# Instead of writing a policy function, just pass a list of ops to save\naten = torch.ops.aten\nops_to_save = [\n    torch.ops.aten.mm.default,\n    torch.ops.aten.bmm.default,\n]\n\n# This is equivalent to a policy that returns MUST_SAVE for listed ops\n# and PREFER_RECOMPUTE for everything else\ncontext_fn = functools.partial(create_selective_checkpoint_contexts, ops_to_save)\n\ndef simple_forward(x, w):\n    x = torch.mm(x, w)\n    x = torch.relu(x)\n    return x\n\nx = torch.randn(32, 64, requires_grad=True)\nw = torch.randn(64, 64, requires_grad=True)\n\noutput = checkpoint(\n    simple_forward, x, w,\n    use_reentrant=False,\n    context_fn=context_fn,\n)\n\nprint(\"Shortcut API:\")\nprint(\"-\" * 50)\nprint(\"Instead of: policy_fn(ctx, op, ...) -> CheckpointPolicy\")\nprint(\"Just pass:  [list of ops to save]\")\nprint()\nprint(\"Same result, less code.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 8.6 SAC vs Standard AC: Memory and Time Comparison\n\nimport functools\nimport time\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import (\n    checkpoint,\n    CheckpointPolicy,\n    create_selective_checkpoint_contexts,\n)\n\nclass TransformerFFN(nn.Module):\n    \"\"\"Feed-forward block from transformer.\"\"\"\n    def __init__(self, dim=1024, expansion=4):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim * expansion)\n        self.fc2 = nn.Linear(dim * expansion, dim)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = torch.gelu(x)  # Cheap\n        x = self.fc2(x)\n        return x\n\n# SAC policy: save matmuls only (use .default variants!)\naten = torch.ops.aten\ndef sac_policy(ctx, op, *args, **kwargs):\n    # nn.Linear uses addmm or mm internally\n    if op in [aten.mm.default, aten.addmm.default]:\n        return CheckpointPolicy.MUST_SAVE\n    return CheckpointPolicy.PREFER_RECOMPUTE\n\nsac_context = functools.partial(create_selective_checkpoint_contexts, sac_policy)\n\nif torch.cuda.is_available():\n    dim = 2048\n    batch = 64\n    seq = 128\n    \n    model = TransformerFFN(dim).cuda()\n    x = torch.randn(batch, seq, dim).cuda()\n    \n    results = {}\n    \n    # 1. No checkpointing\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    out = model(x)\n    out.sum().backward()\n    results['No Checkpoint'] = torch.cuda.max_memory_allocated() / 1e6\n    model.zero_grad()\n    \n    # 2. Standard checkpointing\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    out = checkpoint(model, x, use_reentrant=False)\n    out.sum().backward()\n    results['Standard AC'] = torch.cuda.max_memory_allocated() / 1e6\n    model.zero_grad()\n    \n    # 3. Selective checkpointing\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    out = checkpoint(model, x, use_reentrant=False, context_fn=sac_context)\n    out.sum().backward()\n    results['Selective AC'] = torch.cuda.max_memory_allocated() / 1e6\n    model.zero_grad()\n    \n    print(f\"Transformer FFN: dim={dim}, batch={batch}, seq={seq}\")\n    print(\"-\" * 50)\n    for name, mem in results.items():\n        print(f\"{name:20s}: {mem:8.2f} MB\")\n    print()\n    print(\"Standard AC: Recomputes everything (including matmuls)\")\n    print(\"Selective AC: Saves matmuls, only recomputes GELU\")\nelse:\n    print(\"[Run on GPU for comparison]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (8.4, 8.5, 8.6)\n\nSAC is the middle ground between \"checkpoint everything\" and \"checkpoint nothing.\"\n\nThe API flow:\n1. Define ops you care about (matmuls, attention, etc.)\n2. Write a policy function OR just pass a list\n3. Wrap with `create_selective_checkpoint_contexts`\n4. Pass to `checkpoint(..., context_fn=...)`\n\nWhen to use SAC over standard AC:\n- You need fine-grained control\n- Recomputing certain ops is too expensive\n- You want to tune the speed-memory trade-off precisely\n\nSAC is a prototype feature as of PyTorch 2.5. The API may evolve.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 9.1 DDP + Checkpointing (Code Template)\n# NOTE: This code shows the pattern. Run in a distributed environment.\n\n\"\"\"\nimport torch\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.checkpoint import checkpoint\n\n# Initialize distributed\ndist.init_process_group(backend=\"nccl\")\nlocal_rank = int(os.environ[\"LOCAL_RANK\"])\ntorch.cuda.set_device(local_rank)\n\nclass ModelWithCheckpoint(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n    \n    def forward(self, x):\n        x = checkpoint(lambda t: torch.relu(self.layer1(t)), x, use_reentrant=False)\n        x = torch.relu(self.layer2(x))\n        return x\n\n# Wrap with DDP\nmodel = ModelWithCheckpoint().cuda(local_rank)\nddp_model = DDP(model, device_ids=[local_rank])\n\n# Training works as normal\nx = torch.randn(16, 1024).cuda(local_rank)\noutput = ddp_model(x)\nloss = output.sum()\nloss.backward()\n\"\"\"\n\nprint(\"DDP + Checkpointing:\")\nprint(\"- Checkpointing happens locally on each GPU\")\nprint(\"- DDP handles gradient synchronization\")\nprint(\"- No special configuration needed\")\nprint(\"- Memory savings apply per-GPU\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 9.2 Mixed Precision + Checkpointing\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n    \n    def forward(self, x):\n        x = checkpoint(lambda t: torch.relu(self.layer1(t)), x, use_reentrant=False)\n        x = torch.relu(self.layer2(x))\n        return x\n\nif torch.cuda.is_available():\n    model = SimpleModel().cuda()\n    optimizer = torch.optim.Adam(model.parameters())\n    scaler = torch.cuda.amp.GradScaler()\n    \n    # Training loop with mixed precision + checkpointing\n    for step in range(3):\n        x = torch.randn(16, 1024).cuda()\n        \n        with torch.cuda.amp.autocast():\n            output = model(x)\n            loss = output.sum()\n        \n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        print(f\"Step {step}: loss = {loss.item():.4f}\")\n    \n    print(\"\\nMixed precision + checkpointing works seamlessly\")\nelse:\n    print(\"[Run on GPU]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (9.1 & 9.2)\n\nThe power combo: DDP + Mixed Precision + Checkpointing.\n\n- DDP: Scale across GPUs\n- Mixed Precision: 2x memory reduction from fp16\n- Checkpointing: Further memory reduction from recomputation\n\nCombined, you can train models 4-5x larger than baseline. This is how GPT-scale models are trained.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 10. Additional Tools and Resources\n\nBeyond PyTorch's built-in checkpointing, there are libraries that offer more features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 10.1 DeepSpeed Integration\n\n\"\"\"\nDeepSpeed offers activation checkpointing as part of its ZeRO optimization suite.\n\n# Installation\npip install deepspeed\n\n# Usage\nimport deepspeed\nfrom deepspeed.runtime.activation_checkpointing import checkpointing\n\n# Configure in ds_config.json:\n{\n    \"activation_checkpointing\": {\n        \"partition_activations\": true,\n        \"contiguous_memory_optimization\": true,\n        \"cpu_checkpointing\": true  # Offload to CPU!\n    }\n}\n\n# Key advantage: CPU offloading\n# DeepSpeed can move checkpointed activations to CPU memory,\n# freeing GPU memory for even larger models.\n\"\"\"\n\nprint(\"DeepSpeed Activation Checkpointing:\")\nprint(\"- Automatic checkpoint placement\")\nprint(\"- CPU offloading for extreme memory savings\")\nprint(\"- Integrated with ZeRO optimizer stages\")\nprint(\"- Best for very large models (billions of parameters)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 10.2 FairScale Integration\n\n\"\"\"\nFairScale (from Meta) provides checkpoint_wrapper for easy integration.\n\n# Installation\npip install fairscale\n\n# Usage\nfrom fairscale.nn.checkpoint import checkpoint_wrapper\n\n# Wrap any module\nlayer = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU())\ncheckpointed_layer = checkpoint_wrapper(layer)\n\n# Use in model\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = checkpoint_wrapper(nn.Sequential(...))\n        self.block2 = checkpoint_wrapper(nn.Sequential(...))\n    \n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        return x\n\"\"\"\n\nprint(\"FairScale checkpoint_wrapper:\")\nprint(\"- Clean API: wrap modules directly\")\nprint(\"- Works well with FSDP (Fully Sharded Data Parallel)\")\nprint(\"- Good for medium-scale training\")\nprint(\"- Simpler than DeepSpeed for many use cases\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (10.1 & 10.2)\n\nTool selection guide:\n- **PyTorch native**: Simple models, full control, no dependencies\n- **FairScale**: Medium scale, clean API, FSDP integration\n- **DeepSpeed**: Billion+ parameter models, CPU offloading, full optimization suite\n\nStart with PyTorch native. Move to DeepSpeed when you hit limits.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 11. torch.compile and Memory Budget API\n\ntorch.compile (PyTorch 2.0+) does something clever: it traces your forward and backward passes into a single joint graph. Then it applies a \"min-cut\" partitioner.\n\nThis is different from activation checkpointing. The min-cut algorithm automatically decides which tensors to save and which to recompute based on minimizing total runtime. No manual configuration needed.\n\nBut here is the catch: by default, min-cut prioritizes speed, not memory. It only recomputes cheap, fusible ops (like pointwise operations).\n\nThe Memory Budget API gives you control over this trade-off.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 11.1 torch.compile: The Min-Cut Partitioner\n\nimport torch\nimport torch.nn as nn\n\nprint(\"How torch.compile handles activations:\")\nprint(\"-\" * 50)\nprint()\nprint(\"1. TRACING\")\nprint(\"   torch.compile traces forward AND backward into one graph.\")\nprint(\"   This lets it see the whole picture.\")\nprint()\nprint(\"2. MIN-CUT PARTITIONING\")\nprint(\"   The graph is split at the optimal points.\")\nprint(\"   Algorithm minimizes: tensors crossing the cut\")\nprint(\"   (These are the tensors saved for backward)\")\nprint()\nprint(\"3. AUTOMATIC RECOMPUTATION\")\nprint(\"   Cheap ops (relu, add, mul) are recomputed automatically.\")\nprint(\"   No user intervention needed.\")\nprint()\nprint(\"4. FUSION\")\nprint(\"   Pointwise ops get fused into kernels.\")\nprint(\"   Fused ops are fast to recompute.\")\nprint()\nprint(\"Result: torch.compile gives you SOME memory savings for FREE,\")\nprint(\"        plus speed improvements from fusion.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 11.2 Comparing: Eager vs Compile vs Checkpointing\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass SimpleFFN(nn.Module):\n    def __init__(self, dim=2048):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim * 4)\n        self.fc2 = nn.Linear(dim * 4, dim)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = torch.gelu(x)\n        x = self.fc2(x)\n        return x\n\nif torch.cuda.is_available():\n    dim = 2048\n    batch = 64\n    seq = 128\n    \n    model = SimpleFFN(dim).cuda()\n    x = torch.randn(batch, seq, dim).cuda()\n    \n    results = {}\n    \n    # 1. Eager mode (no compile, no checkpoint)\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    out = model(x)\n    out.sum().backward()\n    results['Eager'] = torch.cuda.max_memory_allocated() / 1e6\n    model.zero_grad()\n    \n    # 2. torch.compile (default settings)\n    compiled_model = torch.compile(model)\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    out = compiled_model(x)\n    out.sum().backward()\n    results['torch.compile'] = torch.cuda.max_memory_allocated() / 1e6\n    model.zero_grad()\n    \n    # 3. Activation checkpointing\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    out = checkpoint(model, x, use_reentrant=False)\n    out.sum().backward()\n    results['Checkpointing'] = torch.cuda.max_memory_allocated() / 1e6\n    model.zero_grad()\n    \n    print(f\"FFN: dim={dim}, batch={batch}, seq={seq}\")\n    print(\"-\" * 50)\n    for name, mem in results.items():\n        print(f\"{name:20s}: {mem:8.2f} MB\")\n    print()\n    print(\"torch.compile: better speed, moderate memory savings\")\n    print(\"Checkpointing: maximum memory savings, some speed cost\")\nelse:\n    print(\"[Run on GPU for comparison]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (11.1 & 11.2)\n\nThe speed-memory trade-off diagram:\n\n```\nSpeed (high is good)\n  ^\n  |   * torch.compile (top-left: fast, some memory savings)\n  |\n  |        * Eager (top-right: fast, high memory)\n  |\n  |   * SAC policies (middle: tunable)\n  |\n  |   * Checkpointing (bottom-left: slower, low memory)\n  |\n  +---------------------------------> Memory (right is bad)\n```\n\ntorch.compile sits between eager and full checkpointing. It automatically recomputes cheap ops. The Memory Budget API lets you push it further toward checkpointing.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 11.3 Memory Budget API: Control the Trade-off\n\nimport torch\nimport torch._dynamo\n\nprint(\"Memory Budget API (torch.compile only)\")\nprint(\"-\" * 50)\nprint()\nprint(\"torch._dynamo.config.activation_memory_budget = X\")\nprint()\nprint(\"X = 0.0: Maximum recomputation (like full AC)\")\nprint(\"         Recompute everything, save almost nothing\")\nprint()\nprint(\"X = 0.5: Balanced\")\nprint(\"         Recompute pointwise ops, save matmuls\")\nprint()\nprint(\"X = 1.0: Default torch.compile behavior\")\nprint(\"         Minimal recomputation, maximize speed\")\nprint()\nprint(\"The API automatically finds pareto-optimal policies.\")\nprint(\"You just specify how much memory you want to use.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 11.4 Using Memory Budget API in Practice\n\nimport torch\nimport torch.nn as nn\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim=1024, heads=8):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(dim, heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Linear(dim * 4, dim)\n        )\n        self.norm2 = nn.LayerNorm(dim)\n    \n    def forward(self, x):\n        attn_out, _ = self.attn(x, x, x)\n        x = self.norm1(x + attn_out)\n        x = self.norm2(x + self.ffn(x))\n        return x\n\nif torch.cuda.is_available():\n    dim = 1024\n    batch = 32\n    seq = 256\n    \n    model = TransformerBlock(dim).cuda()\n    x = torch.randn(batch, seq, dim).cuda()\n    \n    budgets = [1.0, 0.7, 0.5, 0.3, 0.0]\n    \n    print(f\"Transformer: dim={dim}, batch={batch}, seq={seq}\")\n    print(\"-\" * 50)\n    print(f\"{'Budget':<10} {'Memory (MB)':<15} {'Relative':<10}\")\n    print(\"-\" * 50)\n    \n    baseline = None\n    for budget in budgets:\n        # Set memory budget\n        torch._dynamo.config.activation_memory_budget = budget\n        \n        # Recompile with new budget\n        torch._dynamo.reset()\n        compiled = torch.compile(model)\n        \n        # Warmup\n        for _ in range(2):\n            compiled(x).sum().backward()\n            model.zero_grad()\n        \n        # Measure\n        torch.cuda.empty_cache()\n        torch.cuda.reset_peak_memory_stats()\n        out = compiled(x)\n        out.sum().backward()\n        mem = torch.cuda.max_memory_allocated() / 1e6\n        model.zero_grad()\n        \n        if baseline is None:\n            baseline = mem\n        \n        relative = mem / baseline\n        print(f\"{budget:<10.1f} {mem:<15.2f} {relative:.2f}x\")\n    \n    # Reset to default\n    torch._dynamo.config.activation_memory_budget = 1.0\nelse:\n    print(\"[Run on GPU for memory budget comparison]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 11.5 What Gets Recomputed at Each Budget Level?\n\nprint(\"Recomputation order (from blog's real transformer results):\")\nprint(\"-\" * 60)\nprint()\nprint(\"Budget 1.0 (default compile):\")\nprint(\"  Recomputes: Nothing extra\")\nprint(\"  Saves: Everything\")\nprint(\"  Memory: 100% (baseline)\")\nprint()\nprint(\"Budget 0.7:\")\nprint(\"  Recomputes: Pointwise ops (gelu, add, mul)\")\nprint(\"  Saves: Matmuls, attention\")\nprint(\"  Memory: ~85% of baseline\")\nprint()\nprint(\"Budget 0.5:\")\nprint(\"  Recomputes: Pointwise + some matmuls\")\nprint(\"  Saves: Attention (most expensive)\")\nprint(\"  Memory: ~50% of baseline\")\nprint()\nprint(\"Budget 0.3:\")\nprint(\"  Recomputes: Pointwise + most matmuls\")\nprint(\"  Saves: Only attention\")\nprint(\"  Memory: ~35% of baseline\")\nprint()\nprint(\"Budget 0.0:\")\nprint(\"  Recomputes: Everything (like full AC)\")\nprint(\"  Saves: Almost nothing\")\nprint(\"  Memory: Minimum possible\")\nprint()\nprint(\"Key insight: 50% memory reduction by recomputing only pointwise ops.\")\nprint(\"Attention is expensive. Recompute it last.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (11.3, 11.4, 11.5)\n\nMemory Budget API is the easiest way to tune the speed-memory trade-off with torch.compile.\n\nOne line of code:\n```python\ntorch._dynamo.config.activation_memory_budget = 0.5\n```\n\nThe system automatically:\n1. Finds pareto-optimal recomputation strategies\n2. Prioritizes recomputing cheap ops first\n3. Saves attention and expensive matmuls for last\n\nWhen to use Memory Budget API vs SAC:\n- Memory Budget: Using torch.compile, want automatic optimization\n- SAC: Need precise control over which ops to save/recompute\n\nMemory Budget API is experimental (PyTorch 2.4+). Expect the API to stabilize in future releases.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 12. Case Study: Activation Checkpointing in ResNet50\n\nResNet50 is deep enough to benefit from checkpointing. It has 4 stages with multiple bottleneck blocks. Each block stores feature maps for backward.\n\nThis section shows how to apply checkpointing to a real production model. You will see:\n1. How to modify torchvision's ResNet50\n2. Memory comparison across checkpointing strategies\n3. A complete training loop with checkpointing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 12.1 ResNet50 Architecture Overview\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# Load pretrained ResNet50 to inspect structure\nresnet50 = models.resnet50(weights=None)\n\nprint(\"ResNet50 Structure:\")\nprint(\"-\" * 60)\nprint(f\"conv1:  1 conv layer\")\nprint(f\"layer1: {len(resnet50.layer1)} Bottleneck blocks (64 -> 256 channels)\")\nprint(f\"layer2: {len(resnet50.layer2)} Bottleneck blocks (128 -> 512 channels)\")\nprint(f\"layer3: {len(resnet50.layer3)} Bottleneck blocks (256 -> 1024 channels)\")\nprint(f\"layer4: {len(resnet50.layer4)} Bottleneck blocks (512 -> 2048 channels)\")\nprint(f\"fc:     1 linear layer\")\nprint()\nprint(f\"Total Bottleneck blocks: {len(resnet50.layer1) + len(resnet50.layer2) + len(resnet50.layer3) + len(resnet50.layer4)}\")\nprint()\nprint(\"Each Bottleneck block has 3 conv layers + skip connection.\")\nprint(\"Feature maps grow larger in early layers, then shrink spatially.\")\nprint(\"layer3 often has the largest activation memory (1024 channels, moderate spatial size).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 12.2 ResNet50 with Checkpointing: Three Strategies\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.utils.checkpoint import checkpoint\n\nclass ResNet50Checkpointed(nn.Module):\n    \"\"\"ResNet50 with configurable checkpointing strategies.\"\"\"\n    \n    def __init__(self, num_classes=1000, checkpoint_strategy='none'):\n        \"\"\"\n        Args:\n            checkpoint_strategy: 'none', 'per_stage', 'per_block', or 'aggressive'\n        \"\"\"\n        super().__init__()\n        \n        # Load base ResNet50\n        base = models.resnet50(weights=None)\n        \n        # Copy all layers\n        self.conv1 = base.conv1\n        self.bn1 = base.bn1\n        self.relu = base.relu\n        self.maxpool = base.maxpool\n        self.layer1 = base.layer1\n        self.layer2 = base.layer2\n        self.layer3 = base.layer3\n        self.layer4 = base.layer4\n        self.avgpool = base.avgpool\n        self.fc = nn.Linear(2048, num_classes)\n        \n        self.checkpoint_strategy = checkpoint_strategy\n    \n    def _forward_stage(self, stage, x):\n        \"\"\"Forward through a stage (layer1, layer2, etc.)\"\"\"\n        for block in stage:\n            x = block(x)\n        return x\n    \n    def forward(self, x):\n        # Stem\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        \n        if self.checkpoint_strategy == 'none':\n            # No checkpointing\n            x = self.layer1(x)\n            x = self.layer2(x)\n            x = self.layer3(x)\n            x = self.layer4(x)\n            \n        elif self.checkpoint_strategy == 'per_stage':\n            # Checkpoint each stage as a whole\n            x = checkpoint(lambda t: self._forward_stage(self.layer1, t), x, use_reentrant=False)\n            x = checkpoint(lambda t: self._forward_stage(self.layer2, t), x, use_reentrant=False)\n            x = checkpoint(lambda t: self._forward_stage(self.layer3, t), x, use_reentrant=False)\n            x = checkpoint(lambda t: self._forward_stage(self.layer4, t), x, use_reentrant=False)\n            \n        elif self.checkpoint_strategy == 'per_block':\n            # Checkpoint each bottleneck block individually\n            for block in self.layer1:\n                x = checkpoint(block, x, use_reentrant=False)\n            for block in self.layer2:\n                x = checkpoint(block, x, use_reentrant=False)\n            for block in self.layer3:\n                x = checkpoint(block, x, use_reentrant=False)\n            for block in self.layer4:\n                x = checkpoint(block, x, use_reentrant=False)\n                \n        elif self.checkpoint_strategy == 'aggressive':\n            # Checkpoint layer3 and layer4 only (highest memory impact)\n            x = self.layer1(x)\n            x = self.layer2(x)\n            for block in self.layer3:\n                x = checkpoint(block, x, use_reentrant=False)\n            for block in self.layer4:\n                x = checkpoint(block, x, use_reentrant=False)\n        \n        # Head\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        \n        return x\n\nprint(\"ResNet50Checkpointed created with 4 strategies:\")\nprint(\"-\" * 60)\nprint(\"'none':       No checkpointing (baseline)\")\nprint(\"'per_stage':  Checkpoint each of the 4 stages\")\nprint(\"'per_block':  Checkpoint each of the 16 bottleneck blocks\")\nprint(\"'aggressive': Only checkpoint layer3 and layer4 (best ROI)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 12.3 Memory Comparison: ResNet50 Strategies\n\nimport torch\nimport time\n\ndef benchmark_resnet50(strategy, batch_size=32, image_size=224, num_iters=5):\n    \"\"\"Benchmark memory and time for a ResNet50 checkpointing strategy.\"\"\"\n    model = ResNet50Checkpointed(num_classes=1000, checkpoint_strategy=strategy).cuda()\n    model.train()\n    \n    x = torch.randn(batch_size, 3, image_size, image_size).cuda()\n    target = torch.randint(0, 1000, (batch_size,)).cuda()\n    criterion = nn.CrossEntropyLoss()\n    \n    # Warmup\n    for _ in range(2):\n        out = model(x)\n        loss = criterion(out, target)\n        loss.backward()\n        model.zero_grad()\n    \n    # Measure memory\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    torch.cuda.synchronize()\n    \n    start = time.time()\n    for _ in range(num_iters):\n        out = model(x)\n        loss = criterion(out, target)\n        loss.backward()\n        model.zero_grad()\n    torch.cuda.synchronize()\n    elapsed = (time.time() - start) / num_iters * 1000  # ms\n    \n    peak_mem = torch.cuda.max_memory_allocated() / 1e6  # MB\n    \n    del model, x, target\n    torch.cuda.empty_cache()\n    \n    return peak_mem, elapsed\n\nif torch.cuda.is_available():\n    batch_size = 32\n    image_size = 224\n    \n    print(f\"ResNet50 Checkpointing Comparison\")\n    print(f\"Batch size: {batch_size}, Image size: {image_size}x{image_size}\")\n    print(\"-\" * 70)\n    print(f\"{'Strategy':<15} {'Peak Memory (MB)':<20} {'Time (ms)':<15} {'Mem Savings':<15}\")\n    print(\"-\" * 70)\n    \n    strategies = ['none', 'aggressive', 'per_stage', 'per_block']\n    results = {}\n    \n    for strategy in strategies:\n        mem, time_ms = benchmark_resnet50(strategy, batch_size, image_size)\n        results[strategy] = (mem, time_ms)\n    \n    baseline_mem = results['none'][0]\n    for strategy in strategies:\n        mem, time_ms = results[strategy]\n        savings = (1 - mem / baseline_mem) * 100\n        print(f\"{strategy:<15} {mem:<20.2f} {time_ms:<15.2f} {savings:>6.1f}%\")\n    \n    print(\"-\" * 70)\n    print(\"\\nObservations:\")\n    print(\"- 'aggressive' gives best memory/speed balance (checkpoints only deep layers)\")\n    print(\"- 'per_block' gives maximum memory savings but highest overhead\")\n    print(\"- 'per_stage' is a middle ground\")\nelse:\n    print(\"[Run on GPU for ResNet50 benchmarks]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 12.4 Maximum Batch Size: ResNet50\n\ndef find_max_batch_resnet50(strategy, start=16, max_batch=256):\n    \"\"\"Find maximum batch size before OOM for ResNet50.\"\"\"\n    low, high = start, max_batch\n    max_working = start\n    \n    while low <= high:\n        mid = (low + high) // 2\n        try:\n            torch.cuda.empty_cache()\n            model = ResNet50Checkpointed(checkpoint_strategy=strategy).cuda()\n            model.train()\n            x = torch.randn(mid, 3, 224, 224).cuda()\n            target = torch.randint(0, 1000, (mid,)).cuda()\n            \n            out = model(x)\n            loss = nn.CrossEntropyLoss()(out, target)\n            loss.backward()\n            \n            max_working = mid\n            low = mid + 1\n            del model, x, target, out, loss\n            torch.cuda.empty_cache()\n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                high = mid - 1\n                torch.cuda.empty_cache()\n            else:\n                raise\n    return max_working\n\nif torch.cuda.is_available():\n    print(\"Finding maximum batch size for ResNet50 (224x224 images)\")\n    print(\"-\" * 50)\n    \n    strategies = ['none', 'aggressive', 'per_block']\n    baseline = None\n    \n    for strategy in strategies:\n        max_batch = find_max_batch_resnet50(strategy)\n        if baseline is None:\n            baseline = max_batch\n        improvement = max_batch / baseline\n        print(f\"{strategy:<15}: max batch = {max_batch:>3d} ({improvement:.2f}x)\")\n    \n    print(\"-\" * 50)\n    print(\"\\nWith checkpointing, you can fit 1.5-2x larger batches.\")\n    print(\"This is the practical win for production training.\")\nelse:\n    print(\"[Run on GPU to find max batch sizes]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 12.5 Complete Training Loop: ResNet50 with Checkpointing\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndef train_resnet50_with_checkpointing(\n    checkpoint_strategy='aggressive',\n    batch_size=32,\n    num_epochs=3,\n    num_samples=256,  # Small for demo\n    use_amp=True\n):\n    \"\"\"Complete training loop with checkpointing and mixed precision.\"\"\"\n    \n    # Create model\n    model = ResNet50Checkpointed(\n        num_classes=10,  # Simplified for demo\n        checkpoint_strategy=checkpoint_strategy\n    ).cuda()\n    \n    # Create synthetic dataset\n    X = torch.randn(num_samples, 3, 224, 224)\n    y = torch.randint(0, 10, (num_samples,))\n    dataset = TensorDataset(X, y)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    # Optimizer and loss\n    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n    criterion = nn.CrossEntropyLoss()\n    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n    \n    print(f\"Training ResNet50 with '{checkpoint_strategy}' checkpointing\")\n    print(f\"Batch size: {batch_size}, Mixed precision: {use_amp}\")\n    print(\"-\" * 50)\n    \n    model.train()\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        num_batches = 0\n        \n        torch.cuda.reset_peak_memory_stats()\n        \n        for batch_x, batch_y in loader:\n            batch_x = batch_x.cuda()\n            batch_y = batch_y.cuda()\n            \n            optimizer.zero_grad()\n            \n            if use_amp:\n                with torch.cuda.amp.autocast():\n                    outputs = model(batch_x)\n                    loss = criterion(outputs, batch_y)\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                outputs = model(batch_x)\n                loss = criterion(outputs, batch_y)\n                loss.backward()\n                optimizer.step()\n            \n            epoch_loss += loss.item()\n            num_batches += 1\n        \n        peak_mem = torch.cuda.max_memory_allocated() / 1e6\n        avg_loss = epoch_loss / num_batches\n        print(f\"Epoch {epoch+1}/{num_epochs}: loss = {avg_loss:.4f}, peak memory = {peak_mem:.2f} MB\")\n    \n    print(\"-\" * 50)\n    print(\"Training complete!\")\n    return model\n\nif torch.cuda.is_available():\n    # Demo training with aggressive checkpointing\n    model = train_resnet50_with_checkpointing(\n        checkpoint_strategy='aggressive',\n        batch_size=32,\n        num_epochs=3,\n        use_amp=True\n    )\nelse:\n    print(\"[Run on GPU for training demo]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 12.6 Alternative: Using torchvision's Built-in Support\n\nimport torch\nimport torchvision.models as models\nfrom torch.utils.checkpoint import checkpoint_sequential\n\ndef resnet50_with_sequential_checkpoint(num_segments=4):\n    \"\"\"\n    Use checkpoint_sequential on ResNet50's layer blocks.\n    Simpler than custom class, but less flexible.\n    \"\"\"\n    model = models.resnet50(weights=None)\n    \n    # Store original forward\n    original_forward = model.forward\n    \n    def checkpointed_forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        \n        # Checkpoint each layer using checkpoint_sequential\n        x = checkpoint_sequential(self.layer1, len(self.layer1), x, use_reentrant=False)\n        x = checkpoint_sequential(self.layer2, len(self.layer2), x, use_reentrant=False)\n        x = checkpoint_sequential(self.layer3, len(self.layer3), x, use_reentrant=False)\n        x = checkpoint_sequential(self.layer4, len(self.layer4), x, use_reentrant=False)\n        \n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n    \n    # Bind the new forward method\n    import types\n    model.forward = types.MethodType(checkpointed_forward, model)\n    \n    return model\n\n# Test it\nmodel = resnet50_with_sequential_checkpoint()\nprint(\"Alternative approach: Monkey-patch torchvision ResNet50\")\nprint(\"-\" * 60)\nprint(\"Pros:\")\nprint(\"  - Uses official torchvision model\")\nprint(\"  - Simple implementation\")\nprint(\"  - No custom class needed\")\nprint()\nprint(\"Cons:\")\nprint(\"  - Less control over which blocks to checkpoint\")\nprint(\"  - Harder to switch strategies dynamically\")\nprint()\nprint(\"Use the custom class (ResNet50Checkpointed) for production.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (12: ResNet50 Case Study)\n\nResNet50 is a perfect testbed for checkpointing because:\n- 16 bottleneck blocks across 4 stages\n- Deep enough to benefit, not so deep it is impractical\n- Widely used in production\n\nKey findings:\n\n**Strategy Selection:**\n- `aggressive` (checkpoint layer3+layer4): Best ROI. These layers have the most parameters and largest feature maps.\n- `per_block`: Maximum memory savings (~40-50%), but highest compute overhead (~30-40%).\n- `per_stage`: Middle ground. Good for quick wins.\n\n**Practical Impact:**\n- 1.5-2x larger batch sizes possible\n- Combined with mixed precision: up to 3x improvement\n\n**Production Recommendations:**\n1. Start with `aggressive` strategy\n2. Combine with mixed precision (AMP)\n3. Profile before and after\n4. If still OOM, move to `per_block`\n\nThe ResNet50Checkpointed class is production-ready. Copy it into your codebase and configure the strategy based on your GPU memory.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 13. Conclusion\n\nActivation checkpointing is a memory-compute trade-off. It trades extra forward passes for reduced memory usage. The trade-off is usually worth it.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 13.1 Key Takeaways\n\nprint(\"\"\"\nKEY TAKEAWAYS\n=============\n\n1. ACTIVATIONS DOMINATE MEMORY\n   - Not parameters, not gradients\n   - Scales with batch_size * depth * hidden_size\n   - Peak memory occurs at start of backward pass\n\n2. CHECKPOINTING TRADES COMPUTE FOR MEMORY\n   - Save some activations (checkpoints)\n   - Recompute others during backward pass\n   - Typical: 30-50% memory savings, ~50% compute overhead\n\n3. THE SQRT(N) RULE\n   - Optimal checkpointing: O(sqrt(n)) memory for n layers\n   - 100 layers -> ~10 checkpoints -> 10x memory reduction\n\n4. PRACTICAL PATTERNS\n   - checkpoint(): Single modules/functions\n   - checkpoint_sequential(): Sequential models\n   - Always use use_reentrant=False\n\n5. ADVANCED: SELECTIVE ACTIVATION CHECKPOINT (SAC)\n   - Fine-grained control: choose what to save vs recompute\n   - Policy functions: MUST_SAVE for expensive ops, PREFER_RECOMPUTE for cheap\n   - Sweet spot: save matmuls, recompute pointwise ops\n   - Use aten.<op>.default for correct op matching\n\n6. ADVANCED: MEMORY BUDGET API (torch.compile)\n   - One line: torch._dynamo.config.activation_memory_budget = 0.5\n   - Automatic pareto-optimal recomputation strategies\n   - Budget 0 = full AC, Budget 1 = default compile\n\n7. COMBINE WITH OTHER TECHNIQUES\n   - Mixed precision: 2x memory from fp16\n   - Gradient accumulation: Effective larger batches\n   - DDP: Scale across GPUs\n   - Together: Train 4-5x larger models\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 13.2 Decision Framework: When to Use Checkpointing\n\nprint(\"\"\"\nWHEN TO USE ACTIVATION CHECKPOINTING\n====================================\n\nUSE IT WHEN:\n- You are hitting OOM errors\n- You want larger batch sizes\n- Your model has 10+ layers\n- Training time is less critical than memory\n- You are training transformers or deep CNNs\n\nSKIP IT WHEN:\n- Model fits comfortably in memory\n- Training time is the bottleneck (not memory)\n- Model is shallow (< 5 layers)\n- You need maximum training speed\n\nQUICK DECISION TREE:\n\n  OOM Error?\n     |\n     v\n  YES --> Use checkpointing\n     |\n     v\n  Want larger batches?\n     |\n     v\n  YES --> Use checkpointing\n     |\n     v\n  Model > 10 layers?\n     |\n     v\n  YES --> Consider checkpointing\n     |\n     v\n  NO --> Probably skip it\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Final Impressions\n\nActivation checkpointing is not magic. It is a simple trade-off executed well.\n\nYou now understand:\n- Why activations dominate memory (batch x depth x hidden)\n- How checkpointing works (recompute instead of store)\n- When to use it (memory-bound, deep models)\n- Basic implementation (checkpoint, checkpoint_sequential, use_reentrant=False)\n- Advanced control with SAC (choose exactly what to save)\n- Automatic optimization with Memory Budget API (one config line)\n\nThe landscape of techniques:\n- **Eager**: Maximum speed, maximum memory\n- **torch.compile**: Free speedups, some automatic memory savings\n- **Memory Budget API**: Tunable compile-time optimization (0 to 1)\n- **Selective AC**: Manual control over save/recompute decisions\n- **Standard AC**: Maximum memory savings, ~50% compute overhead\n\nStart simple. Add complexity only when needed. Measure everything.\n\nEvery large language model uses some form of activation checkpointing. Now you know exactly how it works and when to use each variant.\n\nGo train something bigger.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}