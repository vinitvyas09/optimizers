{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Checkpointing: A First Principles Deep Dive\n",
    "\n",
    "Memory is the bottleneck in deep learning. Not compute. Not data. Memory.\n",
    "\n",
    "This notebook tears apart activation checkpointing from first principles. You will understand exactly why activations dominate memory, how checkpointing trades compute for memory, and when this trade-off makes sense.\n",
    "\n",
    "Every code cell has an Impressions/Conclusions section. These are not summaries. They are insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Introduction\n",
    "\n",
    "Training deep neural networks hits a wall. That wall is GPU memory.\n",
    "\n",
    "You have a model. You have data. You have compute. But your GPU runs out of memory. Why?\n",
    "\n",
    "Three things consume GPU memory during training:\n",
    "1. **Model parameters** (weights and biases)\n",
    "2. **Gradients** (same size as parameters)\n",
    "3. **Activations** (intermediate outputs from each layer)\n",
    "\n",
    "Here is the surprise: activations dominate. Not parameters. Not gradients. Activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 The Memory Breakdown: Where Does Memory Go?\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def bytes_to_mb(b):\n",
    "    return b / (1024 * 1024)\n",
    "\n",
    "class DeepNetwork(nn.Module):\n",
    "    def __init__(self, input_size=1024, hidden_size=1024, num_layers=10):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_f = input_size if i == 0 else hidden_size\n",
    "            layers.append(nn.Linear(in_f, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = DeepNetwork(input_size=1024, hidden_size=1024, num_layers=10)\n",
    "num_params = count_parameters(model)\n",
    "param_mem = num_params * 4\n",
    "grad_mem = param_mem\n",
    "\n",
    "print(f\"Model: {num_params:,} parameters\")\n",
    "print(f\"Param memory: {bytes_to_mb(param_mem):.2f} MB\")\n",
    "print(f\"Gradient memory: {bytes_to_mb(grad_mem):.2f} MB\")\n",
    "print(f\"Total (params+grads): {bytes_to_mb(param_mem + grad_mem):.2f} MB\")\n",
    "print()\n",
    "print(\"Activation memory by batch size:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for batch_size in [32, 64, 128, 256, 512]:\n",
    "    act_mem = batch_size * 1024 * 10 * 4\n",
    "    total = param_mem + grad_mem + act_mem\n",
    "    pct = (act_mem / total) * 100\n",
    "    print(f\"Batch {batch_size:3d}: {bytes_to_mb(act_mem):6.2f} MB ({pct:.1f}% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (1.1)\n",
    "\n",
    "Activation memory scales with batch size. Parameter memory does not.\n",
    "\n",
    "At batch 32, activations are ~20% of memory. At batch 512, activations are 80%+. This is why you OOM when increasing batch size, not model size.\n",
    "\n",
    "The math:\n",
    "- Parameters: O(model_size)\n",
    "- Activations: O(batch_size × depth × hidden_size)\n",
    "\n",
    "Batch size is the multiplier that kills you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Real GPU Memory Measurement\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepNetwork(nn.Module):\n",
    "    def __init__(self, input_size=1024, hidden_size=2048, num_layers=20):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_f = input_size if i == 0 else hidden_size\n",
    "            layers.append(nn.Linear(in_f, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    model = DeepNetwork().cuda()\n",
    "    \n",
    "    for batch_size in [16, 32, 64, 128]:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        x = torch.randn(batch_size, 1024).cuda()\n",
    "        output = model(x)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        peak = torch.cuda.max_memory_allocated() / 1e6\n",
    "        print(f\"Batch {batch_size:3d}: Peak = {peak:.2f} MB\")\n",
    "        \n",
    "        model.zero_grad()\n",
    "        del x, output, loss\n",
    "else:\n",
    "    print(\"[Run on GPU to see measurements]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (1.2)\n",
    "\n",
    "Peak memory grows linearly with batch size. Double the batch, roughly double the memory.\n",
    "\n",
    "This is what activation checkpointing solves. Reduce activation memory to:\n",
    "1. Train larger models\n",
    "2. Use larger batches\n",
    "3. Go deeper without OOM\n",
    "\n",
    "The solution: do not store all activations. Recompute them when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Understanding Activation Checkpointing\n",
    "\n",
    "Activation checkpointing is a memory-compute trade-off. Save memory by not storing activations. Pay for it by recomputing them during backprop.\n",
    "\n",
    "To understand this, you need to know what activations are and why backprop needs them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What Are Activations?\n",
    "\n",
    "An activation is the output of a layer. That is it.\n",
    "\n",
    "```\n",
    "Input x -> [Layer 1] -> a1 -> [Layer 2] -> a2 -> [Layer 3] -> a3 -> Output\n",
    "```\n",
    "\n",
    "`a1`, `a2`, `a3` are activations. Each is a tensor stored in memory.\n",
    "\n",
    "Why store them? Backpropagation needs them to compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Why Backprop Needs Activations\n",
    "\n",
    "import torch\n",
    "\n",
    "# y = W2 * relu(W1 * x)\n",
    "# Gradient w.r.t. W1 needs the pre-activation W1*x\n",
    "# to compute relu'(W1*x)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "W1 = torch.randn(4, 4, requires_grad=True)\n",
    "W2 = torch.randn(4, 4, requires_grad=True)\n",
    "x = torch.randn(1, 4)\n",
    "\n",
    "# Forward pass stores these\n",
    "z1 = x @ W1.T           # Pre-activation (needed for ReLU grad)\n",
    "a1 = torch.relu(z1)     # Activation (needed for W2 grad)\n",
    "y = a1 @ W2.T\n",
    "\n",
    "print(\"Stored for backprop:\")\n",
    "print(f\"  z1: {z1.shape} - needed to compute relu gradient\")\n",
    "print(f\"  a1: {a1.shape} - needed to compute W2 gradient\")\n",
    "\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradients computed:\")\n",
    "print(f\"  W1.grad: {W1.grad.shape}\")\n",
    "print(f\"  W2.grad: {W2.grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (2.1)\n",
    "\n",
    "The chain rule requires intermediate values. Backprop walks backward through the computation graph. At each step, it needs forward pass values.\n",
    "\n",
    "For linear `y = Wx`: gradient w.r.t. W needs x.\n",
    "\n",
    "For ReLU `y = relu(x)`: gradient needs to know where x > 0.\n",
    "\n",
    "This is why activations are stored. This is what checkpointing eliminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 The Core Trade-off: Store vs Recompute\n",
    "\n",
    "import math\n",
    "\n",
    "print(\"Standard Training:\")\n",
    "print(\"  Forward: x -> a1 -> a2 -> a3 -> loss\")\n",
    "print(\"           [store] [store] [store]\")\n",
    "print(\"  Backward: uses stored a1, a2, a3\")\n",
    "print(\"  Memory: O(n) activations\")\n",
    "print()\n",
    "print(\"With Checkpointing:\")\n",
    "print(\"  Forward: x -> a1 -> a2 -> a3 -> loss\")\n",
    "print(\"           [save]       [save]\")\n",
    "print(\"  Backward: recompute a2 from a1, then use\")\n",
    "print(\"  Memory: O(sqrt(n)) with optimal placement\")\n",
    "print()\n",
    "\n",
    "num_layers = 100\n",
    "optimal = int(math.sqrt(num_layers))\n",
    "\n",
    "print(f\"For {num_layers} layers:\")\n",
    "print(f\"  Standard: {num_layers} activations\")\n",
    "print(f\"  Checkpointed: ~{optimal} activations\")\n",
    "print(f\"  Memory reduction: {num_layers/optimal:.0f}x\")\n",
    "print(f\"  Compute overhead: ~{100/num_layers*optimal:.0f}% extra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (2.2)\n",
    "\n",
    "The sqrt(n) rule: optimal checkpointing reduces memory from O(n) to O(sqrt(n)).\n",
    "\n",
    "For 100 layers:\n",
    "- Standard: 100 activations\n",
    "- Checkpointed: ~10 activations  \n",
    "- Cost: ~10% extra compute\n",
    "\n",
    "10x memory reduction for 10% compute overhead. That is an incredible trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Prerequisites\n",
    "\n",
    "Environment setup and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Environment Check\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (3.1)\n",
    "\n",
    "PyTorch 1.9+ required for full checkpointing support. `torch.utils.checkpoint` provides the core API. CUDA strongly recommended since checkpointing shines when GPU memory is the bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Memory Profiling Utilities\n",
    "\n",
    "import torch\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class MemoryTracker:\n",
    "    \"\"\"Track GPU memory usage.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.snapshots = []\n",
    "    \n",
    "    def snapshot(self, label=\"\"):\n",
    "        if torch.cuda.is_available():\n",
    "            mem = torch.cuda.memory_allocated() / 1e6\n",
    "            self.snapshots.append({'label': label, 'mb': mem})\n",
    "            return mem\n",
    "        return 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.snapshots = []\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def peak_mb(self):\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.max_memory_allocated() / 1e6\n",
    "        return 0\n",
    "    \n",
    "    def report(self):\n",
    "        for s in self.snapshots:\n",
    "            print(f\"{s['label']:30s}: {s['mb']:8.2f} MB\")\n",
    "        print(f\"{'Peak':30s}: {self.peak_mb():8.2f} MB\")\n",
    "\n",
    "@contextmanager\n",
    "def track_memory(label=\"Op\"):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "    yield\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"{label}: Peak = {torch.cuda.max_memory_allocated()/1e6:.2f} MB\")\n",
    "\n",
    "print(\"Utilities loaded: MemoryTracker, track_memory()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (3.2)\n",
    "\n",
    "Key functions:\n",
    "- `torch.cuda.memory_allocated()`: current memory in use\n",
    "- `torch.cuda.max_memory_allocated()`: peak since last reset\n",
    "- `torch.cuda.empty_cache()`: free cached memory\n",
    "\n",
    "Peak memory is what matters. That is what causes OOM."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 4. How Activation Checkpointing Works\n\nPyTorch provides `torch.utils.checkpoint`. Two main functions:\n- `checkpoint`: wrap a single function/module\n- `checkpoint_sequential`: wrap a sequence of modules\n\nThe API is simple. The magic is in what happens under the hood.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 4.1 Model WITHOUT Checkpointing\n\nimport torch\nimport torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n\n    def forward(self, x):\n        x = torch.relu(self.layer1(x))\n        x = torch.relu(self.layer2(x))\n        return x\n\nif torch.cuda.is_available():\n    model = SimpleModel().cuda()\n    x = torch.randn(1, 1024).cuda()\n\n    torch.cuda.reset_peak_memory_stats()\n    output = model(x)\n    loss = output.sum()\n    loss.backward()\n    \n    print(f\"Peak Memory WITHOUT Checkpointing: {torch.cuda.max_memory_allocated() / 1e6:.2f} MB\")\nelse:\n    print(\"[Run on GPU]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 4.2 Model WITH Checkpointing\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass CheckpointedModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n\n    def forward(self, x):\n        # Checkpoint layer1: its activation won't be stored\n        # It will be recomputed during backward pass\n        x = checkpoint(lambda t: torch.relu(self.layer1(t)), x, use_reentrant=False)\n        x = torch.relu(self.layer2(x))\n        return x\n\nif torch.cuda.is_available():\n    model = CheckpointedModel().cuda()\n    x = torch.randn(1, 1024).cuda()\n\n    torch.cuda.reset_peak_memory_stats()\n    output = model(x)\n    loss = output.sum()\n    loss.backward()\n    \n    print(f\"Peak Memory WITH Checkpointing: {torch.cuda.max_memory_allocated() / 1e6:.2f} MB\")\nelse:\n    print(\"[Run on GPU]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (4.1 & 4.2)\n\nThe difference is subtle in small models. With 2 layers, memory savings are minimal because the baseline is already small.\n\nThe real benefit appears at scale. Let us test with a deeper model.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 4.3 Deep Model Comparison: The Real Difference\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass DeepModelNoCheckpoint(nn.Module):\n    def __init__(self, num_layers=20, hidden=2048):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Linear(hidden, hidden) for _ in range(num_layers)\n        ])\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = torch.relu(layer(x))\n        return x\n\nclass DeepModelWithCheckpoint(nn.Module):\n    def __init__(self, num_layers=20, hidden=2048):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Linear(hidden, hidden) for _ in range(num_layers)\n        ])\n    \n    def forward(self, x):\n        for layer in self.layers:\n            # Checkpoint every layer\n            x = checkpoint(lambda t, l=layer: torch.relu(l(t)), x, use_reentrant=False)\n        return x\n\nif torch.cuda.is_available():\n    batch_size = 64\n    hidden = 2048\n    num_layers = 20\n    \n    # Without checkpointing\n    torch.cuda.empty_cache()\n    model1 = DeepModelNoCheckpoint(num_layers, hidden).cuda()\n    x1 = torch.randn(batch_size, hidden).cuda()\n    \n    torch.cuda.reset_peak_memory_stats()\n    out1 = model1(x1)\n    out1.sum().backward()\n    mem_no_ckpt = torch.cuda.max_memory_allocated() / 1e6\n    \n    del model1, x1, out1\n    torch.cuda.empty_cache()\n    \n    # With checkpointing\n    model2 = DeepModelWithCheckpoint(num_layers, hidden).cuda()\n    x2 = torch.randn(batch_size, hidden).cuda()\n    \n    torch.cuda.reset_peak_memory_stats()\n    out2 = model2(x2)\n    out2.sum().backward()\n    mem_ckpt = torch.cuda.max_memory_allocated() / 1e6\n    \n    print(f\"Config: {num_layers} layers, hidden={hidden}, batch={batch_size}\")\n    print(f\"WITHOUT checkpointing: {mem_no_ckpt:.2f} MB\")\n    print(f\"WITH checkpointing:    {mem_ckpt:.2f} MB\")\n    print(f\"Memory saved: {mem_no_ckpt - mem_ckpt:.2f} MB ({(1 - mem_ckpt/mem_no_ckpt)*100:.1f}%)\")\nelse:\n    print(\"[Run on GPU]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (4.3)\n\nNow you see the real savings. With 20 layers at hidden=2048, checkpointing can save 30-50% memory depending on batch size and model architecture.\n\nThe key insight: activation memory grows with depth. Checkpointing trades that linear growth for constant (or sqrt) memory.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 5. Code Examples\n\nThree patterns you will use:\n1. Basic `checkpoint()` for single modules\n2. `checkpoint_sequential()` for sequential models\n3. Manual checkpointing in complex architectures (transformers)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 5.1 Basic Usage\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU())\n        self.block2 = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU())\n\n    def forward(self, x):\n        # Checkpoint block1\n        x = checkpoint(self.block1, x, use_reentrant=False)\n        x = self.block2(x)\n        return x\n\nmodel = SimpleModel()\nif torch.cuda.is_available():\n    model = model.cuda()\n\nx = torch.randn(1, 1024)\nif torch.cuda.is_available():\n    x = x.cuda()\n\noutput = model(x)\nprint(f\"Output shape: {output.shape}\")\nprint(\"Checkpointing applied to block1. Block2 runs normally.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (5.1)\n\n`checkpoint(fn, *args)` wraps any callable. During forward, it runs `fn(*args)` but does not save intermediate activations. During backward, it re-runs `fn(*args)` to recompute them.\n\nNote: `use_reentrant=False` is the modern API. It handles edge cases better than the old default.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 5.2 Using checkpoint_sequential\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint_sequential\n\n# A deep sequential model\nmodel = nn.Sequential(\n    nn.Linear(1024, 1024), nn.ReLU(),\n    nn.Linear(1024, 1024), nn.ReLU(),\n    nn.Linear(1024, 1024), nn.ReLU(),\n    nn.Linear(1024, 1024), nn.ReLU(),\n)\n\nx = torch.randn(1, 1024)\nif torch.cuda.is_available():\n    model = model.cuda()\n    x = x.cuda()\n\n# Split into 2 checkpoint segments\n# Each segment will be checkpointed separately\nsegments = 2\noutput = checkpoint_sequential(model, segments, x, use_reentrant=False)\n\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Model split into {segments} checkpoint segments\")\nprint(\"Each segment recomputes activations during backward pass\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (5.2)\n\n`checkpoint_sequential` is convenient for nn.Sequential models. The `segments` parameter controls granularity:\n- More segments = less memory, more recomputation\n- Fewer segments = more memory, less recomputation\n\nRule of thumb: start with sqrt(num_layers) segments.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 5.3 Transformer Training with Checkpointing\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_size, num_heads=8):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n        self.norm1 = nn.LayerNorm(embed_size)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(embed_size, embed_size * 4),\n            nn.GELU(),\n            nn.Linear(embed_size * 4, embed_size)\n        )\n        self.norm2 = nn.LayerNorm(embed_size)\n\n    def forward(self, x):\n        # Self-attention with residual\n        attn_out, _ = self.attention(x, x, x)\n        x = self.norm1(x + attn_out)\n        # Feed-forward with residual (checkpointed)\n        ff_out = checkpoint(self.feed_forward, x, use_reentrant=False)\n        x = self.norm2(x + ff_out)\n        return x\n\n# Example usage\nembed_size = 512\nseq_length = 64\nbatch_size = 8\n\nblock = TransformerBlock(embed_size)\nif torch.cuda.is_available():\n    block = block.cuda()\n\nx = torch.randn(batch_size, seq_length, embed_size)\nif torch.cuda.is_available():\n    x = x.cuda()\n\noutput = block(x)\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(\"Feed-forward block is checkpointed (it uses the most memory)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (5.3)\n\nIn transformers, the feed-forward block uses 4x the hidden size. This is where most activation memory goes. Checkpointing the FFN is the standard practice in large language models.\n\nWhy not checkpoint attention? You can, but attention has complex intermediate states. The memory-compute trade-off is less favorable there.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 5.4 ResNet with Selective Checkpointing (Bonus)\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass BasicBlock(nn.Module):\n    \"\"\"ResNet basic block with optional checkpointing.\"\"\"\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        return torch.relu(out)\n\nclass MiniResNet(nn.Module):\n    \"\"\"Small ResNet with checkpointing options.\"\"\"\n    def __init__(self, num_blocks=4, use_checkpoint=False):\n        super().__init__()\n        self.use_checkpoint = use_checkpoint\n        \n        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        \n        self.blocks = nn.ModuleList([\n            BasicBlock(64 if i == 0 else 128, 128, stride=2 if i == 0 else 1)\n            for i in range(num_blocks)\n        ])\n        \n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(128, 10)\n    \n    def forward(self, x):\n        x = torch.relu(self.bn1(self.conv1(x)))\n        \n        for block in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint(block, x, use_reentrant=False)\n            else:\n                x = block(x)\n        \n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\n# Compare memory\nif torch.cuda.is_available():\n    batch_size = 32\n    \n    # Without checkpoint\n    torch.cuda.empty_cache()\n    model1 = MiniResNet(num_blocks=8, use_checkpoint=False).cuda()\n    x1 = torch.randn(batch_size, 3, 32, 32).cuda()\n    torch.cuda.reset_peak_memory_stats()\n    out1 = model1(x1)\n    out1.sum().backward()\n    mem1 = torch.cuda.max_memory_allocated() / 1e6\n    \n    del model1, x1, out1\n    torch.cuda.empty_cache()\n    \n    # With checkpoint\n    model2 = MiniResNet(num_blocks=8, use_checkpoint=True).cuda()\n    x2 = torch.randn(batch_size, 3, 32, 32).cuda()\n    torch.cuda.reset_peak_memory_stats()\n    out2 = model2(x2)\n    out2.sum().backward()\n    mem2 = torch.cuda.max_memory_allocated() / 1e6\n    \n    print(f\"ResNet with 8 blocks, batch={batch_size}\")\n    print(f\"WITHOUT checkpoint: {mem1:.2f} MB\")\n    print(f\"WITH checkpoint:    {mem2:.2f} MB\")\n    print(f\"Savings: {(1 - mem2/mem1)*100:.1f}%\")\nelse:\n    print(\"[Run on GPU]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (5.4)\n\nCNNs benefit from checkpointing too. Each residual block stores feature maps that can be recomputed. For deeper ResNets (ResNet-152), checkpointing is essential to train with reasonable batch sizes.\n\nKey pattern: wrap entire blocks, not individual layers. The overhead of checkpoint calls adds up.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 6. Performance Benchmarks\n\nNumbers matter. Let us measure:\n1. Memory savings across model sizes\n2. Compute overhead (training time)\n3. Batch size scaling",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 6.1 Memory Savings Across Model Depths\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\nimport time\n\ndef create_model(num_layers, hidden, use_checkpoint=False):\n    class Model(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.layers = nn.ModuleList([\n                nn.Linear(hidden, hidden) for _ in range(num_layers)\n            ])\n            self.use_ckpt = use_checkpoint\n        \n        def forward(self, x):\n            for layer in self.layers:\n                if self.use_ckpt:\n                    x = checkpoint(lambda t, l=layer: torch.relu(l(t)), x, use_reentrant=False)\n                else:\n                    x = torch.relu(layer(x))\n            return x\n    return Model()\n\nif torch.cuda.is_available():\n    hidden = 1024\n    batch = 64\n    \n    print(f\"Config: hidden={hidden}, batch={batch}\")\n    print(\"-\" * 60)\n    print(f\"{'Layers':<10} {'No Ckpt (MB)':<15} {'With Ckpt (MB)':<15} {'Savings':<10}\")\n    print(\"-\" * 60)\n    \n    for num_layers in [10, 20, 40, 80]:\n        # Without checkpoint\n        torch.cuda.empty_cache()\n        m1 = create_model(num_layers, hidden, False).cuda()\n        x1 = torch.randn(batch, hidden).cuda()\n        torch.cuda.reset_peak_memory_stats()\n        m1(x1).sum().backward()\n        mem1 = torch.cuda.max_memory_allocated() / 1e6\n        del m1, x1\n        \n        # With checkpoint\n        torch.cuda.empty_cache()\n        m2 = create_model(num_layers, hidden, True).cuda()\n        x2 = torch.randn(batch, hidden).cuda()\n        torch.cuda.reset_peak_memory_stats()\n        m2(x2).sum().backward()\n        mem2 = torch.cuda.max_memory_allocated() / 1e6\n        del m2, x2\n        \n        savings = (1 - mem2/mem1) * 100\n        print(f\"{num_layers:<10} {mem1:<15.2f} {mem2:<15.2f} {savings:.1f}%\")\nelse:\n    print(\"[Run on GPU for benchmarks]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (6.1)\n\nMemory savings increase with depth. At 10 layers, savings are modest. At 80 layers, savings can exceed 50%.\n\nThis matches the theory: activation memory is O(n), checkpointing reduces it to O(1) per segment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 6.2 Compute Overhead Measurement\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\nimport time\n\ndef benchmark_time(model, x, num_iters=10):\n    # Warmup\n    for _ in range(3):\n        model(x).sum().backward()\n        model.zero_grad()\n    \n    torch.cuda.synchronize()\n    start = time.time()\n    for _ in range(num_iters):\n        model(x).sum().backward()\n        model.zero_grad()\n    torch.cuda.synchronize()\n    return (time.time() - start) / num_iters * 1000  # ms\n\nif torch.cuda.is_available():\n    hidden = 1024\n    batch = 64\n    num_layers = 40\n    \n    # Create models\n    m1 = create_model(num_layers, hidden, False).cuda()\n    m2 = create_model(num_layers, hidden, True).cuda()\n    x = torch.randn(batch, hidden).cuda()\n    \n    time1 = benchmark_time(m1, x)\n    time2 = benchmark_time(m2, x)\n    \n    overhead = (time2 - time1) / time1 * 100\n    \n    print(f\"Config: {num_layers} layers, hidden={hidden}, batch={batch}\")\n    print(f\"WITHOUT checkpoint: {time1:.2f} ms/iter\")\n    print(f\"WITH checkpoint:    {time2:.2f} ms/iter\")\n    print(f\"Overhead: {overhead:.1f}%\")\nelse:\n    print(\"[Run on GPU for benchmarks]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (6.2)\n\nCompute overhead is typically 20-40% depending on model architecture. This is the extra forward passes during backprop.\n\nThe trade-off: 50% memory savings for 30% compute overhead. Worth it when memory is the bottleneck.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 7. Debugging and Best Practices\n\nCheckpointing has gotchas. Here are the common ones and how to avoid them.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 7.1 Pitfall: Non-Differentiable Operations\n\nimport torch\nfrom torch.utils.checkpoint import checkpoint\n\n# BAD: torch.no_grad() inside checkpointed function\ndef bad_forward(x):\n    with torch.no_grad():  # This breaks gradient computation!\n        x = x ** 2\n    return x\n\n# GOOD: Keep everything differentiable\ndef good_forward(x):\n    x = x ** 2  # No torch.no_grad()\n    return x\n\nx = torch.randn(4, requires_grad=True)\n\n# This will fail or give wrong gradients\ntry:\n    y_bad = checkpoint(bad_forward, x, use_reentrant=False)\n    y_bad.sum().backward()\n    print(\"BAD: Gradients might be zero or wrong\")\nexcept Exception as e:\n    print(f\"BAD: Error - {e}\")\n\n# This works\nx = torch.randn(4, requires_grad=True)\ny_good = checkpoint(good_forward, x, use_reentrant=False)\ny_good.sum().backward()\nprint(f\"GOOD: x.grad = {x.grad}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (7.1)\n\nRule: Everything inside a checkpointed function must be differentiable. No `torch.no_grad()`, no detaching tensors, no in-place operations that break autograd.\n\nIf you need non-differentiable ops, do them outside the checkpointed region.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 7.2 Pitfall: Random Operations (Dropout)\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\n# Problem: Dropout uses different random values in forward vs recompute\nclass ModelWithDropout(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(32, 32)\n        self.dropout = nn.Dropout(0.5)\n    \n    def forward(self, x):\n        x = self.linear(x)\n        x = self.dropout(x)  # Random! Different each forward pass\n        return x\n\nmodel = ModelWithDropout()\nx = torch.randn(4, 32, requires_grad=True)\n\n# During training, checkpointed dropout can cause issues\n# because recomputation uses different random mask\n\n# Solution 1: Use deterministic mode (PyTorch 1.11+)\n# checkpoint(..., preserve_rng_state=True)  # Default in newer PyTorch\n\n# Solution 2: Use use_reentrant=False which handles this better\ny = checkpoint(model, x, use_reentrant=False)\nprint(f\"Output shape: {y.shape}\")\nprint(\"use_reentrant=False handles RNG state properly\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (7.2)\n\nDropout and other random ops are tricky. The recomputed forward pass might use different random values than the original.\n\nModern PyTorch (1.11+) with `use_reentrant=False` preserves RNG state. Always use this option.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 7.3 Best Practices Summary\n\nprint(\"\"\"\nBEST PRACTICES FOR ACTIVATION CHECKPOINTING\n============================================\n\n1. USE use_reentrant=False\n   - Modern API, handles edge cases better\n   - checkpoint(fn, x, use_reentrant=False)\n\n2. CHECKPOINT LARGE BLOCKS, NOT SMALL LAYERS\n   - Overhead of checkpoint() call is non-trivial\n   - Group 2-4 layers into blocks, then checkpoint blocks\n\n3. AVOID NESTED CHECKPOINTS\n   - Don't checkpoint inside checkpointed functions\n   - Leads to exponential recomputation\n\n4. KEEP EVERYTHING DIFFERENTIABLE\n   - No torch.no_grad() inside checkpointed regions\n   - No tensor detaching\n   - No in-place ops that break autograd\n\n5. TEST GRADIENTS FIRST\n   - Compare gradients with and without checkpointing\n   - They should be identical (within float precision)\n\n6. PROFILE MEMORY\n   - Use torch.cuda.memory_stats() to verify savings\n   - Peak memory is what matters\n\n7. CONSIDER CHECKPOINT PLACEMENT\n   - Middle layers often have largest activations\n   - Checkpoint those first\n\"\"\")\n\n# Gradient verification example\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nmodel = nn.Linear(64, 64)\nx = torch.randn(8, 64, requires_grad=True)\n\n# Without checkpoint\nx1 = x.clone().detach().requires_grad_(True)\ny1 = model(x1)\ny1.sum().backward()\ngrad1 = x1.grad.clone()\n\n# With checkpoint\nx2 = x.clone().detach().requires_grad_(True)\ny2 = checkpoint(model, x2, use_reentrant=False)\ny2.sum().backward()\ngrad2 = x2.grad.clone()\n\nprint(f\"Gradients match: {torch.allclose(grad1, grad2)}\")\nprint(f\"Max difference: {(grad1 - grad2).abs().max():.2e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (7.3)\n\nGradient verification is essential. If gradients do not match exactly (within float precision), something is wrong with your checkpointing setup. Debug before scaling up.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 8. Integrating with Distributed Training\n\nCheckpointing works with DDP and mixed precision. The combination is powerful.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 8.1 DDP + Checkpointing (Code Template)\n# NOTE: This code shows the pattern. Run in a distributed environment.\n\n\"\"\"\nimport torch\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.checkpoint import checkpoint\n\n# Initialize distributed\ndist.init_process_group(backend=\"nccl\")\nlocal_rank = int(os.environ[\"LOCAL_RANK\"])\ntorch.cuda.set_device(local_rank)\n\nclass ModelWithCheckpoint(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n    \n    def forward(self, x):\n        x = checkpoint(lambda t: torch.relu(self.layer1(t)), x, use_reentrant=False)\n        x = torch.relu(self.layer2(x))\n        return x\n\n# Wrap with DDP\nmodel = ModelWithCheckpoint().cuda(local_rank)\nddp_model = DDP(model, device_ids=[local_rank])\n\n# Training works as normal\nx = torch.randn(16, 1024).cuda(local_rank)\noutput = ddp_model(x)\nloss = output.sum()\nloss.backward()\n\"\"\"\n\nprint(\"DDP + Checkpointing:\")\nprint(\"- Checkpointing happens locally on each GPU\")\nprint(\"- DDP handles gradient synchronization\")\nprint(\"- No special configuration needed\")\nprint(\"- Memory savings apply per-GPU\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 8.2 Mixed Precision + Checkpointing\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer1 = nn.Linear(1024, 1024)\n        self.layer2 = nn.Linear(1024, 1024)\n    \n    def forward(self, x):\n        x = checkpoint(lambda t: torch.relu(self.layer1(t)), x, use_reentrant=False)\n        x = torch.relu(self.layer2(x))\n        return x\n\nif torch.cuda.is_available():\n    model = SimpleModel().cuda()\n    optimizer = torch.optim.Adam(model.parameters())\n    scaler = torch.cuda.amp.GradScaler()\n    \n    # Training loop with mixed precision + checkpointing\n    for step in range(3):\n        x = torch.randn(16, 1024).cuda()\n        \n        with torch.cuda.amp.autocast():\n            output = model(x)\n            loss = output.sum()\n        \n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        print(f\"Step {step}: loss = {loss.item():.4f}\")\n    \n    print(\"\\nMixed precision + checkpointing works seamlessly\")\nelse:\n    print(\"[Run on GPU]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (8.1 & 8.2)\n\nThe power combo: DDP + Mixed Precision + Checkpointing.\n\n- DDP: Scale across GPUs\n- Mixed Precision: 2x memory reduction from fp16\n- Checkpointing: Further memory reduction from recomputation\n\nCombined, you can train models 4-5x larger than baseline. This is how GPT-scale models are trained.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 10. Additional Tools and Resources\n\nBeyond PyTorch's built-in checkpointing, there are libraries that offer more features.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 10.1 DeepSpeed Integration\n\n\"\"\"\nDeepSpeed offers activation checkpointing as part of its ZeRO optimization suite.\n\n# Installation\npip install deepspeed\n\n# Usage\nimport deepspeed\nfrom deepspeed.runtime.activation_checkpointing import checkpointing\n\n# Configure in ds_config.json:\n{\n    \"activation_checkpointing\": {\n        \"partition_activations\": true,\n        \"contiguous_memory_optimization\": true,\n        \"cpu_checkpointing\": true  # Offload to CPU!\n    }\n}\n\n# Key advantage: CPU offloading\n# DeepSpeed can move checkpointed activations to CPU memory,\n# freeing GPU memory for even larger models.\n\"\"\"\n\nprint(\"DeepSpeed Activation Checkpointing:\")\nprint(\"- Automatic checkpoint placement\")\nprint(\"- CPU offloading for extreme memory savings\")\nprint(\"- Integrated with ZeRO optimizer stages\")\nprint(\"- Best for very large models (billions of parameters)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 10.2 FairScale Integration\n\n\"\"\"\nFairScale (from Meta) provides checkpoint_wrapper for easy integration.\n\n# Installation\npip install fairscale\n\n# Usage\nfrom fairscale.nn.checkpoint import checkpoint_wrapper\n\n# Wrap any module\nlayer = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU())\ncheckpointed_layer = checkpoint_wrapper(layer)\n\n# Use in model\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.block1 = checkpoint_wrapper(nn.Sequential(...))\n        self.block2 = checkpoint_wrapper(nn.Sequential(...))\n    \n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        return x\n\"\"\"\n\nprint(\"FairScale checkpoint_wrapper:\")\nprint(\"- Clean API: wrap modules directly\")\nprint(\"- Works well with FSDP (Fully Sharded Data Parallel)\")\nprint(\"- Good for medium-scale training\")\nprint(\"- Simpler than DeepSpeed for many use cases\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Impressions/Conclusions (10.1 & 10.2)\n\nTool selection guide:\n- **PyTorch native**: Simple models, full control, no dependencies\n- **FairScale**: Medium scale, clean API, FSDP integration\n- **DeepSpeed**: Billion+ parameter models, CPU offloading, full optimization suite\n\nStart with PyTorch native. Move to DeepSpeed when you hit limits.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 12. Conclusion\n\nActivation checkpointing is a memory-compute trade-off. It trades extra forward passes for reduced memory usage. The trade-off is usually worth it.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 12.1 Key Takeaways\n\nprint(\"\"\"\nKEY TAKEAWAYS\n=============\n\n1. ACTIVATIONS DOMINATE MEMORY\n   - Not parameters, not gradients\n   - Scales with batch_size * depth * hidden_size\n\n2. CHECKPOINTING TRADES COMPUTE FOR MEMORY\n   - Save some activations (checkpoints)\n   - Recompute others during backward pass\n   - Typical: 30-50% memory savings, 20-40% compute overhead\n\n3. THE SQRT(N) RULE\n   - Optimal checkpointing: O(sqrt(n)) memory for n layers\n   - 100 layers -> ~10 checkpoints -> 10x memory reduction\n\n4. PRACTICAL PATTERNS\n   - checkpoint(): Single modules/functions\n   - checkpoint_sequential(): Sequential models\n   - Always use use_reentrant=False\n\n5. COMBINE WITH OTHER TECHNIQUES\n   - Mixed precision: 2x memory from fp16\n   - Gradient accumulation: Effective larger batches\n   - DDP: Scale across GPUs\n   - Together: Train 4-5x larger models\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 12.2 Decision Framework: When to Use Checkpointing\n\nprint(\"\"\"\nWHEN TO USE ACTIVATION CHECKPOINTING\n====================================\n\nUSE IT WHEN:\n- You are hitting OOM errors\n- You want larger batch sizes\n- Your model has 10+ layers\n- Training time is less critical than memory\n- You are training transformers or deep CNNs\n\nSKIP IT WHEN:\n- Model fits comfortably in memory\n- Training time is the bottleneck (not memory)\n- Model is shallow (< 5 layers)\n- You need maximum training speed\n\nQUICK DECISION TREE:\n\n  OOM Error?\n     |\n     v\n  YES --> Use checkpointing\n     |\n     v\n  Want larger batches?\n     |\n     v\n  YES --> Use checkpointing\n     |\n     v\n  Model > 10 layers?\n     |\n     v\n  YES --> Consider checkpointing\n     |\n     v\n  NO --> Probably skip it\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Final Impressions\n\nActivation checkpointing is not magic. It is a simple trade-off executed well.\n\nYou now understand:\n- Why activations dominate memory (batch × depth × hidden)\n- How checkpointing works (recompute instead of store)\n- When to use it (memory-bound, deep models)\n- How to implement it (checkpoint, checkpoint_sequential, use_reentrant=False)\n\nThe technique is simple. The impact is massive. Every large language model you have heard of uses some form of activation checkpointing.\n\nGo train something bigger.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}