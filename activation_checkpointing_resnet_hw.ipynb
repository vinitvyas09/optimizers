{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Checkpointing: A First Principles Deep Dive\n",
    "\n",
    "Memory is the bottleneck in deep learning. Not compute. Not data. Memory.\n",
    "\n",
    "This notebook tears apart activation checkpointing from first principles. You will understand exactly why activations dominate memory, how checkpointing trades compute for memory, and when this trade-off makes sense.\n",
    "\n",
    "---\n",
    "\n",
    "## The Core Problem\n",
    "\n",
    "During training you do two things:\n",
    "1. **Forward pass**: compute outputs layer by layer\n",
    "2. **Backward pass**: compute gradients using the chain rule\n",
    "\n",
    "Here's what kills you: backward needs intermediate values from forward. If forward does `y = sin(x)`, backward needs `x` to compute `dy/dx = cos(x)`. PyTorch keeps `x` alive until backward reaches that node. These are \"saved tensors\" in autograd-speak.\n",
    "\n",
    "So activation memory accumulates through forward and peaks at the start of backward. You've got this pile of \"will need later\" tensors sitting in VRAM, waiting.\n",
    "\n",
    "## What Checkpointing Actually Does\n",
    "\n",
    "Think of training like hiking a trail:\n",
    "\n",
    "**Normal training**: You drop breadcrumbs at every step (store all activations). Easy to retrace (backprop), but you're carrying a giant breadcrumb bag (VRAM).\n",
    "\n",
    "**Checkpointing**: You only drop breadcrumbs at a few checkpoints. On the way back, when you need details between checkpoints, you re-walk that segment.\n",
    "\n",
    "That's literally it. Save fewer tensors in forward, recompute them on demand during backward. Memory goes down, compute goes up.\n",
    "\n",
    "## Concrete Example\n",
    "\n",
    "Suppose your forward is:\n",
    "\n",
    "```\n",
    "x --> [f1] --> a --> [f2] --> b --> [f3] --> y\n",
    "```\n",
    "\n",
    "**No checkpointing**: autograd saves `a` and `b` so backward can compute gradients.\n",
    "\n",
    "**Checkpoint after `a`**: you keep `a`, but you don't keep `b`. During backward, when you need `b` to differentiate `f3`, PyTorch reruns `f2(a)` to get it back.\n",
    "\n",
    "Memory goes down (you didn't store `b`). Compute goes up (you recomputed it).\n",
    "\n",
    "## PyTorch API\n",
    "\n",
    "```python\n",
    "out = checkpoint(fn, *args, use_reentrant=False)\n",
    "```\n",
    "\n",
    "What happens:\n",
    "- **Forward**: runs `fn(*args)` but doesn't save intermediates for backward; only keeps `args`\n",
    "- **Backward**: reruns `fn(*args)` to recreate intermediates, then backprops through\n",
    "\n",
    "This is why checkpointing is easiest at \"block\" granularity - Transformer blocks, ResNet blocks, etc.\n",
    "\n",
    "---\n",
    "\n",
    "Every code cell below has an Impressions/Conclusions section. These are not summaries. They are insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Introduction\n",
    "\n",
    "Training deep neural networks hits a wall. That wall is GPU memory.\n",
    "\n",
    "You have a model. You have data. You have compute. But your GPU runs out of memory. Why?\n",
    "\n",
    "Four things consume GPU memory during training:\n",
    "1. **Model parameters** (weights and biases)\n",
    "2. **Gradients** (same size as parameters)\n",
    "3. **Optimizer states** (for Adam: 2x parameter size for m and v buffers)\n",
    "4. **Activations** (intermediate outputs from each layer)\n",
    "\n",
    "For Adam/AdamW, the first three combined are 4x the model size (params + grads + 2 momentum buffers). This is fixed cost.\n",
    "\n",
    "Here is the surprise: activations often dominate anyway. Not parameters. Not gradients. Not optimizer states. Activations. Why? Because activations scale with batch size. Everything else does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 10,496,000 parameters\n",
      "\n",
      "THEORETICAL memory breakdown for full training with Adam:\n",
      "------------------------------------------------------------\n",
      "Parameter memory:  40.04 MB (always allocated)\n",
      "Gradient memory:   40.04 MB (allocated during backward)\n",
      "Optimizer memory:  80.08 MB (Adam: m + v, allocated on first step)\n",
      "Fixed cost total:  160.16 MB\n",
      "\n",
      "Activation memory by batch size (THEORETICAL):\n",
      "------------------------------------------------------------\n",
      "Batch  32: activations =   1.25 MB (0.8% of total)\n",
      "Batch  64: activations =   2.50 MB (1.5% of total)\n",
      "Batch 128: activations =   5.00 MB (3.0% of total)\n",
      "Batch 256: activations =  10.00 MB (5.9% of total)\n",
      "Batch 512: activations =  20.00 MB (11.1% of total)\n",
      "\n",
      "NOTE: These are theoretical estimates. See Section 1.2 for actual GPU measurements.\n"
     ]
    }
   ],
   "source": [
    "# 1.1 The Memory Breakdown: Where Does Memory Go?\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def bytes_to_mb(b):\n",
    "    return b / (1024 * 1024)\n",
    "\n",
    "class DeepNetwork(nn.Module):\n",
    "    def __init__(self, input_size=1024, hidden_size=1024, num_layers=10):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_f = input_size if i == 0 else hidden_size\n",
    "            layers.append(nn.Linear(in_f, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = DeepNetwork(input_size=1024, hidden_size=1024, num_layers=10)\n",
    "num_params = count_parameters(model)\n",
    "\n",
    "# THEORETICAL memory breakdown for a full training setup with Adam\n",
    "# These are projections, not actual allocations in this code\n",
    "param_mem = num_params * 4  # fp32 parameters\n",
    "grad_mem = param_mem        # gradients (allocated during backward)\n",
    "optimizer_mem = param_mem * 2  # Adam's m and v buffers (allocated when optimizer.step() is called)\n",
    "\n",
    "print(f\"Model: {num_params:,} parameters\")\n",
    "print()\n",
    "print(\"THEORETICAL memory breakdown for full training with Adam:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Parameter memory:  {bytes_to_mb(param_mem):.2f} MB (always allocated)\")\n",
    "print(f\"Gradient memory:   {bytes_to_mb(grad_mem):.2f} MB (allocated during backward)\")\n",
    "print(f\"Optimizer memory:  {bytes_to_mb(optimizer_mem):.2f} MB (Adam: m + v, allocated on first step)\")\n",
    "print(f\"Fixed cost total:  {bytes_to_mb(param_mem + grad_mem + optimizer_mem):.2f} MB\")\n",
    "print()\n",
    "print(\"Activation memory by batch size (THEORETICAL):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "fixed_cost = param_mem + grad_mem + optimizer_mem\n",
    "for batch_size in [32, 64, 128, 256, 512]:\n",
    "    # Activation estimate: batch_size * hidden_size * num_layers * 4 bytes\n",
    "    # This is simplified - actual activation memory depends on what PyTorch saves\n",
    "    act_mem = batch_size * 1024 * 10 * 4\n",
    "    total = fixed_cost + act_mem\n",
    "    act_pct = (act_mem / total) * 100\n",
    "    print(f\"Batch {batch_size:3d}: activations = {bytes_to_mb(act_mem):6.2f} MB ({act_pct:.1f}% of total)\")\n",
    "\n",
    "print()\n",
    "print(\"NOTE: These are theoretical estimates. See Section 1.2 for actual GPU measurements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (1.1)\n",
    "\n",
    "This cell shows **theoretical** memory projections for a full training setup. No optimizer or gradients are actually allocated yet - we're just computing what the memory breakdown *would be*.\n",
    "\n",
    "The theoretical fixed costs for Adam/AdamW:\n",
    "- Parameters: 1x model size (always allocated)\n",
    "- Gradients: 1x model size (allocated during backward pass)\n",
    "- Optimizer states (m, v): 2x model size (allocated on first optimizer.step())\n",
    "- Total fixed: 4x model size\n",
    "\n",
    "At small batches, fixed costs dominate. At large batches, activations dominate. This crossover point depends on model depth and hidden size.\n",
    "\n",
    "The math:\n",
    "- Fixed cost: O(model_size)\n",
    "- Activations: O(batch_size × depth × hidden_size)\n",
    "\n",
    "Batch size is the multiplier that kills you. Checkpointing targets activations because that is where the scaling problem lives.\n",
    "\n",
    "**Key distinction**: Section 1.2 shows *actual* GPU memory measurements during real forward/backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Real GPU Memory Measurement\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepNetwork(nn.Module):\n",
    "    \"\"\"Deliberately shallow but wide to make activations dominate over parameters.\"\"\"\n",
    "    def __init__(self, input_size=1024, hidden_size=4096, num_layers=6):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_f = input_size if i == 0 else hidden_size\n",
    "            layers.append(nn.Linear(in_f, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    model = DeepNetwork().cuda()\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    param_mem = num_params * 4 / 1e6\n",
    "    print(f\"Model parameters: {num_params:,} ({param_mem:.1f} MB)\")\n",
    "    print()\n",
    "    \n",
    "    # Use larger batch sizes to show activation scaling\n",
    "    for batch_size in [64, 128, 256, 512]:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        x = torch.randn(batch_size, 1024).cuda()\n",
    "        output = model(x)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        peak = torch.cuda.max_memory_allocated() / 1e6\n",
    "        # Estimate activation memory: batch * hidden * num_layers * 4 bytes\n",
    "        act_estimate = batch_size * 4096 * 6 * 4 / 1e6\n",
    "        print(f\"Batch {batch_size:3d}: Peak = {peak:.1f} MB  (est. activations: {act_estimate:.1f} MB)\")\n",
    "        \n",
    "        model.zero_grad()\n",
    "        del x, output, loss\n",
    "else:\n",
    "    print(\"[Run on GPU to see measurements]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (1.2)\n",
    "\n",
    "Activation memory scales linearly with batch size. But whether you *see* this depends on the ratio of activations to fixed costs (parameters + gradients).\n",
    "\n",
    "**The math:**\n",
    "- Fixed cost: `num_params × 4 bytes × 2` (weights + gradients)\n",
    "- Activation cost: `batch × hidden × depth × 4 bytes`\n",
    "\n",
    "With a 100M parameter model and batch size 32, fixed costs dwarf activations. You won't see much scaling. But as batch size grows, activations eventually dominate.\n",
    "\n",
    "This is exactly what activation checkpointing targets. At the batch sizes where you're actually memory-constrained, activations are the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 134.3M params\n",
      "\n",
      "Memory Timeline:\n",
      "-------------------------------------------------------\n",
      "Model loaded:                          895.5 MB\n",
      "\n",
      "During forward (activations accumulating):\n",
      "  After layer 1:                           899.7 MB  (+4.2)\n",
      "  After layer 2:                           903.9 MB  (+4.2)\n",
      "  After layer 3:                           908.1 MB  (+4.2)\n",
      "  After layer 4:                           912.3 MB  (+4.2)\n",
      "  After layer 5:                           916.5 MB  (+4.2)\n",
      "  After layer 6:                           920.7 MB  (+4.2)\n",
      "  After layer 7:                           924.9 MB  (+4.2)\n",
      "  After layer 8:                           929.1 MB  (+4.2)\n",
      "\n",
      "After forward (all activations live):    928.5 MB\n",
      "PEAK (activations + gradients):       1104.7 MB  <-- this is what kills you\n",
      "After backward (gradients allocated):   1100.5 MB\n",
      "After zero_grad:                       563.5 MB\n",
      "\n",
      "Activation memory: ~33.0 MB\n",
      "Gradient memory:   ~537.0 MB\n"
     ]
    }
   ],
   "source": [
    "# 1.3 When Does Memory Peak? The Critical Insight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InstrumentedNetwork(nn.Module):\n",
    "    \"\"\"Network that tracks memory at each layer.\"\"\"\n",
    "    def __init__(self, num_layers=5, hidden=4096):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(hidden, hidden) for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, track=False):\n",
    "        memory_log = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = torch.relu(layer(x))\n",
    "            if track and torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "                memory_log.append(torch.cuda.memory_allocated() / 1e6)\n",
    "        return x, memory_log\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Use larger batch and hidden size to make activations visible\n",
    "    model = InstrumentedNetwork(num_layers=8, hidden=4096).cuda()\n",
    "    x = torch.randn(256, 4096).cuda()\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model: {num_params/1e6:.1f}M params\")\n",
    "    print()\n",
    "    \n",
    "    # Baseline: just model loaded\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    mem_model_only = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    # Track memory during forward\n",
    "    output, fwd_mem = model(x, track=True)\n",
    "    mem_after_forward = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    # Now trigger backward\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1e6\n",
    "    mem_after_backward = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    # After clearing gradients\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    mem_after_zero_grad = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    print(\"Memory Timeline:\")\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"{'Model loaded:':<35} {mem_model_only:>8.1f} MB\")\n",
    "    print()\n",
    "    print(\"During forward (activations accumulating):\")\n",
    "    for i, mem in enumerate(fwd_mem):\n",
    "        delta = mem - (fwd_mem[i-1] if i > 0 else mem_model_only)\n",
    "        print(f\"  After layer {i+1}:{'':<23} {mem:>8.1f} MB  (+{delta:.1f})\")\n",
    "    print()\n",
    "    print(f\"{'After forward (all activations live):':<35} {mem_after_forward:>8.1f} MB\")\n",
    "    print(f\"{'PEAK (activations + gradients):':<35} {peak_memory:>8.1f} MB  <-- this is what kills you\")\n",
    "    print(f\"{'After backward (gradients allocated):':<35} {mem_after_backward:>8.1f} MB\")\n",
    "    print(f\"{'After zero_grad:':<35} {mem_after_zero_grad:>8.1f} MB\")\n",
    "    print()\n",
    "    \n",
    "    activation_mem = mem_after_forward - mem_model_only\n",
    "    grad_mem = mem_after_backward - mem_after_zero_grad\n",
    "    print(f\"Activation memory: ~{activation_mem:.1f} MB\")\n",
    "    print(f\"Gradient memory:   ~{grad_mem:.1f} MB\")\n",
    "else:\n",
    "    print(\"[Run on GPU to see memory timeline]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (1.3)\n",
    "\n",
    "The memory timeline tells the story:\n",
    "\n",
    "1. **Model loaded**: Just weights in memory\n",
    "2. **Forward pass**: Memory grows as activations accumulate (each layer adds `batch × hidden × 4 bytes`)\n",
    "3. **Peak**: Occurs during backward when you have BOTH activations (not yet freed) AND gradients (being allocated)\n",
    "4. **After backward**: Activations freed, but gradients remain (attached to parameters)\n",
    "5. **After zero_grad**: Back to just weights\n",
    "\n",
    "The peak is what causes OOM. At that moment, you're holding activations AND gradients simultaneously. Checkpointing attacks this by not storing activations during forward—so when backward starts, you only have checkpoints + gradients, not the full activation pile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Understanding Activation Checkpointing\n",
    "\n",
    "Activation checkpointing is a memory-compute trade-off. Save memory by not storing activations. Pay for it by recomputing them during backprop.\n",
    "\n",
    "To understand this, you need to know what activations are and why backprop needs them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What Are Activations?\n",
    "\n",
    "An activation is the output of a layer. That is it.\n",
    "\n",
    "```\n",
    "Input x -> [Layer 1] -> a1 -> [Layer 2] -> a2 -> [Layer 3] -> a3 -> Output\n",
    "```\n",
    "\n",
    "`a1`, `a2`, `a3` are activations. Each is a tensor stored in memory.\n",
    "\n",
    "Why store them? Backpropagation needs them to compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored for backprop:\n",
      "  z1: torch.Size([1, 4]) - needed to compute relu gradient\n",
      "  a1: torch.Size([1, 4]) - needed to compute W2 gradient\n",
      "\n",
      "Gradients computed:\n",
      "  W1.grad: torch.Size([4, 4])\n",
      "  W2.grad: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Why Backprop Needs Activations\n",
    "\n",
    "import torch\n",
    "\n",
    "# y = W2 * relu(W1 * x)\n",
    "# Gradient w.r.t. W1 needs the pre-activation W1*x\n",
    "# to compute relu'(W1*x)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "W1 = torch.randn(4, 4, requires_grad=True)\n",
    "W2 = torch.randn(4, 4, requires_grad=True)\n",
    "x = torch.randn(1, 4)\n",
    "\n",
    "# Forward pass stores these\n",
    "z1 = x @ W1.T           # Pre-activation (needed for ReLU grad)\n",
    "a1 = torch.relu(z1)     # Activation (needed for W2 grad)\n",
    "y = a1 @ W2.T\n",
    "\n",
    "print(\"Stored for backprop:\")\n",
    "print(f\"  z1: {z1.shape} - needed to compute relu gradient\")\n",
    "print(f\"  a1: {a1.shape} - needed to compute W2 gradient\")\n",
    "\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradients computed:\")\n",
    "print(f\"  W1.grad: {W1.grad.shape}\")\n",
    "print(f\"  W2.grad: {W2.grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (2.1)\n",
    "\n",
    "The chain rule requires intermediate values. Backprop walks backward through the computation graph. At each step, it needs forward pass values.\n",
    "\n",
    "For linear `y = Wx`: gradient w.r.t. W needs x.\n",
    "\n",
    "For ReLU `y = relu(x)`: gradient needs to know where x > 0.\n",
    "\n",
    "This is why activations are stored. This is what checkpointing eliminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Training:\n",
      "  Forward: x -> a1 -> a2 -> a3 -> loss\n",
      "           [store] [store] [store]\n",
      "  Backward: uses stored a1, a2, a3\n",
      "  Memory: O(n) activations\n",
      "\n",
      "With Checkpointing:\n",
      "  Forward: x -> a1 -> a2 -> a3 -> loss\n",
      "           [save]       [save]\n",
      "  Backward: recompute a2 from a1, then use\n",
      "  Memory: O(sqrt(n)) with optimal placement\n",
      "\n",
      "For 100 layers:\n",
      "  Standard: 100 activations stored\n",
      "  Checkpointed: ~10 activations stored (at checkpoint boundaries)\n",
      "  Memory reduction: 10x\n",
      "\n",
      "Compute analysis:\n",
      "  Without checkpointing: n forward + n backward = 2n\n",
      "  With checkpointing:    n forward + n backward + n recompute = 3n\n",
      "  Compute overhead: ~50% of total training time\n"
     ]
    }
   ],
   "source": [
    "# 2.2 The Core Trade-off: Store vs Recompute\n",
    "\n",
    "import math\n",
    "\n",
    "print(\"Standard Training:\")\n",
    "print(\"  Forward: x -> a1 -> a2 -> a3 -> loss\")\n",
    "print(\"           [store] [store] [store]\")\n",
    "print(\"  Backward: uses stored a1, a2, a3\")\n",
    "print(\"  Memory: O(n) activations\")\n",
    "print()\n",
    "print(\"With Checkpointing:\")\n",
    "print(\"  Forward: x -> a1 -> a2 -> a3 -> loss\")\n",
    "print(\"           [save]       [save]\")\n",
    "print(\"  Backward: recompute a2 from a1, then use\")\n",
    "print(\"  Memory: O(sqrt(n)) with optimal placement\")\n",
    "print()\n",
    "\n",
    "num_layers = 100\n",
    "optimal = int(math.sqrt(num_layers))\n",
    "\n",
    "# With sqrt(n) checkpoints, we have sqrt(n) segments of sqrt(n) layers each\n",
    "# During backward, each segment must be recomputed once\n",
    "# Recompute cost = sqrt(n) segments × sqrt(n) layers = n layer computations\n",
    "# This equals one additional forward pass worth of compute\n",
    "\n",
    "print(f\"For {num_layers} layers:\")\n",
    "print(f\"  Standard: {num_layers} activations stored\")\n",
    "print(f\"  Checkpointed: ~{optimal} activations stored (at checkpoint boundaries)\")\n",
    "print(f\"  Memory reduction: {num_layers/optimal:.0f}x\")\n",
    "print()\n",
    "print(\"Compute analysis:\")\n",
    "print(f\"  Without checkpointing: n forward + n backward = 2n\")\n",
    "print(f\"  With checkpointing:    n forward + n backward + n recompute = 3n\")\n",
    "print(f\"  Compute overhead: ~50% of total training time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (2.2)\n",
    "\n",
    "The sqrt(n) rule: optimal checkpointing reduces memory from O(n) to O(sqrt(n)).\n",
    "\n",
    "For 100 layers:\n",
    "- Standard: 100 activations stored\n",
    "- Checkpointed: ~10 activations stored (at checkpoint boundaries)\n",
    "- Memory reduction: 10x\n",
    "\n",
    "The compute cost is real:\n",
    "- Without checkpointing: Forward (n) + Backward (n) = 2n operations\n",
    "- With checkpointing: Forward (n) + Backward (n) + Recompute (n) = 3n operations\n",
    "- **Overhead: ~50% of total training time**\n",
    "\n",
    "This is still an excellent trade-off: 10x memory reduction for 50% compute overhead. When memory is the bottleneck (and it usually is), this lets you train models that otherwise wouldn't fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Prerequisites\n",
    "\n",
    "Environment setup and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.0+cu126\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Environment Check\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (3.1)\n",
    "\n",
    "PyTorch 1.9+ required for full checkpointing support. `torch.utils.checkpoint` provides the core API. CUDA strongly recommended since checkpointing shines when GPU memory is the bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilities loaded: MemoryTracker, track_memory()\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Memory Profiling Utilities\n",
    "\n",
    "import torch\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class MemoryTracker:\n",
    "    \"\"\"Track GPU memory usage.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.snapshots = []\n",
    "    \n",
    "    def snapshot(self, label=\"\"):\n",
    "        if torch.cuda.is_available():\n",
    "            mem = torch.cuda.memory_allocated() / 1e6\n",
    "            self.snapshots.append({'label': label, 'mb': mem})\n",
    "            return mem\n",
    "        return 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.snapshots = []\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def peak_mb(self):\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.max_memory_allocated() / 1e6\n",
    "        return 0\n",
    "    \n",
    "    def report(self):\n",
    "        for s in self.snapshots:\n",
    "            print(f\"{s['label']:30s}: {s['mb']:8.2f} MB\")\n",
    "        print(f\"{'Peak':30s}: {self.peak_mb():8.2f} MB\")\n",
    "\n",
    "@contextmanager\n",
    "def track_memory(label=\"Op\"):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "    yield\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"{label}: Peak = {torch.cuda.max_memory_allocated()/1e6:.2f} MB\")\n",
    "\n",
    "print(\"Utilities loaded: MemoryTracker, track_memory()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (3.2)\n",
    "\n",
    "Key functions:\n",
    "- `torch.cuda.memory_allocated()`: current memory in use\n",
    "- `torch.cuda.max_memory_allocated()`: peak since last reset\n",
    "- `torch.cuda.empty_cache()`: free cached memory\n",
    "\n",
    "Peak memory is what matters. That is what causes OOM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. How Activation Checkpointing Works\n",
    "\n",
    "PyTorch provides `torch.utils.checkpoint`. Two main functions:\n",
    "- `checkpoint`: wrap a single function/module\n",
    "- `checkpoint_sequential`: wrap a sequence of modules\n",
    "\n",
    "The API is simple. The magic is in what happens under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak Memory WITHOUT Checkpointing: 567.70 MB\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Model WITHOUT Checkpointing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1024, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = SimpleModel().cuda()\n",
    "    x = torch.randn(1, 1024).cuda()\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    output = model(x)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"Peak Memory WITHOUT Checkpointing: {torch.cuda.max_memory_allocated() / 1e6:.2f} MB\")\n",
    "else:\n",
    "    print(\"[Run on GPU]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak Memory WITH Checkpointing: 43.30 MB\n"
     ]
    }
   ],
   "source": [
    "# 4.2 Model WITH Checkpointing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class CheckpointedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1024, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # NOTE: checkpoint() is most useful on *intermediate* blocks where the input\n",
    "        # activation already requires grad (i.e., it comes from earlier layers).\n",
    "        x = torch.relu(self.layer1(x))\n",
    "\n",
    "        # Checkpoint layer2: its internal activations won't be stored.\n",
    "        # They will be recomputed during backward.\n",
    "        x = checkpoint(lambda t: torch.relu(self.layer2(t)), x, use_reentrant=False)\n",
    "        return x\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = CheckpointedModel().cuda()\n",
    "    x = torch.randn(1, 1024).cuda()\n",
    "\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    output = model(x)\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    print(f\"Peak Memory WITH Checkpointing: {torch.cuda.max_memory_allocated() / 1e6:.2f} MB\")\n",
    "else:\n",
    "    print(\"[Run on GPU]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (4.1 & 4.2)\n",
    "\n",
    "The difference is subtle in small models. With 2 layers, memory savings are minimal because the baseline is already small.\n",
    "\n",
    "The real benefit appears at scale. Let us test with a deeper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4.3 Deep Model Comparison: The Real Difference\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nclass DeepModelNoCheckpoint(nn.Module):\n    def __init__(self, num_layers=20, hidden=2048):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Linear(hidden, hidden) for _ in range(num_layers)\n        ])\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = torch.relu(layer(x))\n        return x\n\nclass DeepModelWithCheckpoint(nn.Module):\n    \"\"\"Checkpoints SEGMENTS of layers, not individual layers.\n    \n    Per-layer checkpointing doesn't help: each checkpoint() saves its input,\n    which is the output of the previous layer. You still store all activations.\n    \n    Segment checkpointing: group N layers, only save input to each segment.\n    Memory: O(num_segments) instead of O(num_layers).\n    \"\"\"\n    def __init__(self, num_layers=20, hidden=2048, segment_size=5):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Linear(hidden, hidden) for _ in range(num_layers)\n        ])\n        self.segment_size = segment_size\n    \n    def _run_segment(self, x, start_idx, end_idx):\n        \"\"\"Run a segment of layers.\"\"\"\n        for i in range(start_idx, end_idx):\n            x = torch.relu(self.layers[i](x))\n        return x\n    \n    def forward(self, x):\n        num_layers = len(self.layers)\n        for start in range(0, num_layers, self.segment_size):\n            end = min(start + self.segment_size, num_layers)\n            # Checkpoint each segment: only the segment INPUT is saved\n            x = checkpoint(\n                self._run_segment, x, start, end,\n                use_reentrant=False\n            )\n        return x\n\nif torch.cuda.is_available():\n    batch_size = 128\n    hidden = 2048\n    num_layers = 20\n    \n    # Without checkpointing\n    torch.cuda.empty_cache()\n    model1 = DeepModelNoCheckpoint(num_layers, hidden).cuda()\n    x1 = torch.randn(batch_size, hidden).cuda()\n    \n    torch.cuda.reset_peak_memory_stats()\n    out1 = model1(x1)\n    out1.sum().backward()\n    mem_no_ckpt = torch.cuda.max_memory_allocated() / 1e6\n    \n    del model1, x1, out1\n    torch.cuda.empty_cache()\n    \n    # With segment checkpointing (4 segments of 5 layers each)\n    model2 = DeepModelWithCheckpoint(num_layers, hidden, segment_size=5).cuda()\n    x2 = torch.randn(batch_size, hidden).cuda()\n    \n    torch.cuda.reset_peak_memory_stats()\n    out2 = model2(x2)\n    out2.sum().backward()\n    mem_ckpt = torch.cuda.max_memory_allocated() / 1e6\n    \n    print(f\"Config: {num_layers} layers, hidden={hidden}, batch={batch_size}\")\n    print(f\"Segment size: 5 layers (4 checkpoints total)\")\n    print()\n    print(f\"WITHOUT checkpointing: {mem_no_ckpt:.2f} MB\")\n    print(f\"WITH checkpointing:    {mem_ckpt:.2f} MB\")\n    print(f\"Memory saved: {mem_no_ckpt - mem_ckpt:.2f} MB ({(1 - mem_ckpt/mem_no_ckpt)*100:.1f}%)\")\nelse:\n    print(\"[Run on GPU]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Impressions/Conclusions (4.3)\n\n**Critical insight: checkpoint granularity matters.**\n\nPer-layer checkpointing (wrapping each layer individually) doesn't save memory. Why? Each `checkpoint()` call saves its input. If you checkpoint layer 2, you save the output of layer 1. Checkpoint layer 3, save output of layer 2. You're still storing all intermediate activations.\n\n**Segment checkpointing** is the fix: group N layers together, checkpoint the group. Now you only save the input to each segment, not every layer's output.\n\nWith 20 layers split into 4 segments of 5:\n- Without checkpointing: store 20 activations\n- With checkpointing: store 4 activations (segment inputs) + recompute 5 layers during backward per segment\n\nThe trade-off: fewer stored activations, but each segment's layers run twice (once in forward, once in backward)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Code Examples\n",
    "\n",
    "Three patterns you will use:\n",
    "1. Basic `checkpoint()` for single modules\n",
    "2. `checkpoint_sequential()` for sequential models\n",
    "3. Manual checkpointing in complex architectures (transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Basic Usage\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU())\n",
    "        self.block2 = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run block1 normally, then checkpoint block2.\n",
    "        # This ensures the checkpointed block receives an activation that requires grad.\n",
    "        x = self.block1(x)\n",
    "        x = checkpoint(self.block2, x, use_reentrant=False)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "x = torch.randn(1, 1024)\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "\n",
    "output = model(x)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"Checkpointing applied to block2. Block1 runs normally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (5.1)\n",
    "\n",
    "`checkpoint(fn, *args)` wraps any callable. During forward, it runs `fn(*args)` but does not save intermediate activations. During backward, it re-runs `fn(*args)` to recompute them.\n",
    "\n",
    "Note: `use_reentrant=False` is the modern API. It handles edge cases better than the old default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Using checkpoint_sequential\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "# A deep sequential model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1024, 1024), nn.ReLU(),\n",
    "    nn.Linear(1024, 1024), nn.ReLU(),\n",
    "    nn.Linear(1024, 1024), nn.ReLU(),\n",
    "    nn.Linear(1024, 1024), nn.ReLU(),\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 1024)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    x = x.cuda()\n",
    "\n",
    "# Split into 2 checkpoint segments\n",
    "# Each segment will be checkpointed separately\n",
    "segments = 2\n",
    "output = checkpoint_sequential(model, segments, x, use_reentrant=False)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Model split into {segments} checkpoint segments\")\n",
    "print(\"Each segment recomputes activations during backward pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (5.2)\n",
    "\n",
    "`checkpoint_sequential` is convenient for nn.Sequential models. The `segments` parameter controls granularity:\n",
    "- More segments = less memory, more recomputation\n",
    "- Fewer segments = more memory, less recomputation\n",
    "\n",
    "Rule of thumb: start with sqrt(num_layers) segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Transformer Training with Checkpointing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_size * 4, embed_size)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        # Feed-forward with residual (checkpointed)\n",
    "        ff_out = checkpoint(self.feed_forward, x, use_reentrant=False)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "embed_size = 512\n",
    "seq_length = 64\n",
    "batch_size = 8\n",
    "\n",
    "block = TransformerBlock(embed_size)\n",
    "if torch.cuda.is_available():\n",
    "    block = block.cuda()\n",
    "\n",
    "x = torch.randn(batch_size, seq_length, embed_size)\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "\n",
    "output = block(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"Feed-forward block is checkpointed (it uses the most memory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (5.3)\n",
    "\n",
    "In transformers, the feed-forward block uses 4x the hidden size. This is where most activation memory goes. Checkpointing the FFN is the standard practice in large language models.\n",
    "\n",
    "Why not checkpoint attention? You can, but attention has complex intermediate states. The memory-compute trade-off is less favorable there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 ResNet with Selective Checkpointing (Bonus)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"ResNet basic block with optional checkpointing.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return torch.relu(out)\n",
    "\n",
    "class MiniResNet(nn.Module):\n",
    "    \"\"\"Small ResNet with checkpointing options.\"\"\"\n",
    "    def __init__(self, num_blocks=4, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            BasicBlock(64 if i == 0 else 128, 128, stride=2 if i == 0 else 1)\n",
    "            for i in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint(block, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = block(x)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Compare memory\n",
    "if torch.cuda.is_available():\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Without checkpoint\n",
    "    torch.cuda.empty_cache()\n",
    "    model1 = MiniResNet(num_blocks=8, use_checkpoint=False).cuda()\n",
    "    x1 = torch.randn(batch_size, 3, 32, 32).cuda()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out1 = model1(x1)\n",
    "    out1.sum().backward()\n",
    "    mem1 = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    del model1, x1, out1\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # With checkpoint\n",
    "    model2 = MiniResNet(num_blocks=8, use_checkpoint=True).cuda()\n",
    "    x2 = torch.randn(batch_size, 3, 32, 32).cuda()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out2 = model2(x2)\n",
    "    out2.sum().backward()\n",
    "    mem2 = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    print(f\"ResNet with 8 blocks, batch={batch_size}\")\n",
    "    print(f\"WITHOUT checkpoint: {mem1:.2f} MB\")\n",
    "    print(f\"WITH checkpoint:    {mem2:.2f} MB\")\n",
    "    print(f\"Savings: {(1 - mem2/mem1)*100:.1f}%\")\n",
    "else:\n",
    "    print(\"[Run on GPU]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (5.4)\n",
    "\n",
    "CNNs benefit from checkpointing too. Each residual block stores feature maps that can be recomputed. For deeper ResNets (ResNet-152), checkpointing is essential to train with reasonable batch sizes.\n",
    "\n",
    "Key pattern: wrap entire blocks, not individual layers. The overhead of checkpoint calls adds up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Performance Benchmarks\n",
    "\n",
    "Numbers matter. Let us measure:\n",
    "1. Memory savings across model sizes\n",
    "2. Compute overhead (training time)\n",
    "3. Batch size scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Memory Savings Across Model Depths\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import time\n",
    "\n",
    "def create_model(num_layers, hidden, use_checkpoint=False):\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.ModuleList([\n",
    "                nn.Linear(hidden, hidden) for _ in range(num_layers)\n",
    "            ])\n",
    "            self.use_ckpt = use_checkpoint\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Avoid checkpointing the very first layer: its input is a user-provided leaf tensor,\n",
    "            # so checkpointing it can be a no-op. Once the first layer runs, activations require\n",
    "            # grad and checkpointing behaves as expected.\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                if self.use_ckpt and i > 0:\n",
    "                    x = checkpoint(lambda t, l=layer: torch.relu(l(t)), x, use_reentrant=False)\n",
    "                else:\n",
    "                    x = torch.relu(layer(x))\n",
    "            return x\n",
    "    return Model()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    hidden = 1024\n",
    "    batch = 64\n",
    "    \n",
    "    print(f\"Config: hidden={hidden}, batch={batch}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Layers':<10} {'No Ckpt (MB)':<15} {'With Ckpt (MB)':<15} {'Savings':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for num_layers in [10, 20, 40, 80]:\n",
    "        # Without checkpoint\n",
    "        torch.cuda.empty_cache()\n",
    "        m1 = create_model(num_layers, hidden, False).cuda()\n",
    "        x1 = torch.randn(batch, hidden).cuda()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        m1(x1).sum().backward()\n",
    "        mem1 = torch.cuda.max_memory_allocated() / 1e6\n",
    "        del m1, x1\n",
    "        \n",
    "        # With checkpoint\n",
    "        torch.cuda.empty_cache()\n",
    "        m2 = create_model(num_layers, hidden, True).cuda()\n",
    "        x2 = torch.randn(batch, hidden).cuda()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        m2(x2).sum().backward()\n",
    "        mem2 = torch.cuda.max_memory_allocated() / 1e6\n",
    "        del m2, x2\n",
    "        \n",
    "        savings = (1 - mem2/mem1) * 100\n",
    "        print(f\"{num_layers:<10} {mem1:<15.2f} {mem2:<15.2f} {savings:.1f}%\")\n",
    "else:\n",
    "    print(\"[Run on GPU for benchmarks]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (6.1)\n",
    "\n",
    "Memory savings increase with depth. At 10 layers, savings are modest. At 80 layers, savings can exceed 50%.\n",
    "\n",
    "This matches the theory: activation memory is O(n), checkpointing reduces it to O(1) per segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Batch Size Scaling: The Real Win\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "def create_model(num_layers, hidden, use_checkpoint=False):\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.ModuleList([\n",
    "                nn.Linear(hidden, hidden) for _ in range(num_layers)\n",
    "            ])\n",
    "            self.use_ckpt = use_checkpoint\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Avoid checkpointing the first layer (see Section 6.1 note).\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                if self.use_ckpt and i > 0:\n",
    "                    x = checkpoint(lambda t, l=layer: torch.relu(l(t)), x, use_reentrant=False)\n",
    "                else:\n",
    "                    x = torch.relu(layer(x))\n",
    "            return x\n",
    "    return Model()\n",
    "\n",
    "def find_max_batch(model_fn, hidden, start=32, max_batch=2048):\n",
    "    \"\"\"Find maximum batch size before OOM (CUDA only).\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is required for find_max_batch().\")\n",
    "\n",
    "    start = max(1, int(start))\n",
    "    max_batch = max(start, int(max_batch))\n",
    "\n",
    "    def can_run(batch_size):\n",
    "        model = None\n",
    "        x = None\n",
    "        out = None\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            model = model_fn().cuda()\n",
    "            model.train()\n",
    "            x = torch.randn(batch_size, hidden, device=\"cuda\")\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            out = model(x)\n",
    "            out.sum().backward()\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                return False\n",
    "            raise\n",
    "        finally:\n",
    "            # Best-effort cleanup after OOMs\n",
    "            try:\n",
    "                if model is not None:\n",
    "                    model.zero_grad(set_to_none=True)\n",
    "            except Exception:\n",
    "                pass\n",
    "            del model, x, out\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Exponential search to find an upper bound, then binary search.\n",
    "    max_working = 0\n",
    "    batch = start\n",
    "    while batch <= max_batch and can_run(batch):\n",
    "        max_working = batch\n",
    "        batch *= 2\n",
    "\n",
    "    low = max_working + 1\n",
    "    high = min(batch, max_batch)\n",
    "\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        if can_run(mid):\n",
    "            max_working = mid\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "\n",
    "    return max_working\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    hidden = 2048\n",
    "    num_layers = 30\n",
    "    \n",
    "    print(f\"Finding maximum batch size for {num_layers}-layer model, hidden={hidden}\")\n",
    "    print(\"This demonstrates the practical benefit of checkpointing.\\n\")\n",
    "    \n",
    "    # Max batch without checkpointing\n",
    "    model_no_ckpt = lambda: create_model(num_layers, hidden, False)\n",
    "    max_batch_no_ckpt = find_max_batch(model_no_ckpt, hidden)\n",
    "    \n",
    "    # Max batch with checkpointing\n",
    "    model_ckpt = lambda: create_model(num_layers, hidden, True)\n",
    "    max_batch_ckpt = find_max_batch(model_ckpt, hidden)\n",
    "\n",
    "    print(f\"WITHOUT checkpointing: max batch = {max_batch_no_ckpt}\")\n",
    "    print(f\"WITH checkpointing:    max batch = {max_batch_ckpt}\")\n",
    "\n",
    "    if max_batch_no_ckpt == 0:\n",
    "        print(\"Baseline OOM even at batch size 1. Reduce hidden size / layers.\")\n",
    "    else:\n",
    "        improvement = max_batch_ckpt / max_batch_no_ckpt\n",
    "        print(f\"Improvement: {improvement:.1f}x larger batches possible\")\n",
    "else:\n",
    "    print(\"[Run on GPU to find max batch sizes]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Compute Overhead Measurement\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import time\n",
    "\n",
    "def benchmark_time(model, x, num_iters=10):\n",
    "    # Warmup\n",
    "    for _ in range(3):\n",
    "        model(x).sum().backward()\n",
    "        model.zero_grad()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(num_iters):\n",
    "        model(x).sum().backward()\n",
    "        model.zero_grad()\n",
    "    torch.cuda.synchronize()\n",
    "    return (time.time() - start) / num_iters * 1000  # ms\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    hidden = 1024\n",
    "    batch = 64\n",
    "    num_layers = 40\n",
    "    \n",
    "    # Create models\n",
    "    m1 = create_model(num_layers, hidden, False).cuda()\n",
    "    m2 = create_model(num_layers, hidden, True).cuda()\n",
    "    x = torch.randn(batch, hidden).cuda()\n",
    "    \n",
    "    time1 = benchmark_time(m1, x)\n",
    "    time2 = benchmark_time(m2, x)\n",
    "    \n",
    "    overhead = (time2 - time1) / time1 * 100\n",
    "    \n",
    "    print(f\"Config: {num_layers} layers, hidden={hidden}, batch={batch}\")\n",
    "    print(f\"WITHOUT checkpoint: {time1:.2f} ms/iter\")\n",
    "    print(f\"WITH checkpoint:    {time2:.2f} ms/iter\")\n",
    "    print(f\"Overhead: {overhead:.1f}%\")\n",
    "else:\n",
    "    print(\"[Run on GPU for benchmarks]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (6.2)\n",
    "\n",
    "This is the practical payoff. Checkpointing lets you use 2-3x larger batch sizes.\n",
    "\n",
    "Larger batches mean:\n",
    "- Better gradient estimates (less noise)\n",
    "- Higher GPU utilization\n",
    "- Faster convergence (sometimes)\n",
    "\n",
    "The compute overhead of checkpointing is often offset by the efficiency gains from larger batches. You recompute more, but you also process more data per step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (6.3)\n",
    "\n",
    "Compute overhead is typically 30-50% depending on model architecture. This is the extra forward passes during backprop.\n",
    "\n",
    "The trade-off: 50% memory savings for ~50% compute overhead. Worth it when memory is the bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Debugging and Best Practices\n",
    "\n",
    "Checkpointing has gotchas. Here are the common ones and how to avoid them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Pitfall: Non-Differentiable Operations\n",
    "\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# BAD: torch.no_grad() inside checkpointed function\n",
    "def bad_forward(x):\n",
    "    with torch.no_grad():  # This breaks gradient computation!\n",
    "        x = x ** 2\n",
    "    return x\n",
    "\n",
    "# GOOD: Keep everything differentiable\n",
    "def good_forward(x):\n",
    "    x = x ** 2  # No torch.no_grad()\n",
    "    return x\n",
    "\n",
    "x = torch.randn(4, requires_grad=True)\n",
    "\n",
    "# This will fail or give wrong gradients\n",
    "try:\n",
    "    y_bad = checkpoint(bad_forward, x, use_reentrant=False)\n",
    "    y_bad.sum().backward()\n",
    "    print(\"BAD: Gradients might be zero or wrong\")\n",
    "except Exception as e:\n",
    "    print(f\"BAD: Error - {e}\")\n",
    "\n",
    "# This works\n",
    "x = torch.randn(4, requires_grad=True)\n",
    "y_good = checkpoint(good_forward, x, use_reentrant=False)\n",
    "y_good.sum().backward()\n",
    "print(f\"GOOD: x.grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (7.1)\n",
    "\n",
    "Rule: Everything inside a checkpointed function must be differentiable. No `torch.no_grad()`, no detaching tensors, no in-place operations that break autograd.\n",
    "\n",
    "If you need non-differentiable ops, do them outside the checkpointed region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Pitfall: Random Operations (Dropout)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# Problem: Dropout uses random masks.\n",
    "# If RNG state is NOT preserved, the recomputed forward in backward\n",
    "# will sample a different mask than the original forward.\n",
    "class ModelWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(32, 32)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)  # Random! Different each forward pass\n",
    "        return x\n",
    "\n",
    "model = ModelWithDropout()\n",
    "x = torch.randn(4, 32, requires_grad=True)\n",
    "\n",
    "# Fix: preserve RNG state (default) so recomputation matches the original forward.\n",
    "# use_reentrant=False is recommended for other reasons, but RNG correctness comes from preserve_rng_state.\n",
    "\n",
    "y = checkpoint(model, x, use_reentrant=False, preserve_rng_state=True)\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(\"RNG state preserved: dropout mask matches between forward and recompute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (7.2)\n",
    "\n",
    "Dropout and other random ops are tricky. The recomputed forward pass must use the same random mask as the original forward.\n",
    "\n",
    "In PyTorch, RNG correctness comes from `preserve_rng_state=True` (the default for `checkpoint`). Keep it enabled when checkpointing stochastic ops. `use_reentrant=False` is recommended for other reasons, but RNG correctness is controlled by `preserve_rng_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Best Practices Summary\n",
    "\n",
    "print(\"\"\"\n",
    "BEST PRACTICES FOR ACTIVATION CHECKPOINTING\n",
    "============================================\n",
    "\n",
    "1. USE use_reentrant=False\n",
    "   - Modern API, handles edge cases better\n",
    "   - checkpoint(fn, x, use_reentrant=False)\n",
    "\n",
    "2. CHECKPOINT LARGE BLOCKS, NOT SMALL LAYERS\n",
    "   - Overhead of checkpoint() call is non-trivial\n",
    "   - Group 2-4 layers into blocks, then checkpoint blocks\n",
    "\n",
    "3. AVOID NESTED CHECKPOINTS\n",
    "   - Don't checkpoint inside checkpointed functions\n",
    "   - Leads to exponential recomputation\n",
    "\n",
    "4. KEEP EVERYTHING DIFFERENTIABLE\n",
    "   - No torch.no_grad() inside checkpointed regions\n",
    "   - No tensor detaching\n",
    "   - No in-place ops that break autograd\n",
    "\n",
    "5. TEST GRADIENTS FIRST\n",
    "   - Compare gradients with and without checkpointing\n",
    "   - They should be identical (within float precision)\n",
    "\n",
    "6. PROFILE MEMORY\n",
    "   - Use torch.cuda.memory_stats() to verify savings\n",
    "   - Peak memory is what matters\n",
    "\n",
    "7. CONSIDER CHECKPOINT PLACEMENT\n",
    "   - Middle layers often have largest activations\n",
    "   - Checkpoint those first\n",
    "\"\"\")\n",
    "\n",
    "# Gradient verification example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "model = nn.Linear(64, 64)\n",
    "x = torch.randn(8, 64, requires_grad=True)\n",
    "\n",
    "# Without checkpoint\n",
    "x1 = x.clone().detach().requires_grad_(True)\n",
    "y1 = model(x1)\n",
    "y1.sum().backward()\n",
    "grad1 = x1.grad.clone()\n",
    "\n",
    "# With checkpoint\n",
    "x2 = x.clone().detach().requires_grad_(True)\n",
    "y2 = checkpoint(model, x2, use_reentrant=False)\n",
    "y2.sum().backward()\n",
    "grad2 = x2.grad.clone()\n",
    "\n",
    "print(f\"Gradients match: {torch.allclose(grad1, grad2)}\")\n",
    "print(f\"Max difference: {(grad1 - grad2).abs().max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (7.3)\n",
    "\n",
    "Gradient verification is essential. If gradients do not match exactly (within float precision), something is wrong with your checkpointing setup. Debug before scaling up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Selective Activation Checkpoint (SAC)\n",
    "\n",
    "Standard checkpointing is all-or-nothing. Every op in the checkpointed region gets recomputed during backward.\n",
    "\n",
    "Selective Activation Checkpoint (SAC) gives you granular control. You choose which operations to save and which to recompute.\n",
    "\n",
    "Why does this matter? Not all operations are equal:\n",
    "- Matmuls are expensive to recompute\n",
    "- Pointwise ops (relu, sigmoid) are cheap\n",
    "- Attention is very expensive\n",
    "\n",
    "SAC lets you save the expensive ones and recompute the cheap ones. Best of both worlds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Integrating with Distributed Training\n",
    "\n",
    "Checkpointing works with DDP and mixed precision. The combination is powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1 The CheckpointPolicy Enum\n",
    "\n",
    "try:\n",
    "    from torch.utils.checkpoint import CheckpointPolicy\n",
    "except Exception as e:\n",
    "    CheckpointPolicy = None\n",
    "    print(\"CheckpointPolicy not available in this PyTorch build.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "if CheckpointPolicy is not None:\n",
    "    print(\"CheckpointPolicy has four options:\")\n",
    "    print(\"-\" * 50)\n",
    "    print()\n",
    "    print(\"MUST_SAVE:\")\n",
    "    print(\"  - Always save this op's output\")\n",
    "    print(\"  - Never recompute it\")\n",
    "    print(\"  - Use for expensive ops (matmul, attention)\")\n",
    "    print()\n",
    "    print(\"PREFER_SAVE:\")\n",
    "    print(\"  - Save if possible\")\n",
    "    print(\"  - torch.compile may override this\")\n",
    "    print()\n",
    "    print(\"MUST_RECOMPUTE:\")\n",
    "    print(\"  - Always recompute this op\")\n",
    "    print(\"  - Never save its output\")\n",
    "    print()\n",
    "    print(\"PREFER_RECOMPUTE:\")\n",
    "    print(\"  - Recompute if possible\")\n",
    "    print(\"  - torch.compile may override this\")\n",
    "    print(\"  - Use for cheap ops (relu, elementwise)\")\n",
    "    print()\n",
    "    print(\"The MUST_ variants are strict. The PREFER_ variants are hints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (8.1)\n",
    "\n",
    "The four policies give you a 2x2 matrix:\n",
    "- MUST vs PREFER: How strict?\n",
    "- SAVE vs RECOMPUTE: What action?\n",
    "\n",
    "Use MUST_ when you know for certain. Use PREFER_ when you want torch.compile to potentially optimize further.\n",
    "\n",
    "Key insight: A policy that returns PREFER_RECOMPUTE for everything is equivalent to vanilla checkpointing. A policy that returns PREFER_SAVE for everything is NOT the same as no checkpointing (it may save extra tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Policy Function: Save Matmuls, Recompute Everything Else\n",
    "\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from torch.utils.checkpoint import CheckpointPolicy\n",
    "except Exception as e:\n",
    "    CheckpointPolicy = None\n",
    "    print(\"CheckpointPolicy not available; skipping SAC policy definition.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "aten = torch.ops.aten\n",
    "\n",
    "def _maybe_default(op_name: str):\n",
    "    \"\"\"Return torch.ops.aten.<op_name>.default if it exists, else None.\"\"\"\n",
    "    try:\n",
    "        return getattr(getattr(aten, op_name), \"default\")\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "# Policy 1: conservative - only save the most expensive ops.\n",
    "# IMPORTANT: Use the .default variants for correct op matching.\n",
    "compute_intensive_ops_basic = [\n",
    "    _maybe_default(\"mm\"),\n",
    "    _maybe_default(\"bmm\"),\n",
    "    _maybe_default(\"addmm\"),\n",
    "]\n",
    "compute_intensive_ops_basic = [op for op in compute_intensive_ops_basic if op is not None]\n",
    "\n",
    "def policy_save_matmuls(ctx, op, *args, **kwargs):\n",
    "    \"\"\"Save matmuls, recompute everything else.\"\"\"\n",
    "    if CheckpointPolicy is None:\n",
    "        raise RuntimeError(\"CheckpointPolicy is not available in this PyTorch build.\")\n",
    "    if op in compute_intensive_ops_basic:\n",
    "        return CheckpointPolicy.MUST_SAVE\n",
    "    return CheckpointPolicy.PREFER_RECOMPUTE\n",
    "\n",
    "print(\"Policy 1: Save Matmuls Only\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Saves: aten.mm.default, aten.bmm.default, aten.addmm.default (when available)\")\n",
    "print(\"Recomputes: relu, gelu, sigmoid, layernorm, etc.\")\n",
    "print()\n",
    "print(\"NOTE: Always match against .default (e.g., aten.mm.default)\")\n",
    "print(\"to match the actual ops passed to the policy function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 Policy Function: Save All Compute-Intensive Ops\n",
    "\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from torch.utils.checkpoint import CheckpointPolicy\n",
    "except Exception as e:\n",
    "    CheckpointPolicy = None\n",
    "    print(\"CheckpointPolicy not available; skipping SAC policy definition.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "aten = torch.ops.aten\n",
    "\n",
    "def _maybe_default(op_name: str):\n",
    "    try:\n",
    "        return getattr(getattr(aten, op_name), \"default\")\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "# Policy 2: aggressive - save everything expensive.\n",
    "# We build this list dynamically so older/newer PyTorch builds don't crash.\n",
    "op_names = [\n",
    "    \"mm\",\n",
    "    \"bmm\",\n",
    "    \"addmm\",\n",
    "    \"convolution\",\n",
    "    \"upsample_bilinear2d\",\n",
    "    \"_scaled_mm\",\n",
    "    \"linear\",\n",
    "    \"_scaled_dot_product_flash_attention\",\n",
    "    \"_scaled_dot_product_efficient_attention\",\n",
    "]\n",
    "\n",
    "compute_intensive_ops_full = []\n",
    "missing = []\n",
    "for name in op_names:\n",
    "    op = _maybe_default(name)\n",
    "    if op is None:\n",
    "        missing.append(name)\n",
    "    else:\n",
    "        compute_intensive_ops_full.append(op)\n",
    "\n",
    "def policy_save_all_expensive(ctx, op, *args, **kwargs):\n",
    "    \"\"\"Save all compute-intensive ops, including attention (when present).\"\"\"\n",
    "    if CheckpointPolicy is None:\n",
    "        raise RuntimeError(\"CheckpointPolicy is not available in this PyTorch build.\")\n",
    "    if op in compute_intensive_ops_full:\n",
    "        return CheckpointPolicy.MUST_SAVE\n",
    "    return CheckpointPolicy.PREFER_RECOMPUTE\n",
    "\n",
    "print(\"Policy 2: Save All Expensive Ops\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Saves: matmuls + convolutions + attention + upsampling (when available)\")\n",
    "print(\"Recomputes: only cheap pointwise ops\")\n",
    "if missing:\n",
    "    print(f\"Missing ops in this build: {', '.join(missing)}\")\n",
    "print()\n",
    "print(\"NOTE: Always match against .default (e.g., aten.convolution.default)\")\n",
    "print(\"and expect op availability to vary by build/hardware.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (8.2 & 8.3)\n",
    "\n",
    "Two policies, two positions on the speed-memory curve:\n",
    "\n",
    "Policy 1 (save matmuls only):\n",
    "- Memory: Low (close to full checkpointing)\n",
    "- Speed: Medium (recomputes everything except matmuls)\n",
    "\n",
    "Policy 2 (save all expensive):\n",
    "- Memory: Medium (saves attention and convolutions too)\n",
    "- Speed: High (only recomputes cheap pointwise ops)\n",
    "\n",
    "The key realization: pointwise ops are cheap to recompute but take significant memory. Recomputing just those gives you substantial memory savings with minimal compute overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 Using SAC in Practice: The Full API\n",
    "\n",
    "import functools\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "try:\n",
    "    from torch.utils.checkpoint import CheckpointPolicy, create_selective_checkpoint_contexts\n",
    "except Exception as e:\n",
    "    CheckpointPolicy = None\n",
    "    create_selective_checkpoint_contexts = None\n",
    "    print(\"Selective checkpointing APIs not available in this PyTorch build.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "if CheckpointPolicy is not None and create_selective_checkpoint_contexts is not None:\n",
    "    aten = torch.ops.aten\n",
    "\n",
    "    def _maybe_default(op_name: str):\n",
    "        try:\n",
    "            return getattr(getattr(aten, op_name), \"default\")\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "    # Define which ops to save - MUST use .default variants\n",
    "    ops_to_save = [\n",
    "        _maybe_default(\"mm\"),\n",
    "        _maybe_default(\"bmm\"),\n",
    "        _maybe_default(\"addmm\"),\n",
    "    ]\n",
    "    ops_to_save = [op for op in ops_to_save if op is not None]\n",
    "\n",
    "    def policy_fn(ctx, op, *args, **kwargs):\n",
    "        \"\"\"Policy function for selective checkpointing.\"\"\"\n",
    "        if op in ops_to_save:\n",
    "            return CheckpointPolicy.MUST_SAVE\n",
    "        return CheckpointPolicy.PREFER_RECOMPUTE\n",
    "\n",
    "    # Create the context function using functools.partial\n",
    "    context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n",
    "\n",
    "    def _run_checkpoint(fn, *args, context_fn=None):\n",
    "        \"\"\"Call checkpoint() with best-effort compatibility across versions.\"\"\"\n",
    "        kwargs = {\"use_reentrant\": False}\n",
    "        if context_fn is not None:\n",
    "            kwargs[\"context_fn\"] = context_fn\n",
    "        try:\n",
    "            return checkpoint(fn, *args, **kwargs)\n",
    "        except TypeError:\n",
    "            # Drop unsupported kwargs (older builds may not have context_fn/use_reentrant).\n",
    "            kwargs.pop(\"context_fn\", None)\n",
    "            try:\n",
    "                return checkpoint(fn, *args, **kwargs)\n",
    "            except TypeError:\n",
    "                kwargs.pop(\"use_reentrant\", None)\n",
    "                return checkpoint(fn, *args)\n",
    "\n",
    "    def forward_fn(x, weight1, weight2):\n",
    "        \"\"\"Example forward: two matmuls with activations.\"\"\"\n",
    "        x = torch.mm(x, weight1)   # Expected to match aten.mm.default\n",
    "        x = torch.relu(x)          # Cheap: often recomputed\n",
    "        x = torch.mm(x, weight2)   # Expected to match aten.mm.default\n",
    "        x = torch.sigmoid(x)       # Cheap: often recomputed\n",
    "        return x\n",
    "\n",
    "    x = torch.randn(32, 64, requires_grad=True)\n",
    "    w1 = torch.randn(64, 64, requires_grad=True)\n",
    "    w2 = torch.randn(64, 64, requires_grad=True)\n",
    "\n",
    "    output = _run_checkpoint(forward_fn, x, w1, w2, context_fn=context_fn)\n",
    "\n",
    "    print(\"SAC API Usage:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"1. Define policy_fn(ctx, op, *args, **kwargs) -> CheckpointPolicy\")\n",
    "    print(\"2. Create context_fn with functools.partial\")\n",
    "    print(\"3. Pass context_fn to checkpoint(..., context_fn=context_fn)\")\n",
    "    print()\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"Matmuls saved. Cheap activations will be recomputed during backward.\")\n",
    "    print()\n",
    "    print(\"IMPORTANT: Use aten.<op>.default for op matching, not aten.<op>\")\n",
    "else:\n",
    "    print(\"[Skipping SAC demo: missing CheckpointPolicy/create_selective_checkpoint_contexts]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5 Shortcut: Allowlist of Ops (No Boilerplate Policy)\n",
    "\n",
    "import functools\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "try:\n",
    "    from torch.utils.checkpoint import CheckpointPolicy, create_selective_checkpoint_contexts\n",
    "except Exception as e:\n",
    "    CheckpointPolicy = None\n",
    "    create_selective_checkpoint_contexts = None\n",
    "    print(\"Selective checkpointing APIs not available in this PyTorch build.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "aten = torch.ops.aten\n",
    "\n",
    "def _maybe_default(op_name: str):\n",
    "    try:\n",
    "        return getattr(getattr(aten, op_name), \"default\")\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "# Allowlist of ops to save (built dynamically)\n",
    "ops_to_save = [\n",
    "    _maybe_default(\"mm\"),\n",
    "    _maybe_default(\"bmm\"),\n",
    "]\n",
    "ops_to_save = [op for op in ops_to_save if op is not None]\n",
    "\n",
    "def allowlist_policy(ops):\n",
    "    ops = set(ops)\n",
    "\n",
    "    def _policy(ctx, op, *args, **kwargs):\n",
    "        if CheckpointPolicy is None:\n",
    "            raise RuntimeError(\"CheckpointPolicy is not available in this PyTorch build.\")\n",
    "        if op in ops:\n",
    "            return CheckpointPolicy.MUST_SAVE\n",
    "        return CheckpointPolicy.PREFER_RECOMPUTE\n",
    "\n",
    "    return _policy\n",
    "\n",
    "if CheckpointPolicy is not None and create_selective_checkpoint_contexts is not None:\n",
    "    policy_fn = allowlist_policy(ops_to_save)\n",
    "    context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n",
    "\n",
    "    def _run_checkpoint(fn, *args, context_fn=None):\n",
    "        kwargs = {\"use_reentrant\": False}\n",
    "        if context_fn is not None:\n",
    "            kwargs[\"context_fn\"] = context_fn\n",
    "        try:\n",
    "            return checkpoint(fn, *args, **kwargs)\n",
    "        except TypeError:\n",
    "            kwargs.pop(\"context_fn\", None)\n",
    "            try:\n",
    "                return checkpoint(fn, *args, **kwargs)\n",
    "            except TypeError:\n",
    "                return checkpoint(fn, *args)\n",
    "\n",
    "    def simple_forward(x, w):\n",
    "        x = torch.mm(x, w)\n",
    "        x = torch.relu(x)\n",
    "        return x\n",
    "\n",
    "    x = torch.randn(32, 64, requires_grad=True)\n",
    "    w = torch.randn(64, 64, requires_grad=True)\n",
    "\n",
    "    output = _run_checkpoint(simple_forward, x, w, context_fn=context_fn)\n",
    "\n",
    "    print(\"Shortcut API:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Allowlist: choose ops to save (e.g., mm/bmm)\")\n",
    "    print(\"Everything else defaults to PREFER_RECOMPUTE\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "else:\n",
    "    print(\"[Skipping SAC shortcut demo: missing APIs]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.6 SAC vs Standard AC: Memory and Time Comparison\n",
    "\n",
    "import functools\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "try:\n",
    "    from torch.utils.checkpoint import CheckpointPolicy, create_selective_checkpoint_contexts\n",
    "except Exception as e:\n",
    "    CheckpointPolicy = None\n",
    "    create_selective_checkpoint_contexts = None\n",
    "    print(\"Selective checkpointing APIs not available in this PyTorch build.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "class TransformerFFN(nn.Module):\n",
    "    \"\"\"Feed-forward block from a transformer.\"\"\"\n",
    "    def __init__(self, dim=1024, expansion=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * expansion)\n",
    "        self.fc2 = nn.Linear(dim * expansion, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.gelu(x)  # Cheap pointwise\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def _run_checkpoint(fn, *args, context_fn=None):\n",
    "    \"\"\"Call checkpoint() with best-effort compatibility across versions.\"\"\"\n",
    "    kwargs = {\"use_reentrant\": False}\n",
    "    if context_fn is not None:\n",
    "        kwargs[\"context_fn\"] = context_fn\n",
    "    try:\n",
    "        return checkpoint(fn, *args, **kwargs)\n",
    "    except TypeError:\n",
    "        kwargs.pop(\"context_fn\", None)\n",
    "        try:\n",
    "            return checkpoint(fn, *args, **kwargs)\n",
    "        except TypeError:\n",
    "            return checkpoint(fn, *args)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dim = 2048\n",
    "    batch = 64\n",
    "    seq = 128\n",
    "\n",
    "    # Add a small stem so the checkpointed region receives activations that require grad.\n",
    "    stem = nn.Linear(dim, dim).cuda()\n",
    "    ffn = TransformerFFN(dim).cuda()\n",
    "\n",
    "    x = torch.randn(batch, seq, dim, device=\"cuda\")\n",
    "\n",
    "    sac_context = None\n",
    "    if CheckpointPolicy is not None and create_selective_checkpoint_contexts is not None:\n",
    "        aten = torch.ops.aten\n",
    "\n",
    "        def _maybe_default(op_name: str):\n",
    "            try:\n",
    "                return getattr(getattr(aten, op_name), \"default\")\n",
    "            except AttributeError:\n",
    "                return None\n",
    "\n",
    "        saved_ops = [op for op in [_maybe_default(\"mm\"), _maybe_default(\"addmm\")] if op is not None]\n",
    "\n",
    "        def sac_policy(ctx, op, *args, **kwargs):\n",
    "            # nn.Linear typically lowers to addmm/mm\n",
    "            if op in saved_ops:\n",
    "                return CheckpointPolicy.MUST_SAVE\n",
    "            return CheckpointPolicy.PREFER_RECOMPUTE\n",
    "\n",
    "        sac_context = functools.partial(create_selective_checkpoint_contexts, sac_policy)\n",
    "    else:\n",
    "        print(\"SAC APIs not available: will skip Selective AC benchmark\")\n",
    "\n",
    "    def bench(mode: str, iters: int = 5, warmup: int = 2):\n",
    "        def step():\n",
    "            h = stem(x)\n",
    "            if mode == \"none\":\n",
    "                out = ffn(h)\n",
    "            elif mode == \"standard\":\n",
    "                out = _run_checkpoint(ffn, h)\n",
    "            elif mode == \"selective\":\n",
    "                if sac_context is None:\n",
    "                    raise RuntimeError(\"Selective checkpointing context not available\")\n",
    "                out = _run_checkpoint(ffn, h, context_fn=sac_context)\n",
    "            else:\n",
    "                raise ValueError(mode)\n",
    "\n",
    "            out.sum().backward()\n",
    "            stem.zero_grad(set_to_none=True)\n",
    "            ffn.zero_grad(set_to_none=True)\n",
    "\n",
    "        for _ in range(warmup):\n",
    "            step()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        start = time.time()\n",
    "        for _ in range(iters):\n",
    "            step()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        time_ms = (time.time() - start) / iters * 1000\n",
    "        mem_mb = torch.cuda.max_memory_allocated() / 1e6\n",
    "        return mem_mb, time_ms\n",
    "\n",
    "    results = {\n",
    "        \"No Checkpoint\": bench(\"none\"),\n",
    "        \"Standard AC\": bench(\"standard\"),\n",
    "    }\n",
    "    if sac_context is not None:\n",
    "        results[\"Selective AC\"] = bench(\"selective\")\n",
    "\n",
    "    print(f\"Transformer FFN: dim={dim}, batch={batch}, seq={seq}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Mode':<15} {'Peak Memory (MB)':<18} {'Time (ms/iter)':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    for name, (mem, t) in results.items():\n",
    "        print(f\"{name:<15} {mem:<18.2f} {t:<15.2f}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"Standard AC: recomputes everything inside the checkpointed region\")\n",
    "    print(\"Selective AC (when available): can save matmuls and recompute only GELU\")\n",
    "else:\n",
    "    print(\"[Run on GPU for comparison]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (8.4, 8.5, 8.6)\n",
    "\n",
    "SAC is the middle ground between \"checkpoint everything\" and \"checkpoint nothing.\"\n",
    "\n",
    "The API flow:\n",
    "1. Define ops you care about (matmuls, attention, etc.)\n",
    "2. Write a policy function (an allowlist wrapper is often enough)\n",
    "3. Create contexts with `create_selective_checkpoint_contexts` (when available)\n",
    "4. Pass to `checkpoint(..., context_fn=...)`\n",
    "\n",
    "When to use SAC over standard AC:\n",
    "- You need fine-grained control\n",
    "- Recomputing certain ops is too expensive\n",
    "- You want to tune the speed-memory trade-off precisely\n",
    "\n",
    "Availability varies by PyTorch version/build. Treat SAC as evolving/experimental and guard imports in production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 DDP + Checkpointing (Code Template)\n",
    "# NOTE: This code shows the pattern. Run in a distributed environment.\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# Initialize distributed\n",
    "dist.init_process_group(backend=\"nccl\")\n",
    "local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "torch.cuda.set_device(local_rank)\n",
    "\n",
    "class ModelWithCheckpoint(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1024, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 1024)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = checkpoint(lambda t: torch.relu(self.layer1(t)), x, use_reentrant=False)\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Wrap with DDP\n",
    "model = ModelWithCheckpoint().cuda(local_rank)\n",
    "ddp_model = DDP(model, device_ids=[local_rank])\n",
    "\n",
    "# Training works as normal\n",
    "x = torch.randn(16, 1024).cuda(local_rank)\n",
    "output = ddp_model(x)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "\"\"\"\n",
    "\n",
    "print(\"DDP + Checkpointing:\")\n",
    "print(\"- Checkpointing happens locally on each GPU\")\n",
    "print(\"- DDP handles gradient synchronization\")\n",
    "print(\"- No special configuration needed\")\n",
    "print(\"- Memory savings apply per-GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 Mixed Precision + Checkpointing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1024, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 1024)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = checkpoint(lambda t: torch.relu(self.layer1(t)), x, use_reentrant=False)\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = SimpleModel().cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    # Training loop with mixed precision + checkpointing\n",
    "    for step in range(3):\n",
    "        x = torch.randn(16, 1024).cuda()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(x)\n",
    "            loss = output.sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        print(f\"Step {step}: loss = {loss.item():.4f}\")\n",
    "    \n",
    "    print(\"\\nMixed precision + checkpointing works seamlessly\")\n",
    "else:\n",
    "    print(\"[Run on GPU]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (9.1 & 9.2)\n",
    "\n",
    "The power combo: DDP + Mixed Precision + Checkpointing.\n",
    "\n",
    "- DDP: Scale across GPUs\n",
    "- Mixed Precision: 2x memory reduction from fp16\n",
    "- Checkpointing: Further memory reduction from recomputation\n",
    "\n",
    "Combined, you can train models 4-5x larger than baseline. This is how GPT-scale models are trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 10. Additional Tools and Resources\n",
    "\n",
    "Beyond PyTorch's built-in checkpointing, there are libraries that offer more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 DeepSpeed Integration\n",
    "\n",
    "\"\"\"\n",
    "DeepSpeed offers activation checkpointing as part of its ZeRO optimization suite.\n",
    "\n",
    "# Installation\n",
    "pip install deepspeed\n",
    "\n",
    "# Usage\n",
    "import deepspeed\n",
    "from deepspeed.runtime.activation_checkpointing import checkpointing\n",
    "\n",
    "# Configure in ds_config.json:\n",
    "{\n",
    "    \"activation_checkpointing\": {\n",
    "        \"partition_activations\": true,\n",
    "        \"contiguous_memory_optimization\": true,\n",
    "        \"cpu_checkpointing\": true  # Offload to CPU!\n",
    "    }\n",
    "}\n",
    "\n",
    "# Key advantage: CPU offloading\n",
    "# DeepSpeed can move checkpointed activations to CPU memory,\n",
    "# freeing GPU memory for even larger models.\n",
    "\"\"\"\n",
    "\n",
    "print(\"DeepSpeed Activation Checkpointing:\")\n",
    "print(\"- Automatic checkpoint placement\")\n",
    "print(\"- CPU offloading for extreme memory savings\")\n",
    "print(\"- Integrated with ZeRO optimizer stages\")\n",
    "print(\"- Best for very large models (billions of parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 FairScale Integration\n",
    "\n",
    "\"\"\"\n",
    "FairScale (from Meta) provides checkpoint_wrapper for easy integration.\n",
    "\n",
    "# Installation\n",
    "pip install fairscale\n",
    "\n",
    "# Usage\n",
    "from fairscale.nn.checkpoint import checkpoint_wrapper\n",
    "\n",
    "# Wrap any module\n",
    "layer = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU())\n",
    "checkpointed_layer = checkpoint_wrapper(layer)\n",
    "\n",
    "# Use in model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = checkpoint_wrapper(nn.Sequential(...))\n",
    "        self.block2 = checkpoint_wrapper(nn.Sequential(...))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        return x\n",
    "\"\"\"\n",
    "\n",
    "print(\"FairScale checkpoint_wrapper:\")\n",
    "print(\"- Clean API: wrap modules directly\")\n",
    "print(\"- Works well with FSDP (Fully Sharded Data Parallel)\")\n",
    "print(\"- Good for medium-scale training\")\n",
    "print(\"- Simpler than DeepSpeed for many use cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (10.1 & 10.2)\n",
    "\n",
    "Tool selection guide:\n",
    "- **PyTorch native**: Simple models, full control, no dependencies\n",
    "- **FairScale**: Medium scale, clean API, FSDP integration\n",
    "- **DeepSpeed**: Billion+ parameter models, CPU offloading, full optimization suite\n",
    "\n",
    "Start with PyTorch native. Move to DeepSpeed when you hit limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 11. torch.compile and Memory Budget API\n",
    "\n",
    "torch.compile (PyTorch 2.0+) does something clever: it traces your forward and backward passes into a single joint graph. Then it applies a \"min-cut\" partitioner.\n",
    "\n",
    "This is different from activation checkpointing. The min-cut algorithm automatically decides which tensors to save and which to recompute based on minimizing total runtime. No manual configuration needed.\n",
    "\n",
    "But here is the catch: by default, min-cut prioritizes speed, not memory. It only recomputes cheap, fusible ops (like pointwise operations).\n",
    "\n",
    "The Memory Budget API gives you control over this trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.1 torch.compile: The Min-Cut Partitioner\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"How torch.compile handles activations:\")\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "print(\"1. TRACING\")\n",
    "print(\"   torch.compile traces forward AND backward into one graph.\")\n",
    "print(\"   This lets it see the whole picture.\")\n",
    "print()\n",
    "print(\"2. MIN-CUT PARTITIONING\")\n",
    "print(\"   The graph is split at the optimal points.\")\n",
    "print(\"   Algorithm minimizes: tensors crossing the cut\")\n",
    "print(\"   (These are the tensors saved for backward)\")\n",
    "print()\n",
    "print(\"3. AUTOMATIC RECOMPUTATION\")\n",
    "print(\"   Cheap ops (relu, add, mul) are recomputed automatically.\")\n",
    "print(\"   No user intervention needed.\")\n",
    "print()\n",
    "print(\"4. FUSION\")\n",
    "print(\"   Pointwise ops get fused into kernels.\")\n",
    "print(\"   Fused ops are fast to recompute.\")\n",
    "print()\n",
    "print(\"Result: torch.compile gives you SOME memory savings for FREE,\")\n",
    "print(\"        plus speed improvements from fusion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.2 Comparing: Eager vs Compile vs Checkpointing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class SimpleFFN(nn.Module):\n",
    "    def __init__(self, dim=2048):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Wrapper(nn.Module):\n",
    "    \"\"\"Adds a small stem so the checkpointed region receives activations that require grad.\"\"\"\n",
    "    def __init__(self, dim=2048, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Linear(dim, dim)\n",
    "        self.ffn = SimpleFFN(dim)\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        if self.use_checkpoint:\n",
    "            x = checkpoint(self.ffn, x, use_reentrant=False)\n",
    "        else:\n",
    "            x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "def warmup(fn, zero_grad_fn, x, iters=2):\n",
    "    for _ in range(iters):\n",
    "        fn(x).sum().backward()\n",
    "        zero_grad_fn()\n",
    "\n",
    "def peak_memory_mb(fn, zero_grad_fn, x):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out = fn(x)\n",
    "    out.sum().backward()\n",
    "    peak = torch.cuda.max_memory_allocated() / 1e6\n",
    "    zero_grad_fn()\n",
    "    return peak\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dim = 2048\n",
    "    batch = 64\n",
    "    seq = 128\n",
    "\n",
    "    x = torch.randn(batch, seq, dim, device=\"cuda\")\n",
    "\n",
    "    eager_model = Wrapper(dim, use_checkpoint=False).cuda()\n",
    "    ckpt_model = Wrapper(dim, use_checkpoint=True).cuda()\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 1) Eager mode\n",
    "    warmup(eager_model, lambda: eager_model.zero_grad(set_to_none=True), x)\n",
    "    results['Eager'] = peak_memory_mb(eager_model, lambda: eager_model.zero_grad(set_to_none=True), x)\n",
    "\n",
    "    # 2) torch.compile (if available)\n",
    "    if hasattr(torch, \"compile\"):\n",
    "        compiled = torch.compile(eager_model)\n",
    "        warmup(compiled, lambda: eager_model.zero_grad(set_to_none=True), x)\n",
    "        results['torch.compile'] = peak_memory_mb(compiled, lambda: eager_model.zero_grad(set_to_none=True), x)\n",
    "    else:\n",
    "        print(\"torch.compile not available in this PyTorch build\")\n",
    "\n",
    "    # 3) Activation checkpointing\n",
    "    warmup(ckpt_model, lambda: ckpt_model.zero_grad(set_to_none=True), x)\n",
    "    results['Checkpointing'] = peak_memory_mb(ckpt_model, lambda: ckpt_model.zero_grad(set_to_none=True), x)\n",
    "\n",
    "    print(f\"FFN: dim={dim}, batch={batch}, seq={seq}\")\n",
    "    print(\"-\" * 50)\n",
    "    for name, mem in results.items():\n",
    "        print(f\"{name:20s}: {mem:8.2f} MB\")\n",
    "    print()\n",
    "    print(\"torch.compile: better speed, moderate memory savings\")\n",
    "    print(\"Checkpointing: maximum memory savings, some speed cost\")\n",
    "else:\n",
    "    print(\"[Run on GPU for comparison]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (11.1 & 11.2)\n",
    "\n",
    "The speed-memory trade-off diagram:\n",
    "\n",
    "```\n",
    "Speed (high is good)\n",
    "  ^\n",
    "  |   * torch.compile (top-left: fast, some memory savings)\n",
    "  |\n",
    "  |        * Eager (top-right: fast, high memory)\n",
    "  |\n",
    "  |   * SAC policies (middle: tunable)\n",
    "  |\n",
    "  |   * Checkpointing (bottom-left: slower, low memory)\n",
    "  |\n",
    "  +---------------------------------> Memory (right is bad)\n",
    "```\n",
    "\n",
    "torch.compile sits between eager and full checkpointing. It automatically recomputes cheap ops. The Memory Budget API lets you push it further toward checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.3 Memory Budget API: Control the Trade-off\n",
    "\n",
    "import torch\n",
    "\n",
    "if not hasattr(torch, \"compile\"):\n",
    "    print(\"torch.compile not available in this PyTorch build\")\n",
    "else:\n",
    "    try:\n",
    "        import torch._dynamo as dynamo\n",
    "    except Exception as e:\n",
    "        dynamo = None\n",
    "        print(\"torch._dynamo not available; cannot use memory budget APIs\")\n",
    "        print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "    print(\"Memory Budget API (torch.compile only)\")\n",
    "    print(\"-\" * 50)\n",
    "    print()\n",
    "\n",
    "    if dynamo is None or not hasattr(dynamo.config, \"activation_memory_budget\"):\n",
    "        print(\"activation_memory_budget not found in torch._dynamo.config in this build\")\n",
    "    else:\n",
    "        print(\"torch._dynamo.config.activation_memory_budget = X\")\n",
    "        print()\n",
    "        print(\"X = 0.0: Maximum recomputation (like full AC)\")\n",
    "        print(\"         Recompute everything, save almost nothing\")\n",
    "        print()\n",
    "        print(\"X = 0.5: Balanced\")\n",
    "        print(\"         Recompute pointwise ops, save matmuls\")\n",
    "        print()\n",
    "        print(\"X = 1.0: Default torch.compile behavior\")\n",
    "        print(\"         Minimal recomputation, maximize speed\")\n",
    "        print()\n",
    "        print(\"The API automatically finds pareto-optimal policies.\")\n",
    "        print(\"You just specify how much memory you want to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.4 Using Memory Budget API in Practice\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim=1024, heads=8):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        x = self.norm2(x + self.ffn(x))\n",
    "        return x\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"[Run on GPU for memory budget comparison]\")\n",
    "elif not hasattr(torch, \"compile\"):\n",
    "    print(\"torch.compile not available in this PyTorch build\")\n",
    "else:\n",
    "    try:\n",
    "        import torch._dynamo as dynamo\n",
    "    except Exception as e:\n",
    "        dynamo = None\n",
    "        print(\"torch._dynamo not available; cannot use memory budget APIs\")\n",
    "        print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "    if dynamo is None or not hasattr(dynamo.config, \"activation_memory_budget\"):\n",
    "        print(\"activation_memory_budget not available in torch._dynamo.config in this build\")\n",
    "    else:\n",
    "        dim = 1024\n",
    "        batch = 32\n",
    "        seq = 256\n",
    "\n",
    "        model = TransformerBlock(dim).cuda()\n",
    "        x = torch.randn(batch, seq, dim, device=\"cuda\")\n",
    "\n",
    "        budgets = [1.0, 0.7, 0.5, 0.3, 0.0]\n",
    "\n",
    "        print(f\"Transformer: dim={dim}, batch={batch}, seq={seq}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Budget':<10} {'Memory (MB)':<15} {'Relative':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        baseline = None\n",
    "        original_budget = dynamo.config.activation_memory_budget\n",
    "\n",
    "        try:\n",
    "            for budget in budgets:\n",
    "                dynamo.config.activation_memory_budget = float(budget)\n",
    "\n",
    "                # Recompile with new budget\n",
    "                if hasattr(dynamo, \"reset\"):\n",
    "                    dynamo.reset()\n",
    "                compiled = torch.compile(model)\n",
    "\n",
    "                # Warmup\n",
    "                for _ in range(2):\n",
    "                    compiled(x).sum().backward()\n",
    "                    model.zero_grad(set_to_none=True)\n",
    "\n",
    "                # Measure\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "                out = compiled(x)\n",
    "                out.sum().backward()\n",
    "                mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "                model.zero_grad(set_to_none=True)\n",
    "\n",
    "                if baseline is None:\n",
    "                    baseline = mem\n",
    "\n",
    "                relative = mem / baseline\n",
    "                print(f\"{budget:<10.1f} {mem:<15.2f} {relative:.2f}x\")\n",
    "        finally:\n",
    "            dynamo.config.activation_memory_budget = original_budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.5 What Gets Recomputed at Each Budget Level?\n",
    "\n",
    "print(\"Recomputation order (from blog's real transformer results):\")\n",
    "print(\"-\" * 60)\n",
    "print()\n",
    "print(\"Budget 1.0 (default compile):\")\n",
    "print(\"  Recomputes: Nothing extra\")\n",
    "print(\"  Saves: Everything\")\n",
    "print(\"  Memory: 100% (baseline)\")\n",
    "print()\n",
    "print(\"Budget 0.7:\")\n",
    "print(\"  Recomputes: Pointwise ops (gelu, add, mul)\")\n",
    "print(\"  Saves: Matmuls, attention\")\n",
    "print(\"  Memory: ~85% of baseline\")\n",
    "print()\n",
    "print(\"Budget 0.5:\")\n",
    "print(\"  Recomputes: Pointwise + some matmuls\")\n",
    "print(\"  Saves: Attention (most expensive)\")\n",
    "print(\"  Memory: ~50% of baseline\")\n",
    "print()\n",
    "print(\"Budget 0.3:\")\n",
    "print(\"  Recomputes: Pointwise + most matmuls\")\n",
    "print(\"  Saves: Only attention\")\n",
    "print(\"  Memory: ~35% of baseline\")\n",
    "print()\n",
    "print(\"Budget 0.0:\")\n",
    "print(\"  Recomputes: Everything (like full AC)\")\n",
    "print(\"  Saves: Almost nothing\")\n",
    "print(\"  Memory: Minimum possible\")\n",
    "print()\n",
    "print(\"Key insight: 50% memory reduction by recomputing only pointwise ops.\")\n",
    "print(\"Attention is expensive. Recompute it last.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (11.3, 11.4, 11.5)\n",
    "\n",
    "Memory Budget API is the easiest way to tune the speed-memory trade-off with torch.compile.\n",
    "\n",
    "One line of code (when available):\n",
    "```python\n",
    "torch._dynamo.config.activation_memory_budget = 0.5\n",
    "```\n",
    "\n",
    "The system automatically:\n",
    "1. Finds pareto-optimal recomputation strategies\n",
    "2. Prioritizes recomputing cheap ops first\n",
    "3. Saves attention and expensive matmuls for last\n",
    "\n",
    "When to use Memory Budget API vs SAC:\n",
    "- Memory Budget: Using torch.compile, want automatic optimization\n",
    "- SAC: Need precise control over which ops to save/recompute\n",
    "\n",
    "Availability varies by PyTorch build. Guard `torch._dynamo.config.activation_memory_budget` before relying on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 12. Case Study: Activation Checkpointing in ResNet50\n",
    "\n",
    "ResNet50 is deep enough to benefit from checkpointing. It has 4 stages with multiple bottleneck blocks. Each block stores feature maps for backward.\n",
    "\n",
    "This section shows how to apply checkpointing to a real production model. You will see:\n",
    "1. How to modify torchvision's ResNet50\n",
    "2. Memory comparison across checkpointing strategies\n",
    "3. A complete training loop with checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.1 ResNet50 Architecture Overview\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    import torchvision.models as models\n",
    "except Exception as e:\n",
    "    models = None\n",
    "    print(\"torchvision is required for the ResNet50 case study.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "if models is not None:\n",
    "    # Load ResNet50 without downloading weights\n",
    "    try:\n",
    "        resnet50 = models.resnet50(weights=None)\n",
    "    except TypeError:\n",
    "        # Older torchvision\n",
    "        resnet50 = models.resnet50(pretrained=False)\n",
    "\n",
    "    print(\"ResNet50 Structure:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"conv1:  1 conv layer\")\n",
    "    print(f\"layer1: {len(resnet50.layer1)} Bottleneck blocks (64 -> 256 channels)\")\n",
    "    print(f\"layer2: {len(resnet50.layer2)} Bottleneck blocks (128 -> 512 channels)\")\n",
    "    print(f\"layer3: {len(resnet50.layer3)} Bottleneck blocks (256 -> 1024 channels)\")\n",
    "    print(f\"layer4: {len(resnet50.layer4)} Bottleneck blocks (512 -> 2048 channels)\")\n",
    "    print(f\"fc:     1 linear layer\")\n",
    "    print()\n",
    "    print(\n",
    "        f\"Total Bottleneck blocks: {len(resnet50.layer1) + len(resnet50.layer2) + len(resnet50.layer3) + len(resnet50.layer4)}\"\n",
    "    )\n",
    "    print()\n",
    "    print(\"Each Bottleneck block has 3 conv layers + skip connection.\")\n",
    "    print(\"Feature maps grow larger in early layers, then shrink spatially.\")\n",
    "    print(\"layer3 often has the largest activation memory (1024 channels, moderate spatial size).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.2 ResNet50 with Checkpointing: Three Strategies\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "try:\n",
    "    import torchvision.models as models\n",
    "except Exception as e:\n",
    "    models = None\n",
    "    ResNet50Checkpointed = None\n",
    "    print(\"torchvision is required for ResNet50Checkpointed.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "if models is not None:\n",
    "    class ResNet50Checkpointed(nn.Module):\n",
    "        \"\"\"ResNet50 with configurable checkpointing strategies.\"\"\"\n",
    "\n",
    "        def __init__(self, num_classes=1000, checkpoint_strategy='none'):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                checkpoint_strategy: 'none', 'per_stage', 'per_block', or 'aggressive'\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "\n",
    "            # Load base ResNet50 without downloading weights\n",
    "            try:\n",
    "                base = models.resnet50(weights=None)\n",
    "            except TypeError:\n",
    "                base = models.resnet50(pretrained=False)\n",
    "\n",
    "            # Copy all layers\n",
    "            self.conv1 = base.conv1\n",
    "            self.bn1 = base.bn1\n",
    "            self.relu = base.relu\n",
    "            self.maxpool = base.maxpool\n",
    "            self.layer1 = base.layer1\n",
    "            self.layer2 = base.layer2\n",
    "            self.layer3 = base.layer3\n",
    "            self.layer4 = base.layer4\n",
    "            self.avgpool = base.avgpool\n",
    "            self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "            self.checkpoint_strategy = checkpoint_strategy\n",
    "\n",
    "        def _forward_stage(self, stage, x):\n",
    "            \"\"\"Forward through a stage (layer1, layer2, etc.).\"\"\"\n",
    "            for block in stage:\n",
    "                x = block(x)\n",
    "            return x\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Stem\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "            # NOTE: ResNet blocks include BatchNorm, which updates running stats in train mode.\n",
    "            # Checkpointing recomputes forward in backward, which can update BN stats twice.\n",
    "            # For strict parity, consider freezing BN stats (eval for BN) or using GroupNorm.\n",
    "\n",
    "            if self.checkpoint_strategy == 'none':\n",
    "                x = self.layer1(x)\n",
    "                x = self.layer2(x)\n",
    "                x = self.layer3(x)\n",
    "                x = self.layer4(x)\n",
    "\n",
    "            elif self.checkpoint_strategy == 'per_stage':\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer1, t), x, use_reentrant=False)\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer2, t), x, use_reentrant=False)\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer3, t), x, use_reentrant=False)\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer4, t), x, use_reentrant=False)\n",
    "\n",
    "            elif self.checkpoint_strategy == 'per_block':\n",
    "                for block in self.layer1:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer2:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer3:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer4:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "\n",
    "            elif self.checkpoint_strategy == 'aggressive':\n",
    "                x = self.layer1(x)\n",
    "                x = self.layer2(x)\n",
    "                for block in self.layer3:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer4:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown checkpoint_strategy: {self.checkpoint_strategy}\")\n",
    "\n",
    "            # Head\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "    print(\"ResNet50Checkpointed created with 4 strategies:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"'none':       No checkpointing (baseline)\")\n",
    "    print(\"'per_stage':  Checkpoint each of the 4 stages\")\n",
    "    print(\"'per_block':  Checkpoint each of the 16 bottleneck blocks\")\n",
    "    print(\"'aggressive': Only checkpoint layer3 and layer4 (best ROI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.3 Memory Comparison: ResNet50 Strategies\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "def benchmark_resnet50(strategy, batch_size=32, image_size=224, num_iters=5):\n",
    "    \"\"\"Benchmark memory and time for a ResNet50 checkpointing strategy.\"\"\"\n",
    "    if ResNet50Checkpointed is None:\n",
    "        raise RuntimeError(\"ResNet50Checkpointed is not available (torchvision import likely failed).\")\n",
    "\n",
    "    model = ResNet50Checkpointed(num_classes=1000, checkpoint_strategy=strategy).cuda()\n",
    "    model.train()\n",
    "\n",
    "    x = torch.randn(batch_size, 3, image_size, image_size, device=\"cuda\")\n",
    "    target = torch.randint(0, 1000, (batch_size,), device=\"cuda\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(2):\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Measure memory + time\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(num_iters):\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed = (time.time() - start) / num_iters * 1000  # ms/iter\n",
    "    peak_mem = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
    "\n",
    "    del model, x, target\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return peak_mem, elapsed\n",
    "\n",
    "if torch.cuda.is_available() and ResNet50Checkpointed is not None:\n",
    "    batch_size = 32\n",
    "    image_size = 224\n",
    "\n",
    "    print(\"ResNet50 Checkpointing Comparison\")\n",
    "    print(f\"Batch size: {batch_size}, Image size: {image_size}x{image_size}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Strategy':<15} {'Peak Memory (MB)':<20} {'Time (ms)':<15} {'Mem Savings':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    strategies = ['none', 'aggressive', 'per_stage', 'per_block']\n",
    "    results = {}\n",
    "\n",
    "    for strategy in strategies:\n",
    "        mem, time_ms = benchmark_resnet50(strategy, batch_size, image_size)\n",
    "        results[strategy] = (mem, time_ms)\n",
    "\n",
    "    baseline_mem = results['none'][0]\n",
    "    for strategy in strategies:\n",
    "        mem, time_ms = results[strategy]\n",
    "        savings = (1 - mem / baseline_mem) * 100\n",
    "        print(f\"{strategy:<15} {mem:<20.2f} {time_ms:<15.2f} {savings:>6.1f}%\")\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "    print(\"\\nObservations:\")\n",
    "    print(\"- 'aggressive' gives best memory/speed balance (checkpoints only deep layers)\")\n",
    "    print(\"- 'per_block' gives maximum memory savings but highest overhead\")\n",
    "    print(\"- 'per_stage' is a middle ground\")\n",
    "else:\n",
    "    print(\"[Run on GPU and ensure torchvision is installed for ResNet50 benchmarks]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.4 Maximum Batch Size: ResNet50\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def find_max_batch_resnet50(strategy, start=16, max_batch=256):\n",
    "    \"\"\"Find maximum batch size before OOM for ResNet50 (CUDA only).\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is required for find_max_batch_resnet50().\")\n",
    "    if ResNet50Checkpointed is None:\n",
    "        raise RuntimeError(\"ResNet50Checkpointed is not available (torchvision import likely failed).\")\n",
    "\n",
    "    start = max(1, int(start))\n",
    "    max_batch = max(start, int(max_batch))\n",
    "\n",
    "    def can_run(batch_size):\n",
    "        model = None\n",
    "        x = None\n",
    "        target = None\n",
    "        out = None\n",
    "        loss = None\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            model = ResNet50Checkpointed(checkpoint_strategy=strategy).cuda()\n",
    "            model.train()\n",
    "            x = torch.randn(batch_size, 3, 224, 224, device=\"cuda\")\n",
    "            target = torch.randint(0, 1000, (batch_size,), device=\"cuda\")\n",
    "\n",
    "            out = model(x)\n",
    "            loss = nn.CrossEntropyLoss()(out, target)\n",
    "            loss.backward()\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                return False\n",
    "            raise\n",
    "        finally:\n",
    "            try:\n",
    "                if model is not None:\n",
    "                    model.zero_grad(set_to_none=True)\n",
    "            except Exception:\n",
    "                pass\n",
    "            del model, x, target, out, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Exponential search to find an upper bound, then binary search.\n",
    "    max_working = 0\n",
    "    batch = start\n",
    "    while batch <= max_batch and can_run(batch):\n",
    "        max_working = batch\n",
    "        batch *= 2\n",
    "\n",
    "    low = max_working + 1\n",
    "    high = min(batch, max_batch)\n",
    "\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        if can_run(mid):\n",
    "            max_working = mid\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "\n",
    "    return max_working\n",
    "\n",
    "if torch.cuda.is_available() and ResNet50Checkpointed is not None:\n",
    "    print(\"Finding maximum batch size for ResNet50 (224x224 images)\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    strategies = ['none', 'aggressive', 'per_block']\n",
    "    baseline = None\n",
    "\n",
    "    for strategy in strategies:\n",
    "        max_batch = find_max_batch_resnet50(strategy)\n",
    "        if baseline is None:\n",
    "            baseline = max_batch\n",
    "        improvement = (max_batch / baseline) if baseline else float('inf')\n",
    "        print(f\"{strategy:<15}: max batch = {max_batch:>3d} ({improvement:.2f}x)\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nWith checkpointing, you can often fit larger batches.\")\n",
    "else:\n",
    "    print(\"[Run on GPU and ensure torchvision is installed to find max batch sizes]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.5 Complete Training Loop: ResNet50 with Checkpointing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_resnet50_with_checkpointing(\n",
    "    checkpoint_strategy='aggressive',\n",
    "    batch_size=32,\n",
    "    num_epochs=3,\n",
    "    num_samples=256,  # Small for demo\n",
    "    use_amp=True,\n",
    "):\n",
    "    \"\"\"Complete training loop with checkpointing and mixed precision.\"\"\"\n",
    "\n",
    "    if ResNet50Checkpointed is None:\n",
    "        raise RuntimeError(\"ResNet50Checkpointed is not available (torchvision import likely failed).\")\n",
    "\n",
    "    # Create model\n",
    "    model = ResNet50Checkpointed(\n",
    "        num_classes=10,  # Simplified for demo\n",
    "        checkpoint_strategy=checkpoint_strategy,\n",
    "    ).cuda()\n",
    "\n",
    "    # Create synthetic dataset\n",
    "    X = torch.randn(num_samples, 3, 224, 224)\n",
    "    y = torch.randint(0, 10, (num_samples,))\n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler() if (use_amp and torch.cuda.is_available()) else None\n",
    "\n",
    "    print(f\"Training ResNet50 with '{checkpoint_strategy}' checkpointing\")\n",
    "    print(f\"Batch size: {batch_size}, Mixed precision: {use_amp and scaler is not None}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.cuda()\n",
    "            batch_y = batch_y.cuda()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            if scaler is not None:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(batch_x)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: loss = {avg_loss:.4f}, peak memory = {peak_mem:.2f} MB\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "if torch.cuda.is_available() and ResNet50Checkpointed is not None:\n",
    "    model = train_resnet50_with_checkpointing(\n",
    "        checkpoint_strategy='aggressive',\n",
    "        batch_size=32,\n",
    "        num_epochs=3,\n",
    "        use_amp=True,\n",
    "    )\n",
    "else:\n",
    "    print(\"[Run on GPU and ensure torchvision is installed for training demo]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.6 Alternative: Using torchvision's Built-in Support\n",
    "\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "try:\n",
    "    import torchvision.models as models\n",
    "except Exception as e:\n",
    "    models = None\n",
    "    print(\"torchvision is required for this alternative ResNet50 example.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "\n",
    "def _checkpoint_sequential(mod, segments, x):\n",
    "    \"\"\"checkpoint_sequential signature varies across PyTorch versions.\"\"\"\n",
    "    try:\n",
    "        return checkpoint_sequential(mod, segments, x, use_reentrant=False)\n",
    "    except TypeError:\n",
    "        return checkpoint_sequential(mod, segments, x)\n",
    "\n",
    "\n",
    "def resnet50_with_sequential_checkpoint(num_segments=4):\n",
    "    \"\"\"Monkey-patch torchvision ResNet50 to checkpoint its stage blocks.\"\"\"\n",
    "    if models is None:\n",
    "        raise RuntimeError(\"torchvision is not available\")\n",
    "\n",
    "    try:\n",
    "        model = models.resnet50(weights=None)\n",
    "    except TypeError:\n",
    "        model = models.resnet50(pretrained=False)\n",
    "\n",
    "    def checkpointed_forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Checkpoint each stage with user-controlled granularity.\n",
    "        x = _checkpoint_sequential(self.layer1, min(num_segments, len(self.layer1)), x)\n",
    "        x = _checkpoint_sequential(self.layer2, min(num_segments, len(self.layer2)), x)\n",
    "        x = _checkpoint_sequential(self.layer3, min(num_segments, len(self.layer3)), x)\n",
    "        x = _checkpoint_sequential(self.layer4, min(num_segments, len(self.layer4)), x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    import types\n",
    "\n",
    "    model.forward = types.MethodType(checkpointed_forward, model)\n",
    "    return model\n",
    "\n",
    "# Test it\n",
    "if models is not None:\n",
    "    model = resnet50_with_sequential_checkpoint(num_segments=4)\n",
    "    print(\"Alternative approach: Monkey-patch torchvision ResNet50\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Pros:\")\n",
    "    print(\"  - Uses official torchvision model\")\n",
    "    print(\"  - Simple implementation\")\n",
    "    print(\"  - Adjustable granularity via num_segments\")\n",
    "    print()\n",
    "    print(\"Cons:\")\n",
    "    print(\"  - Less control over which blocks to checkpoint\")\n",
    "    print(\"  - Harder to switch strategies dynamically\")\n",
    "    print()\n",
    "    print(\"Use the custom class (ResNet50Checkpointed) for flexible experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (12: ResNet50 Case Study)\n",
    "\n",
    "ResNet50 is a perfect testbed for checkpointing because:\n",
    "- 16 bottleneck blocks across 4 stages\n",
    "- Deep enough to benefit, not so deep it is impractical\n",
    "- Widely used in production\n",
    "\n",
    "Key findings:\n",
    "\n",
    "**Strategy Selection:**\n",
    "- `aggressive` (checkpoint layer3+layer4): Best ROI. These layers have the most parameters and largest feature maps.\n",
    "- `per_block`: Maximum memory savings (~40-50%), but highest compute overhead (~30-40%).\n",
    "- `per_stage`: Middle ground. Good for quick wins.\n",
    "\n",
    "**Practical Impact:**\n",
    "- 1.5-2x larger batch sizes possible\n",
    "- Combined with mixed precision: up to 3x improvement\n",
    "\n",
    "**BatchNorm Caveat:**\n",
    "ResNet blocks use BatchNorm. Checkpointing recomputes forward in backward, which can update BN running stats twice in `train()` mode. If you need strict parity, consider freezing BN stats (set BN layers to eval) or using GroupNorm.\n",
    "\n",
    "**Production Recommendations:**\n",
    "1. Start with `aggressive` strategy\n",
    "2. Combine with mixed precision (AMP)\n",
    "3. Profile before and after\n",
    "4. If still OOM, move to `per_block`\n",
    "\n",
    "The ResNet50Checkpointed class is a solid starting point. Copy it into your codebase and adapt it to your training setup and normalization strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 13. Conclusion\n",
    "\n",
    "Activation checkpointing is a memory-compute trade-off. It trades extra forward passes for reduced memory usage. The trade-off is usually worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.1 Key Takeaways\n",
    "\n",
    "print(\"\"\"\n",
    "KEY TAKEAWAYS\n",
    "=============\n",
    "\n",
    "1. ACTIVATIONS DOMINATE MEMORY\n",
    "   - Not parameters, not gradients\n",
    "   - Scales with batch_size * depth * hidden_size\n",
    "   - Peak memory occurs at start of backward pass\n",
    "\n",
    "2. CHECKPOINTING TRADES COMPUTE FOR MEMORY\n",
    "   - Save some activations (checkpoints)\n",
    "   - Recompute others during backward pass\n",
    "   - Typical: 30-50% memory savings, ~50% compute overhead\n",
    "\n",
    "3. THE SQRT(N) RULE\n",
    "   - Optimal checkpointing: O(sqrt(n)) memory for n layers\n",
    "   - 100 layers -> ~10 checkpoints -> 10x memory reduction\n",
    "\n",
    "4. PRACTICAL PATTERNS\n",
    "   - checkpoint(): Single modules/functions\n",
    "   - checkpoint_sequential(): Sequential models\n",
    "   - Always use use_reentrant=False\n",
    "\n",
    "5. ADVANCED: SELECTIVE ACTIVATION CHECKPOINT (SAC)\n",
    "   - Fine-grained control: choose what to save vs recompute\n",
    "   - Policy functions: MUST_SAVE for expensive ops, PREFER_RECOMPUTE for cheap\n",
    "   - Sweet spot: save matmuls, recompute pointwise ops\n",
    "   - Use aten.<op>.default for correct op matching\n",
    "\n",
    "6. ADVANCED: MEMORY BUDGET API (torch.compile)\n",
    "   - One line: torch._dynamo.config.activation_memory_budget = 0.5\n",
    "   - Automatic pareto-optimal recomputation strategies\n",
    "   - Budget 0 = full AC, Budget 1 = default compile\n",
    "\n",
    "7. COMBINE WITH OTHER TECHNIQUES\n",
    "   - Mixed precision: 2x memory from fp16\n",
    "   - Gradient accumulation: Effective larger batches\n",
    "   - DDP: Scale across GPUs\n",
    "   - Together: Train 4-5x larger models\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.2 Decision Framework: When to Use Checkpointing\n",
    "\n",
    "print(\"\"\"\n",
    "WHEN TO USE ACTIVATION CHECKPOINTING\n",
    "====================================\n",
    "\n",
    "USE IT WHEN:\n",
    "- You are hitting OOM errors\n",
    "- You want larger batch sizes\n",
    "- Your model has 10+ layers\n",
    "- Training time is less critical than memory\n",
    "- You are training transformers or deep CNNs\n",
    "\n",
    "SKIP IT WHEN:\n",
    "- Model fits comfortably in memory\n",
    "- Training time is the bottleneck (not memory)\n",
    "- Model is shallow (< 5 layers)\n",
    "- You need maximum training speed\n",
    "\n",
    "QUICK DECISION TREE:\n",
    "\n",
    "  OOM Error?\n",
    "     |\n",
    "     v\n",
    "  YES --> Use checkpointing\n",
    "     |\n",
    "     v\n",
    "  Want larger batches?\n",
    "     |\n",
    "     v\n",
    "  YES --> Use checkpointing\n",
    "     |\n",
    "     v\n",
    "  Model > 10 layers?\n",
    "     |\n",
    "     v\n",
    "  YES --> Consider checkpointing\n",
    "     |\n",
    "     v\n",
    "  NO --> Probably skip it\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Impressions\n",
    "\n",
    "Activation checkpointing is not magic. It is a simple trade-off executed well.\n",
    "\n",
    "You now understand:\n",
    "- Why activations dominate memory (batch x depth x hidden)\n",
    "- How checkpointing works (recompute instead of store)\n",
    "- When to use it (memory-bound, deep models)\n",
    "- Basic implementation (checkpoint, checkpoint_sequential, use_reentrant=False)\n",
    "- Advanced control with SAC (choose exactly what to save)\n",
    "- Automatic optimization with Memory Budget API (one config line)\n",
    "\n",
    "The landscape of techniques:\n",
    "- **Eager**: Maximum speed, maximum memory\n",
    "- **torch.compile**: Free speedups, some automatic memory savings\n",
    "- **Memory Budget API**: Tunable compile-time optimization (0 to 1)\n",
    "- **Selective AC**: Manual control over save/recompute decisions\n",
    "- **Standard AC**: Maximum memory savings, ~50% compute overhead\n",
    "\n",
    "Start simple. Add complexity only when needed. Measure everything.\n",
    "\n",
    "Every large language model uses some form of activation checkpointing. Now you know exactly how it works and when to use each variant.\n",
    "\n",
    "Go train something bigger."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}