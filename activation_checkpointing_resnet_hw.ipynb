{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Checkpointing: A First Principles Deep DiveMemory is the bottleneck in deep learning. Not compute. Not data. Memory.This notebook tears apart activation checkpointing from first principles. You will understand exactly why activations dominate memory, how checkpointing trades compute for memory, and when this trade-off makes sense.---## The Core ProblemDuring training you do two things:1. **Forward pass**: compute outputs layer by layer2. **Backward pass**: compute gradients using the chain ruleHere's what kills you: backward needs intermediate values from forward. If forward does `y = sin(x)`, backward needs `x` to compute `dy/dx = cos(x)`. PyTorch keeps `x` alive until backward reaches that node. These are \"saved tensors\" in autograd-speak.So activation memory accumulates through forward and peaks at the start of backward. You've got this pile of \"will need later\" tensors sitting in VRAM, waiting.## What Checkpointing Actually DoesThink of training like hiking a trail:**Normal training**: You drop breadcrumbs at every step (store all activations). Easy to retrace (backprop), but you're carrying a giant breadcrumb bag (VRAM).**Checkpointing**: You only drop breadcrumbs at a few checkpoints. On the way back, when you need details between checkpoints, you re-walk that segment.That's literally it. Save fewer tensors in forward, recompute them on demand during backward. Memory goes down, compute goes up.## Concrete ExampleSuppose your forward is:```x --> [f1] --> a --> [f2] --> b --> [f3] --> y```**No checkpointing**: autograd saves `a` and `b` so backward can compute gradients.**Checkpoint after `a`**: you keep `a`, but you don't keep `b`. During backward, when you need `b` to differentiate `f3`, PyTorch reruns `f2(a)` to get it back.Memory goes down (you didn't store `b`). Compute goes up (you recomputed `b`).---## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.0+cu126\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "# 0.1 Environment Check\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---# 1. The Memory ProblemTraining deep neural networks hits a wall. That wall is GPU memory.Four things consume GPU memory during training:1. **Model parameters** (weights and biases)2. **Gradients** (same size as parameters)3. **Optimizer states** (for Adam: 2x parameter size for m and v buffers)4. **Activations** (intermediate outputs from each layer)For Adam/AdamW, the first three combined are 4x the model size (params + grads + 2 momentum buffers). This is fixed cost.Here is the surprise: **activations often dominate anyway**. Not parameters. Not gradients. Not optimizer states. Activations. Why? Because activations scale with batch size. Everything else does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PART 1: THEORETICAL MEMORY BREAKDOWN\n",
      "============================================================\n",
      "Model: 10,496,000 parameters\n",
      "\n",
      "Fixed costs for training with Adam:\n",
      "  Parameter memory:  40.04 MB\n",
      "  Gradient memory:   40.04 MB\n",
      "  Optimizer memory:  80.08 MB (m + v)\n",
      "  Total fixed:       160.16 MB\n",
      "\n",
      "Activation memory by batch size (THEORETICAL):\n",
      "  Batch  32: activations =   1.25 MB (0.8% of total)\n",
      "  Batch 128: activations =   5.00 MB (3.0% of total)\n",
      "  Batch 512: activations =  20.00 MB (11.1% of total)\n",
      "\n",
      "============================================================\n",
      "PART 2: REAL GPU MEASUREMENT\n",
      "============================================================\n",
      "GPU: Tesla T4\n",
      "\n",
      "Batch  64: Peak = 1983.6 MB  (est. activations: 6.3 MB)\n",
      "Batch 256: Peak = 1990.6 MB  (est. activations: 25.2 MB)\n",
      "Batch 512: Peak = 2000.1 MB  (est. activations: 50.3 MB)\n",
      "\n",
      "============================================================\n",
      "PART 3: MEMORY TIMELINE (When Does Memory Peak?)\n",
      "============================================================\n",
      "Model: 134.3M params\n",
      "\n",
      "Memory Timeline:\n",
      "-------------------------------------------------------\n",
      "Model loaded:                         1817.6 MB\n",
      "\n",
      "During forward (activations accumulating):\n",
      "  After layer 1:                          1821.7 MB  (+4.2)\n",
      "  After layer 2:                          1825.9 MB  (+4.2)\n",
      "  After layer 3:                          1830.1 MB  (+4.2)\n",
      "  After layer 4:                          1834.3 MB  (+4.2)\n",
      "  After layer 5:                          1838.5 MB  (+4.2)\n",
      "  After layer 6:                          1842.7 MB  (+4.2)\n",
      "  After layer 7:                          1846.9 MB  (+4.2)\n",
      "  After layer 8:                          1851.1 MB  (+4.2)\n",
      "\n",
      "After forward (all activations live):   1851.1 MB\n",
      "PEAK (activations + gradients):       2362.9 MB  <-- this kills you\n",
      "After backward:                       2358.8 MB\n",
      "After zero_grad:                      1821.7 MB\n",
      "\n",
      "KEY INSIGHT: Activation memory ~33.6 MB dominates during forward pass\n",
      "Memory peaks at start of backward when both activations and gradients exist\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Memory Breakdown: Theory, Measurement, and Timing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ===== PART 1: Theoretical Breakdown =====\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def bytes_to_mb(b):\n",
    "    return b / (1024 * 1024)\n",
    "\n",
    "class DeepNetwork(nn.Module):\n",
    "    def __init__(self, input_size=1024, hidden_size=1024, num_layers=10):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_f = input_size if i == 0 else hidden_size\n",
    "            layers.append(nn.Linear(in_f, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = DeepNetwork(input_size=1024, hidden_size=1024, num_layers=10)\n",
    "num_params = count_parameters(model)\n",
    "\n",
    "# Theoretical memory breakdown for a full training setup with Adam\n",
    "param_mem = num_params * 4  # fp32 parameters\n",
    "grad_mem = param_mem        # gradients\n",
    "optimizer_mem = param_mem * 2  # Adam's m and v buffers\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 1: THEORETICAL MEMORY BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {num_params:,} parameters\")\n",
    "print()\n",
    "print(\"Fixed costs for training with Adam:\")\n",
    "print(f\"  Parameter memory:  {bytes_to_mb(param_mem):.2f} MB\")\n",
    "print(f\"  Gradient memory:   {bytes_to_mb(grad_mem):.2f} MB\")\n",
    "print(f\"  Optimizer memory:  {bytes_to_mb(optimizer_mem):.2f} MB (m + v)\")\n",
    "print(f\"  Total fixed:       {bytes_to_mb(param_mem + grad_mem + optimizer_mem):.2f} MB\")\n",
    "print()\n",
    "print(\"Activation memory by batch size (THEORETICAL):\")\n",
    "fixed_cost = param_mem + grad_mem + optimizer_mem\n",
    "for batch_size in [32, 128, 512]:\n",
    "    act_mem = batch_size * 1024 * 10 * 4\n",
    "    total = fixed_cost + act_mem\n",
    "    act_pct = (act_mem / total) * 100\n",
    "    print(f\"  Batch {batch_size:3d}: activations = {bytes_to_mb(act_mem):6.2f} MB ({act_pct:.1f}% of total)\")\n",
    "\n",
    "# ===== PART 2: Real GPU Measurement =====\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 2: REAL GPU MEASUREMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Wider network to make activations dominate\n",
    "    class WideNetwork(nn.Module):\n",
    "        def __init__(self, hidden_size=4096, num_layers=6):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            for i in range(num_layers):\n",
    "                layers.append(nn.Linear(1024 if i == 0 else hidden_size, hidden_size))\n",
    "                layers.append(nn.ReLU())\n",
    "            self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "\n",
    "    model = WideNetwork().cuda()\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print()\n",
    "    \n",
    "    for batch_size in [64, 256, 512]:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        x = torch.randn(batch_size, 1024).cuda()\n",
    "        output = model(x)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        peak = torch.cuda.max_memory_allocated() / 1e6\n",
    "        act_estimate = batch_size * 4096 * 6 * 4 / 1e6\n",
    "        print(f\"Batch {batch_size:3d}: Peak = {peak:.1f} MB  (est. activations: {act_estimate:.1f} MB)\")\n",
    "        \n",
    "        model.zero_grad()\n",
    "        del x, output, loss\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"[Run on GPU to see measurements]\")\n",
    "\n",
    "# ===== PART 3: Memory Timeline =====\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 3: MEMORY TIMELINE (When Does Memory Peak?)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class InstrumentedNetwork(nn.Module):\n",
    "    def __init__(self, num_layers=5, hidden=4096):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(hidden, hidden) for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, track=False):\n",
    "        memory_log = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = torch.relu(layer(x))\n",
    "            if track and torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "                memory_log.append(torch.cuda.memory_allocated() / 1e6)\n",
    "        return x, memory_log\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = InstrumentedNetwork(num_layers=8, hidden=4096).cuda()\n",
    "    x = torch.randn(256, 4096).cuda()\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model: {num_params/1e6:.1f}M params\")\n",
    "    print()\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    mem_model_only = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    output, fwd_mem = model(x, track=True)\n",
    "    mem_after_forward = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1e6\n",
    "    mem_after_backward = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    model.zero_grad(set_to_none=True)\n",
    "    mem_after_zero_grad = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    print(\"Memory Timeline:\")\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"{'Model loaded:':<35} {mem_model_only:>8.1f} MB\")\n",
    "    print()\n",
    "    print(\"During forward (activations accumulating):\")\n",
    "    for i, mem in enumerate(fwd_mem):\n",
    "        delta = mem - (fwd_mem[i-1] if i > 0 else mem_model_only)\n",
    "        print(f\"  After layer {i+1}:{'':<23} {mem:>8.1f} MB  (+{delta:.1f})\")\n",
    "    print()\n",
    "    print(f\"{'After forward (all activations live):':<35} {mem_after_forward:>8.1f} MB\")\n",
    "    print(f\"{'PEAK (activations + gradients):':<35} {peak_memory:>8.1f} MB  <-- this kills you\")\n",
    "    print(f\"{'After backward:':<35} {mem_after_backward:>8.1f} MB\")\n",
    "    print(f\"{'After zero_grad:':<35} {mem_after_zero_grad:>8.1f} MB\")\n",
    "    print()\n",
    "    \n",
    "    activation_mem = mem_after_forward - mem_model_only\n",
    "    print(f\"KEY INSIGHT: Activation memory ~{activation_mem:.1f} MB dominates during forward pass\")\n",
    "    print(\"Memory peaks at start of backward when both activations and gradients exist\")\n",
    "else:\n",
    "    print(\"[Run on GPU to see memory timeline]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---# 2. Understanding Activation CheckpointingActivation checkpointing is a memory-compute trade-off. Save memory by not storing activations. Pay for it by recomputing them during backprop.To understand this, you need to know what activations are and why backprop needs them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored for backprop:\n",
      "  z1: torch.Size([1, 4]) - needed to compute relu gradient\n",
      "  a1: torch.Size([1, 4]) - needed to compute W2 gradient\n",
      "\n",
      "Gradients computed:\n",
      "  W1.grad: torch.Size([4, 4])\n",
      "  W2.grad: torch.Size([4, 4])\n",
      "\n",
      "The chain rule requires intermediate values.\n",
      "PyTorch keeps them alive until backward finishes.\n",
      "This is why activations accumulate through forward pass.\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Why Backprop Needs Activations\n",
    "\n",
    "import torch\n",
    "\n",
    "# y = W2 * relu(W1 * x)\n",
    "# Gradient w.r.t. W1 needs the pre-activation W1*x to compute relu'(W1*x)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "W1 = torch.randn(4, 4, requires_grad=True)\n",
    "W2 = torch.randn(4, 4, requires_grad=True)\n",
    "x = torch.randn(1, 4)\n",
    "\n",
    "# Forward pass stores these\n",
    "z1 = x @ W1.T           # Pre-activation (needed for ReLU grad)\n",
    "a1 = torch.relu(z1)     # Activation (needed for W2 grad)\n",
    "y = a1 @ W2.T\n",
    "\n",
    "print(\"Stored for backprop:\")\n",
    "print(f\"  z1: {z1.shape} - needed to compute relu gradient\")\n",
    "print(f\"  a1: {a1.shape} - needed to compute W2 gradient\")\n",
    "\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradients computed:\")\n",
    "print(f\"  W1.grad: {W1.grad.shape}\")\n",
    "print(f\"  W2.grad: {W2.grad.shape}\")\n",
    "print()\n",
    "print(\"The chain rule requires intermediate values.\")\n",
    "print(\"PyTorch keeps them alive until backward finishes.\")\n",
    "print(\"This is why activations accumulate through forward pass.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Training:\n",
      "  Forward: x -> a1 -> a2 -> a3 -> loss\n",
      "           [store] [store] [store]\n",
      "  Backward: uses stored a1, a2, a3\n",
      "  Memory: O(n) activations\n",
      "\n",
      "With Checkpointing:\n",
      "  Forward: x -> a1 -> a2 -> a3 -> loss\n",
      "           [save]       [save]\n",
      "  Backward: recompute a2 from a1, then use\n",
      "  Memory: O(sqrt(n)) with optimal placement\n",
      "\n",
      "For 100 layers:\n",
      "  Standard: 100 activations stored\n",
      "  Checkpointed: ~10 activations stored (at checkpoint boundaries)\n",
      "  Memory reduction: 10x\n",
      "\n",
      "Compute analysis:\n",
      "  Without checkpointing: n forward + n backward = 2n\n",
      "  With checkpointing:    n forward + n backward + n recompute = 3n\n",
      "  Compute overhead: ~50% of total training time\n",
      "\n",
      "KEY: The sqrt(n) rule - optimal checkpointing reduces memory from O(n) to O(sqrt(n))\n"
     ]
    }
   ],
   "source": [
    "# 2.2 The Core Trade-off: Store vs Recompute\n",
    "\n",
    "import math\n",
    "\n",
    "print(\"Standard Training:\")\n",
    "print(\"  Forward: x -> a1 -> a2 -> a3 -> loss\")\n",
    "print(\"           [store] [store] [store]\")\n",
    "print(\"  Backward: uses stored a1, a2, a3\")\n",
    "print(\"  Memory: O(n) activations\")\n",
    "print()\n",
    "print(\"With Checkpointing:\")\n",
    "print(\"  Forward: x -> a1 -> a2 -> a3 -> loss\")\n",
    "print(\"           [save]       [save]\")\n",
    "print(\"  Backward: recompute a2 from a1, then use\")\n",
    "print(\"  Memory: O(sqrt(n)) with optimal placement\")\n",
    "print()\n",
    "\n",
    "num_layers = 100\n",
    "optimal = int(math.sqrt(num_layers))\n",
    "\n",
    "print(f\"For {num_layers} layers:\")\n",
    "print(f\"  Standard: {num_layers} activations stored\")\n",
    "print(f\"  Checkpointed: ~{optimal} activations stored (at checkpoint boundaries)\")\n",
    "print(f\"  Memory reduction: {num_layers/optimal:.0f}x\")\n",
    "print()\n",
    "print(\"Compute analysis:\")\n",
    "print(f\"  Without checkpointing: n forward + n backward = 2n\")\n",
    "print(f\"  With checkpointing:    n forward + n backward + n recompute = 3n\")\n",
    "print(f\"  Compute overhead: ~50% of total training time\")\n",
    "print()\n",
    "print(\"KEY: The sqrt(n) rule - optimal checkpointing reduces memory from O(n) to O(sqrt(n))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---# 3. PyTorch API - Basic UsagePyTorch provides `torch.utils.checkpoint.checkpoint()` for activation checkpointing. The basic pattern is simple: wrap the function you want to checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Checkpoint Comparison (small model)\n",
      "--------------------------------------------------\n",
      "WITHOUT Checkpointing: 1834.36 MB\n",
      "WITH Checkpointing:    1815.92 MB\n",
      "\n",
      "NOTE: Small model shows minimal difference.\n",
      "Savings become significant with deeper models (see Section 4).\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Basic Checkpoint: Side-by-Side Comparison\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# ===== WITHOUT Checkpointing =====\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1024, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# ===== WITH Checkpointing =====\n",
    "class CheckpointedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1024, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run layer1 normally (so input to checkpointed region requires grad)\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        # Checkpoint layer2: activations won't be stored, will be recomputed\n",
    "        x = checkpoint(lambda t: torch.relu(self.layer2(t)), x, use_reentrant=False)\n",
    "        return x\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Test without checkpointing\n",
    "    model1 = SimpleModel().cuda()\n",
    "    x1 = torch.randn(1, 1024).cuda()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out1 = model1(x1)\n",
    "    out1.sum().backward()\n",
    "    mem_no_ckpt = torch.cuda.max_memory_allocated() / 1e6\n",
    "    del model1, x1, out1\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Test with checkpointing\n",
    "    model2 = CheckpointedModel().cuda()\n",
    "    x2 = torch.randn(1, 1024).cuda()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out2 = model2(x2)\n",
    "    out2.sum().backward()\n",
    "    mem_with_ckpt = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    print(\"Basic Checkpoint Comparison (small model)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"WITHOUT Checkpointing: {mem_no_ckpt:.2f} MB\")\n",
    "    print(f\"WITH Checkpointing:    {mem_with_ckpt:.2f} MB\")\n",
    "    print()\n",
    "    print(\"NOTE: Small model shows minimal difference.\")\n",
    "    print(\"Savings become significant with deeper models (see Section 4).\")\n",
    "else:\n",
    "    print(\"[Run on GPU to see memory comparison]\")\n",
    "    print()\n",
    "    print(\"API Usage:\")\n",
    "    print(\"  checkpoint(fn, *args, use_reentrant=False)\")\n",
    "    print(\"  - fn: callable to checkpoint\")\n",
    "    print(\"  - Always use use_reentrant=False (modern API)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---# 4. PyTorch API - Sequential ModelsFor deep sequential models, use `checkpoint_sequential`. It splits the model into segments and checkpoints each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 4.1 Deep Model Comparison: The Real Difference\n\nimport gc\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\n\nclass FFNBlock(nn.Module):\n    \"\"\"FFN block with 4x expansion - the memory-hungry part of transformers.\"\"\"\n    def __init__(self, dim):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, dim * 4)\n        self.fc2 = nn.Linear(dim * 4, dim)\n    \n    def forward(self, x):\n        return self.fc2(F.gelu(self.fc1(x)))\n\nif torch.cuda.is_available():\n    num_layers = 12\n    hidden = 2048\n    batch_size = 128\n    \n    # ===== Test 1: WITHOUT checkpointing =====\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    \n    model1 = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n    x1 = torch.randn(batch_size, hidden, device='cuda', requires_grad=True)\n    \n    out = x1\n    for block in model1:\n        out = block(out)\n    out.sum().backward()\n    \n    mem_no_ckpt = torch.cuda.max_memory_allocated() / 1e6\n    \n    del model1, x1, out\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    \n    # ===== Test 2: WITH checkpointing =====\n    model2 = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n    x2 = torch.randn(batch_size, hidden, device='cuda', requires_grad=True)\n    \n    out2 = x2\n    for block in model2:\n        out2 = checkpoint(block, out2, use_reentrant=False)\n    out2.sum().backward()\n    \n    mem_ckpt = torch.cuda.max_memory_allocated() / 1e6\n    \n    saved = mem_no_ckpt - mem_ckpt\n    pct = (saved / mem_no_ckpt) * 100 if mem_no_ckpt > 0 else 0\n    \n    print(f\"Config: {num_layers} FFN blocks (4x expansion), hidden={hidden}, batch={batch_size}\")\n    print(\"=\" * 60)\n    print()\n    print(f\"Peak memory WITHOUT checkpointing: {mem_no_ckpt:,.0f} MB\")\n    print(f\"Peak memory WITH checkpointing:    {mem_ckpt:,.0f} MB\")\n    print(f\"Memory saved:                      {saved:,.0f} MB ({pct:.1f}%)\")\n    print()\n    print(\"WHY THIS MATTERS:\")\n    print(f\"  - You could increase batch size by ~{int(saved/100)*10}% before hitting OOM\")\n    print(f\"  - Or fit ~{pct:.0f}% more layers in the same memory\")\n    print()\n    print(\"TRADE-OFF: ~50% more compute (one extra forward pass per block)\")\nelse:\n    print(\"[Run on GPU]\")"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 1024])\n",
      "Model split into 2 checkpoint segments\n",
      "Each segment recomputes activations during backward pass\n",
      "\n",
      "API: checkpoint_sequential(sequential_model, segments, input, use_reentrant=False)\n",
      "- Convenience wrapper for nn.Sequential models\n",
      "- Works well with FSDP and DDP\n"
     ]
    }
   ],
   "source": [
    "# 4.2 Using checkpoint_sequential\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "# A deep sequential model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1024, 1024), nn.ReLU(),\n",
    "    nn.Linear(1024, 1024), nn.ReLU(),\n",
    "    nn.Linear(1024, 1024), nn.ReLU(),\n",
    "    nn.Linear(1024, 1024), nn.ReLU(),\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 1024)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    x = x.cuda()\n",
    "\n",
    "# Split into 2 checkpoint segments\n",
    "segments = 2\n",
    "output = checkpoint_sequential(model, segments, x, use_reentrant=False)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Model split into {segments} checkpoint segments\")\n",
    "print(\"Each segment recomputes activations during backward pass\")\n",
    "print()\n",
    "print(\"API: checkpoint_sequential(sequential_model, segments, input, use_reentrant=False)\")\n",
    "print(\"- Convenience wrapper for nn.Sequential models\")\n",
    "print(\"- Works well with FSDP and DDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---# 5. Practical Pattern - TransformersIn transformers, the feed-forward block uses the most memory (4x expansion). This is where checkpointing helps most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 64, 512])\n",
      "Output shape: torch.Size([8, 64, 512])\n",
      "\n",
      "Feed-forward block is checkpointed (it uses the most memory)\n",
      "Attention is NOT checkpointed (expensive to recompute)\n",
      "\n",
      "Pattern: Checkpoint the FFN, keep attention cached\n",
      "This is how most transformer implementations do it.\n"
     ]
    }
   ],
   "source": [
    "# 5.1 Transformer Training with Checkpointing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_size * 4, embed_size)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        # Feed-forward with residual (checkpointed)\n",
    "        ff_out = checkpoint(self.feed_forward, x, use_reentrant=False)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "embed_size = 512\n",
    "seq_length = 64\n",
    "batch_size = 8\n",
    "\n",
    "block = TransformerBlock(embed_size)\n",
    "if torch.cuda.is_available():\n",
    "    block = block.cuda()\n",
    "\n",
    "x = torch.randn(batch_size, seq_length, embed_size)\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "\n",
    "output = block(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print()\n",
    "print(\"Feed-forward block is checkpointed (it uses the most memory)\")\n",
    "print(\"Attention is NOT checkpointed (expensive to recompute)\")\n",
    "print()\n",
    "print(\"Pattern: Checkpoint the FFN, keep attention cached\")\n",
    "print(\"This is how most transformer implementations do it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---# 6. Debugging and Best PracticesCheckpointing has gotchas. Here are the common ones and how to avoid them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAD: Error - element 0 of tensors does not require grad and does not have a grad_fn\n",
      "GOOD: x.grad = tensor([-0.0629,  4.8470,  1.8998, -1.8578])\n",
      "\n",
      "RULE: Everything inside a checkpointed function must be differentiable.\n",
      "No torch.no_grad(), no tensor.detach(), no in-place ops that break autograd.\n"
     ]
    }
   ],
   "source": [
    "# 6.1 Pitfall: Non-Differentiable Operations\n",
    "\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# BAD: torch.no_grad() inside checkpointed function\n",
    "def bad_forward(x):\n",
    "    with torch.no_grad():  # This breaks gradient computation!\n",
    "        x = x ** 2\n",
    "    return x\n",
    "\n",
    "# GOOD: Keep everything differentiable\n",
    "def good_forward(x):\n",
    "    x = x ** 2  # No torch.no_grad()\n",
    "    return x\n",
    "\n",
    "x = torch.randn(4, requires_grad=True)\n",
    "\n",
    "# This will fail or give wrong gradients\n",
    "try:\n",
    "    y_bad = checkpoint(bad_forward, x, use_reentrant=False)\n",
    "    y_bad.sum().backward()\n",
    "    print(\"BAD: Gradients might be zero or wrong\")\n",
    "except Exception as e:\n",
    "    print(f\"BAD: Error - {e}\")\n",
    "\n",
    "# This works\n",
    "x = torch.randn(4, requires_grad=True)\n",
    "y_good = checkpoint(good_forward, x, use_reentrant=False)\n",
    "y_good.sum().backward()\n",
    "print(f\"GOOD: x.grad = {x.grad}\")\n",
    "print()\n",
    "print(\"RULE: Everything inside a checkpointed function must be differentiable.\")\n",
    "print(\"No torch.no_grad(), no tensor.detach(), no in-place ops that break autograd.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 32])\n",
      "RNG state preserved: dropout mask matches between forward and recompute\n",
      "\n",
      "KEY: preserve_rng_state=True is the default for use_reentrant=False\n",
      "Dropout and other random ops work correctly without extra configuration.\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Pitfall: Random Operations (Dropout)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# Problem: Dropout uses random masks.\n",
    "# If RNG state is NOT preserved, the recomputed forward in backward\n",
    "# will sample a different mask than the original forward.\n",
    "class ModelWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(32, 32)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)  # Random! Different each forward pass\n",
    "        return x\n",
    "\n",
    "model = ModelWithDropout()\n",
    "x = torch.randn(4, 32, requires_grad=True)\n",
    "\n",
    "# Fix: preserve RNG state (default) so recomputation matches the original forward.\n",
    "y = checkpoint(model, x, use_reentrant=False, preserve_rng_state=True)\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(\"RNG state preserved: dropout mask matches between forward and recompute\")\n",
    "print()\n",
    "print(\"KEY: preserve_rng_state=True is the default for use_reentrant=False\")\n",
    "print(\"Dropout and other random ops work correctly without extra configuration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEST PRACTICES FOR ACTIVATION CHECKPOINTING\n",
      "============================================\n",
      "\n",
      "1. USE use_reentrant=False\n",
      "   - Modern API, handles edge cases better\n",
      "   - checkpoint(fn, x, use_reentrant=False)\n",
      "\n",
      "2. CHECKPOINT LARGE BLOCKS, NOT SMALL LAYERS\n",
      "   - Overhead of checkpoint() call is non-trivial\n",
      "   - Group 2-4 layers into blocks, then checkpoint blocks\n",
      "\n",
      "3. AVOID NESTED CHECKPOINTS\n",
      "   - Don't checkpoint inside checkpointed functions\n",
      "   - Leads to exponential recomputation\n",
      "\n",
      "4. KEEP EVERYTHING DIFFERENTIABLE\n",
      "   - No torch.no_grad() inside checkpointed regions\n",
      "   - No tensor detaching\n",
      "   - No in-place ops that break autograd\n",
      "\n",
      "5. TEST GRADIENTS FIRST\n",
      "   - Compare gradients with and without checkpointing\n",
      "   - They should be identical (within float precision)\n",
      "\n",
      "6. PROFILE MEMORY\n",
      "   - Use torch.cuda.memory_stats() to verify savings\n",
      "   - Peak memory is what matters\n",
      "\n",
      "7. CONSIDER CHECKPOINT PLACEMENT\n",
      "   - Middle layers often have largest activations\n",
      "   - Checkpoint those first\n",
      "\n",
      "Gradients match: True\n",
      "Max difference: 0.00e+00\n",
      "\n",
      "ALWAYS verify gradients when adding checkpointing to new code.\n"
     ]
    }
   ],
   "source": [
    "# 6.3 Best Practices Summary\n",
    "\n",
    "print(\"\"\"\n",
    "BEST PRACTICES FOR ACTIVATION CHECKPOINTING\n",
    "============================================\n",
    "\n",
    "1. USE use_reentrant=False\n",
    "   - Modern API, handles edge cases better\n",
    "   - checkpoint(fn, x, use_reentrant=False)\n",
    "\n",
    "2. CHECKPOINT LARGE BLOCKS, NOT SMALL LAYERS\n",
    "   - Overhead of checkpoint() call is non-trivial\n",
    "   - Group 2-4 layers into blocks, then checkpoint blocks\n",
    "\n",
    "3. AVOID NESTED CHECKPOINTS\n",
    "   - Don't checkpoint inside checkpointed functions\n",
    "   - Leads to exponential recomputation\n",
    "\n",
    "4. KEEP EVERYTHING DIFFERENTIABLE\n",
    "   - No torch.no_grad() inside checkpointed regions\n",
    "   - No tensor detaching\n",
    "   - No in-place ops that break autograd\n",
    "\n",
    "5. TEST GRADIENTS FIRST\n",
    "   - Compare gradients with and without checkpointing\n",
    "   - They should be identical (within float precision)\n",
    "\n",
    "6. PROFILE MEMORY\n",
    "   - Use torch.cuda.memory_stats() to verify savings\n",
    "   - Peak memory is what matters\n",
    "\n",
    "7. CONSIDER CHECKPOINT PLACEMENT\n",
    "   - Middle layers often have largest activations\n",
    "   - Checkpoint those first\n",
    "\"\"\")\n",
    "\n",
    "# Gradient verification example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "model = nn.Linear(64, 64)\n",
    "x = torch.randn(8, 64, requires_grad=True)\n",
    "\n",
    "# Without checkpoint\n",
    "x1 = x.clone().detach().requires_grad_(True)\n",
    "y1 = model(x1)\n",
    "y1.sum().backward()\n",
    "grad1 = x1.grad.clone()\n",
    "\n",
    "# With checkpoint\n",
    "x2 = x.clone().detach().requires_grad_(True)\n",
    "y2 = checkpoint(model, x2, use_reentrant=False)\n",
    "y2.sum().backward()\n",
    "grad2 = x2.grad.clone()\n",
    "\n",
    "print(f\"Gradients match: {torch.allclose(grad1, grad2)}\")\n",
    "print(f\"Max difference: {(grad1 - grad2).abs().max():.2e}\")\n",
    "print()\n",
    "print(\"ALWAYS verify gradients when adding checkpointing to new code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---# 7. Selective Activation Checkpoint (SAC)Standard checkpointing is all-or-nothing. Every op in the checkpointed region gets recomputed during backward.Selective Activation Checkpoint (SAC) gives you granular control. You choose which operations to save and which to recompute.Why does this matter? Not all operations are equal:- Matmuls are expensive to recompute- Pointwise ops (relu, sigmoid) are cheap- Attention is very expensiveSAC lets you save the expensive ones and recompute the cheap ones. Best of both worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheckpointPolicy has four options:\n",
      "--------------------------------------------------\n",
      "\n",
      "MUST_SAVE:\n",
      "  - Always save this op's output\n",
      "  - Never recompute it\n",
      "  - Use for expensive ops (matmul, attention)\n",
      "\n",
      "PREFER_SAVE:\n",
      "  - Save if possible\n",
      "  - torch.compile may override this\n",
      "\n",
      "MUST_RECOMPUTE:\n",
      "  - Always recompute this op\n",
      "  - Never save its output\n",
      "\n",
      "PREFER_RECOMPUTE:\n",
      "  - Recompute if possible\n",
      "  - torch.compile may override this\n",
      "  - Use for cheap ops (relu, elementwise)\n",
      "\n",
      "The MUST_ variants are strict. The PREFER_ variants are hints.\n",
      "\n",
      "RULE: MUST_SAVE for expensive ops, PREFER_RECOMPUTE for cheap ops.\n"
     ]
    }
   ],
   "source": [
    "# 7.1 The CheckpointPolicy Enum\n",
    "\n",
    "try:\n",
    "    from torch.utils.checkpoint import CheckpointPolicy\n",
    "except Exception as e:\n",
    "    CheckpointPolicy = None\n",
    "    print(\"CheckpointPolicy not available in this PyTorch build.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "if CheckpointPolicy is not None:\n",
    "    print(\"CheckpointPolicy has four options:\")\n",
    "    print(\"-\" * 50)\n",
    "    print()\n",
    "    print(\"MUST_SAVE:\")\n",
    "    print(\"  - Always save this op's output\")\n",
    "    print(\"  - Never recompute it\")\n",
    "    print(\"  - Use for expensive ops (matmul, attention)\")\n",
    "    print()\n",
    "    print(\"PREFER_SAVE:\")\n",
    "    print(\"  - Save if possible\")\n",
    "    print(\"  - torch.compile may override this\")\n",
    "    print()\n",
    "    print(\"MUST_RECOMPUTE:\")\n",
    "    print(\"  - Always recompute this op\")\n",
    "    print(\"  - Never save its output\")\n",
    "    print()\n",
    "    print(\"PREFER_RECOMPUTE:\")\n",
    "    print(\"  - Recompute if possible\")\n",
    "    print(\"  - torch.compile may override this\")\n",
    "    print(\"  - Use for cheap ops (relu, elementwise)\")\n",
    "    print()\n",
    "    print(\"The MUST_ variants are strict. The PREFER_ variants are hints.\")\n",
    "    print()\n",
    "    print(\"RULE: MUST_SAVE for expensive ops, PREFER_RECOMPUTE for cheap ops.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two SAC Policies Defined:\n",
      "============================================================\n",
      "\n",
      "Policy 1: policy_save_matmuls (Conservative)\n",
      "  Saves: mm, bmm, addmm\n",
      "  Recomputes: relu, gelu, sigmoid, layernorm, etc.\n",
      "  Use when: Speed is important, memory savings secondary\n",
      "\n",
      "Policy 2: policy_save_all_expensive (Aggressive)\n",
      "  Saves: matmuls + convolutions + attention + upsampling\n",
      "  Recomputes: only cheap pointwise ops\n",
      "  Use when: Maximum memory savings needed\n",
      "\n",
      "NOTE: Always match against .default (e.g., aten.mm.default)\n",
      "to match the actual ops passed to the policy function.\n"
     ]
    }
   ],
   "source": [
    "# 7.2 Policy Functions: What to Save vs Recompute\n",
    "\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from torch.utils.checkpoint import CheckpointPolicy\n",
    "except Exception as e:\n",
    "    CheckpointPolicy = None\n",
    "    print(\"CheckpointPolicy not available; skipping SAC policy definition.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "aten = torch.ops.aten\n",
    "\n",
    "def _maybe_default(op_name: str):\n",
    "    \"\"\"Return torch.ops.aten.<op_name>.default if it exists, else None.\"\"\"\n",
    "    try:\n",
    "        return getattr(getattr(aten, op_name), \"default\")\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "# ===== Policy 1: Conservative - Save only matmuls =====\n",
    "compute_intensive_ops_basic = [\n",
    "    _maybe_default(\"mm\"),\n",
    "    _maybe_default(\"bmm\"),\n",
    "    _maybe_default(\"addmm\"),\n",
    "]\n",
    "compute_intensive_ops_basic = [op for op in compute_intensive_ops_basic if op is not None]\n",
    "\n",
    "def policy_save_matmuls(ctx, op, *args, **kwargs):\n",
    "    \"\"\"Save matmuls, recompute everything else.\"\"\"\n",
    "    if CheckpointPolicy is None:\n",
    "        raise RuntimeError(\"CheckpointPolicy is not available in this PyTorch build.\")\n",
    "    if op in compute_intensive_ops_basic:\n",
    "        return CheckpointPolicy.MUST_SAVE\n",
    "    return CheckpointPolicy.PREFER_RECOMPUTE\n",
    "\n",
    "# ===== Policy 2: Aggressive - Save all expensive ops =====\n",
    "op_names = [\n",
    "    \"mm\", \"bmm\", \"addmm\", \"convolution\", \"upsample_bilinear2d\",\n",
    "    \"_scaled_mm\", \"linear\",\n",
    "    \"_scaled_dot_product_flash_attention\",\n",
    "    \"_scaled_dot_product_efficient_attention\",\n",
    "]\n",
    "\n",
    "compute_intensive_ops_full = []\n",
    "missing = []\n",
    "for name in op_names:\n",
    "    op = _maybe_default(name)\n",
    "    if op is None:\n",
    "        missing.append(name)\n",
    "    else:\n",
    "        compute_intensive_ops_full.append(op)\n",
    "\n",
    "def policy_save_all_expensive(ctx, op, *args, **kwargs):\n",
    "    \"\"\"Save all compute-intensive ops, including attention (when present).\"\"\"\n",
    "    if CheckpointPolicy is None:\n",
    "        raise RuntimeError(\"CheckpointPolicy is not available in this PyTorch build.\")\n",
    "    if op in compute_intensive_ops_full:\n",
    "        return CheckpointPolicy.MUST_SAVE\n",
    "    return CheckpointPolicy.PREFER_RECOMPUTE\n",
    "\n",
    "print(\"Two SAC Policies Defined:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Policy 1: policy_save_matmuls (Conservative)\")\n",
    "print(\"  Saves: mm, bmm, addmm\")\n",
    "print(\"  Recomputes: relu, gelu, sigmoid, layernorm, etc.\")\n",
    "print(\"  Use when: Speed is important, memory savings secondary\")\n",
    "print()\n",
    "print(\"Policy 2: policy_save_all_expensive (Aggressive)\")\n",
    "print(\"  Saves: matmuls + convolutions + attention + upsampling\")\n",
    "print(\"  Recomputes: only cheap pointwise ops\")\n",
    "if missing:\n",
    "    print(f\"  (Missing ops in this build: {', '.join(missing)})\")\n",
    "print(\"  Use when: Maximum memory savings needed\")\n",
    "print()\n",
    "print(\"NOTE: Always match against .default (e.g., aten.mm.default)\")\n",
    "print(\"to match the actual ops passed to the policy function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAC API Usage:\n",
      "--------------------------------------------------\n",
      "1. Define policy_fn(ctx, op, *args, **kwargs) -> CheckpointPolicy\n",
      "2. Create context_fn with functools.partial\n",
      "3. Pass context_fn to checkpoint(..., context_fn=context_fn)\n",
      "\n",
      "Output shape: torch.Size([32, 64])\n",
      "Matmuls saved. Cheap activations will be recomputed during backward.\n",
      "\n",
      "SHORTCUT: Use allowlist_policy(ops) to avoid boilerplate.\n"
     ]
    }
   ],
   "source": [
    "# 7.3 Using SAC in Practice\n",
    "\n",
    "import functools\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "try:\n",
    "    from torch.utils.checkpoint import CheckpointPolicy, create_selective_checkpoint_contexts\n",
    "except Exception as e:\n",
    "    CheckpointPolicy = None\n",
    "    create_selective_checkpoint_contexts = None\n",
    "    print(\"Selective checkpointing APIs not available in this PyTorch build.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "if CheckpointPolicy is not None and create_selective_checkpoint_contexts is not None:\n",
    "    aten = torch.ops.aten\n",
    "\n",
    "    def _maybe_default(op_name: str):\n",
    "        try:\n",
    "            return getattr(getattr(aten, op_name), \"default\")\n",
    "        except AttributeError:\n",
    "            return None\n",
    "\n",
    "    # Build op allowlist dynamically\n",
    "    ops_to_save = [\n",
    "        _maybe_default(\"mm\"),\n",
    "        _maybe_default(\"bmm\"),\n",
    "        _maybe_default(\"addmm\"),\n",
    "    ]\n",
    "    ops_to_save = [op for op in ops_to_save if op is not None]\n",
    "\n",
    "    # ===== Method 1: Explicit policy function =====\n",
    "    def policy_fn(ctx, op, *args, **kwargs):\n",
    "        if op in ops_to_save:\n",
    "            return CheckpointPolicy.MUST_SAVE\n",
    "        return CheckpointPolicy.PREFER_RECOMPUTE\n",
    "\n",
    "    # ===== Method 2: Allowlist shortcut =====\n",
    "    def allowlist_policy(ops):\n",
    "        ops = set(ops)\n",
    "        def _policy(ctx, op, *args, **kwargs):\n",
    "            if op in ops:\n",
    "                return CheckpointPolicy.MUST_SAVE\n",
    "            return CheckpointPolicy.PREFER_RECOMPUTE\n",
    "        return _policy\n",
    "\n",
    "    # Create the context function\n",
    "    context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n",
    "\n",
    "    def _run_checkpoint(fn, *args, context_fn=None):\n",
    "        \"\"\"Call checkpoint() with best-effort compatibility across versions.\"\"\"\n",
    "        kwargs = {\"use_reentrant\": False}\n",
    "        if context_fn is not None:\n",
    "            kwargs[\"context_fn\"] = context_fn\n",
    "        try:\n",
    "            return checkpoint(fn, *args, **kwargs)\n",
    "        except TypeError:\n",
    "            kwargs.pop(\"context_fn\", None)\n",
    "            try:\n",
    "                return checkpoint(fn, *args, **kwargs)\n",
    "            except TypeError:\n",
    "                kwargs.pop(\"use_reentrant\", None)\n",
    "                return checkpoint(fn, *args)\n",
    "\n",
    "    def forward_fn(x, weight1, weight2):\n",
    "        x = torch.mm(x, weight1)   # Saved (matmul)\n",
    "        x = torch.relu(x)          # Recomputed (cheap)\n",
    "        x = torch.mm(x, weight2)   # Saved (matmul)\n",
    "        x = torch.sigmoid(x)       # Recomputed (cheap)\n",
    "        return x\n",
    "\n",
    "    x = torch.randn(32, 64, requires_grad=True)\n",
    "    w1 = torch.randn(64, 64, requires_grad=True)\n",
    "    w2 = torch.randn(64, 64, requires_grad=True)\n",
    "\n",
    "    output = _run_checkpoint(forward_fn, x, w1, w2, context_fn=context_fn)\n",
    "\n",
    "    print(\"SAC API Usage:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"1. Define policy_fn(ctx, op, *args, **kwargs) -> CheckpointPolicy\")\n",
    "    print(\"2. Create context_fn with functools.partial\")\n",
    "    print(\"3. Pass context_fn to checkpoint(..., context_fn=context_fn)\")\n",
    "    print()\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"Matmuls saved. Cheap activations will be recomputed during backward.\")\n",
    "    print()\n",
    "    print(\"SHORTCUT: Use allowlist_policy(ops) to avoid boilerplate.\")\n",
    "else:\n",
    "    print(\"[Skipping SAC demo: missing CheckpointPolicy/create_selective_checkpoint_contexts]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer FFN: dim=2048, batch=64, seq=128\n",
      "----------------------------------------------------------------------\n",
      "Mode            Peak Memory (MB)   Time (ms/iter) \n",
      "----------------------------------------------------------------------\n",
      "No Checkpoint   2567.75            448.69         \n",
      "Standard AC     2567.75            528.41         \n",
      "Selective AC    2567.75            462.01         \n",
      "----------------------------------------------------------------------\n",
      "Standard AC: recomputes everything inside the checkpointed region\n",
      "Selective AC (when available): can save matmuls and recompute only GELU\n"
     ]
    }
   ],
   "source": [
    "# 7.4 SAC vs Standard AC: Memory and Time Comparison\n",
    "\n",
    "import functools\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "try:\n",
    "    from torch.utils.checkpoint import CheckpointPolicy, create_selective_checkpoint_contexts\n",
    "except Exception as e:\n",
    "    CheckpointPolicy = None\n",
    "    create_selective_checkpoint_contexts = None\n",
    "    print(\"Selective checkpointing APIs not available in this PyTorch build.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "class TransformerFFN(nn.Module):\n",
    "    \"\"\"Feed-forward block from a transformer.\"\"\"\n",
    "    def __init__(self, dim=1024, expansion=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * expansion)\n",
    "        self.fc2 = nn.Linear(dim * expansion, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)  # Cheap pointwise\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def _run_checkpoint(fn, *args, context_fn=None):\n",
    "    \"\"\"Call checkpoint() with best-effort compatibility across versions.\"\"\"\n",
    "    kwargs = {\"use_reentrant\": False}\n",
    "    if context_fn is not None:\n",
    "        kwargs[\"context_fn\"] = context_fn\n",
    "    try:\n",
    "        return checkpoint(fn, *args, **kwargs)\n",
    "    except TypeError:\n",
    "        kwargs.pop(\"context_fn\", None)\n",
    "        try:\n",
    "            return checkpoint(fn, *args, **kwargs)\n",
    "        except TypeError:\n",
    "            return checkpoint(fn, *args)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dim = 2048\n",
    "    batch = 64\n",
    "    seq = 128\n",
    "\n",
    "    # Add a small stem so the checkpointed region receives activations that require grad.\n",
    "    stem = nn.Linear(dim, dim).cuda()\n",
    "    ffn = TransformerFFN(dim).cuda()\n",
    "\n",
    "    x = torch.randn(batch, seq, dim, device=\"cuda\")\n",
    "\n",
    "    sac_context = None\n",
    "    if CheckpointPolicy is not None and create_selective_checkpoint_contexts is not None:\n",
    "        aten = torch.ops.aten\n",
    "\n",
    "        def _maybe_default(op_name: str):\n",
    "            try:\n",
    "                return getattr(getattr(aten, op_name), \"default\")\n",
    "            except AttributeError:\n",
    "                return None\n",
    "\n",
    "        saved_ops = [op for op in [_maybe_default(\"mm\"), _maybe_default(\"addmm\")] if op is not None]\n",
    "\n",
    "        def sac_policy(ctx, op, *args, **kwargs):\n",
    "            # nn.Linear typically lowers to addmm/mm\n",
    "            if op in saved_ops:\n",
    "                return CheckpointPolicy.MUST_SAVE\n",
    "            return CheckpointPolicy.PREFER_RECOMPUTE\n",
    "\n",
    "        sac_context = functools.partial(create_selective_checkpoint_contexts, sac_policy)\n",
    "    else:\n",
    "        print(\"SAC APIs not available: will skip Selective AC benchmark\")\n",
    "\n",
    "    def bench(mode: str, iters: int = 5, warmup: int = 2):\n",
    "        def step():\n",
    "            h = stem(x)\n",
    "            if mode == \"none\":\n",
    "                out = ffn(h)\n",
    "            elif mode == \"standard\":\n",
    "                out = _run_checkpoint(ffn, h)\n",
    "            elif mode == \"selective\":\n",
    "                if sac_context is None:\n",
    "                    raise RuntimeError(\"Selective checkpointing context not available\")\n",
    "                out = _run_checkpoint(ffn, h, context_fn=sac_context)\n",
    "            else:\n",
    "                raise ValueError(mode)\n",
    "\n",
    "            out.sum().backward()\n",
    "            stem.zero_grad(set_to_none=True)\n",
    "            ffn.zero_grad(set_to_none=True)\n",
    "\n",
    "        for _ in range(warmup):\n",
    "            step()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        start = time.time()\n",
    "        for _ in range(iters):\n",
    "            step()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        time_ms = (time.time() - start) / iters * 1000\n",
    "        mem_mb = torch.cuda.max_memory_allocated() / 1e6\n",
    "        return mem_mb, time_ms\n",
    "\n",
    "    results = {\n",
    "        \"No Checkpoint\": bench(\"none\"),\n",
    "        \"Standard AC\": bench(\"standard\"),\n",
    "    }\n",
    "    if sac_context is not None:\n",
    "        results[\"Selective AC\"] = bench(\"selective\")\n",
    "\n",
    "    print(f\"Transformer FFN: dim={dim}, batch={batch}, seq={seq}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Mode':<15} {'Peak Memory (MB)':<18} {'Time (ms/iter)':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    for name, (mem, t) in results.items():\n",
    "        print(f\"{name:<15} {mem:<18.2f} {t:<15.2f}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(\"Standard AC: recomputes everything inside the checkpointed region\")\n",
    "    print(\"Selective AC (when available): can save matmuls and recompute only GELU\")\n",
    "else:\n",
    "    print(\"[Run on GPU for comparison]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---# 8. Distributed Training and Mixed PrecisionCheckpointing works seamlessly with DDP and mixed precision. No special configuration needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDP + Checkpointing:\n",
      "- Checkpointing happens locally on each GPU\n",
      "- DDP handles gradient synchronization\n",
      "- No special configuration needed\n",
      "- Memory savings apply per-GPU\n"
     ]
    }
   ],
   "source": [
    "# 8.1 DDP + Checkpointing (Code Template)\n",
    "# NOTE: This code shows the pattern. Run in a distributed environment.\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# Initialize distributed\n",
    "dist.init_process_group(backend=\"nccl\")\n",
    "local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "torch.cuda.set_device(local_rank)\n",
    "\n",
    "class ModelWithCheckpoint(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1024, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 1024)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = checkpoint(lambda t: torch.relu(self.layer1(t)), x, use_reentrant=False)\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# Wrap with DDP\n",
    "model = ModelWithCheckpoint().cuda(local_rank)\n",
    "ddp_model = DDP(model, device_ids=[local_rank])\n",
    "\n",
    "# Training works as normal\n",
    "x = torch.randn(16, 1024).cuda(local_rank)\n",
    "output = ddp_model(x)\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "\"\"\"\n",
    "\n",
    "print(\"DDP + Checkpointing:\")\n",
    "print(\"- Checkpointing happens locally on each GPU\")\n",
    "print(\"- DDP handles gradient synchronization\")\n",
    "print(\"- No special configuration needed\")\n",
    "print(\"- Memory savings apply per-GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 1532.7600\n",
      "Step 1: loss = 1574.5704\n",
      "Step 2: loss = 1518.4087\n",
      "\n",
      "Mixed precision + checkpointing works seamlessly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-237665819.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipython-input-237665819.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "# 8.2 Mixed Precision + Checkpointing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1024, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 1024)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = checkpoint(lambda t: torch.relu(self.layer1(t)), x, use_reentrant=False)\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = SimpleModel().cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    # Training loop with mixed precision + checkpointing\n",
    "    for step in range(3):\n",
    "        x = torch.randn(16, 1024).cuda()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(x)\n",
    "            loss = output.sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        print(f\"Step {step}: loss = {loss.item():.4f}\")\n",
    "    \n",
    "    print(\"\\nMixed precision + checkpointing works seamlessly\")\n",
    "else:\n",
    "    print(\"[Run on GPU]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---# 9. torch.compile and Memory Budget APItorch.compile (PyTorch 2.0+) traces your forward and backward passes into a single joint graph and applies a \"min-cut\" partitioner that automatically decides which tensors to save and which to recompute.By default, min-cut prioritizes speed, not memory. The Memory Budget API gives you control over this trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How torch.compile handles activations:\n",
      "--------------------------------------------------\n",
      "\n",
      "1. TRACING\n",
      "   torch.compile traces forward AND backward into one graph.\n",
      "   This lets it see the whole picture.\n",
      "\n",
      "2. MIN-CUT PARTITIONING\n",
      "   The graph is split at the optimal points.\n",
      "   Algorithm minimizes: tensors crossing the cut\n",
      "   (These are the tensors saved for backward)\n",
      "\n",
      "3. AUTOMATIC RECOMPUTATION\n",
      "   Cheap ops (relu, add, mul) are recomputed automatically.\n",
      "   No user intervention needed.\n",
      "\n",
      "4. FUSION\n",
      "   Pointwise ops get fused into kernels.\n",
      "   Fused ops are fast to recompute.\n",
      "\n",
      "Result: torch.compile gives you SOME memory savings for FREE,\n",
      "        plus speed improvements from fusion.\n"
     ]
    }
   ],
   "source": [
    "# 9.1 torch.compile: The Min-Cut Partitioner\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"How torch.compile handles activations:\")\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "print(\"1. TRACING\")\n",
    "print(\"   torch.compile traces forward AND backward into one graph.\")\n",
    "print(\"   This lets it see the whole picture.\")\n",
    "print()\n",
    "print(\"2. MIN-CUT PARTITIONING\")\n",
    "print(\"   The graph is split at the optimal points.\")\n",
    "print(\"   Algorithm minimizes: tensors crossing the cut\")\n",
    "print(\"   (These are the tensors saved for backward)\")\n",
    "print()\n",
    "print(\"3. AUTOMATIC RECOMPUTATION\")\n",
    "print(\"   Cheap ops (relu, add, mul) are recomputed automatically.\")\n",
    "print(\"   No user intervention needed.\")\n",
    "print()\n",
    "print(\"4. FUSION\")\n",
    "print(\"   Pointwise ops get fused into kernels.\")\n",
    "print(\"   Fused ops are fast to recompute.\")\n",
    "print()\n",
    "print(\"Result: torch.compile gives you SOME memory savings for FREE,\")\n",
    "print(\"        plus speed improvements from fusion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN: dim=2048, batch=64, seq=128\n",
      "--------------------------------------------------\n",
      "Eager               :  2718.83 MB\n",
      "torch.compile       :  2299.36 MB\n",
      "Checkpointing       :  2567.79 MB\n",
      "\n",
      "torch.compile: better speed, moderate memory savings\n",
      "Checkpointing: maximum memory savings, some speed cost\n"
     ]
    }
   ],
   "source": [
    "# 9.2 Comparing: Eager vs Compile vs Checkpointing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class SimpleFFN(nn.Module):\n",
    "    def __init__(self, dim=2048):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Wrapper(nn.Module):\n",
    "    \"\"\"Adds a small stem so the checkpointed region receives activations that require grad.\"\"\"\n",
    "    def __init__(self, dim=2048, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Linear(dim, dim)\n",
    "        self.ffn = SimpleFFN(dim)\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        if self.use_checkpoint:\n",
    "            x = checkpoint(self.ffn, x, use_reentrant=False)\n",
    "        else:\n",
    "            x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "def warmup(fn, zero_grad_fn, x, iters=2):\n",
    "    for _ in range(iters):\n",
    "        fn(x).sum().backward()\n",
    "        zero_grad_fn()\n",
    "\n",
    "def peak_memory_mb(fn, zero_grad_fn, x):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out = fn(x)\n",
    "    out.sum().backward()\n",
    "    peak = torch.cuda.max_memory_allocated() / 1e6\n",
    "    zero_grad_fn()\n",
    "    return peak\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dim = 2048\n",
    "    batch = 64\n",
    "    seq = 128\n",
    "\n",
    "    x = torch.randn(batch, seq, dim, device=\"cuda\")\n",
    "\n",
    "    eager_model = Wrapper(dim, use_checkpoint=False).cuda()\n",
    "    ckpt_model = Wrapper(dim, use_checkpoint=True).cuda()\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 1) Eager mode\n",
    "    warmup(eager_model, lambda: eager_model.zero_grad(set_to_none=True), x)\n",
    "    results['Eager'] = peak_memory_mb(eager_model, lambda: eager_model.zero_grad(set_to_none=True), x)\n",
    "\n",
    "    # 2) torch.compile (if available)\n",
    "    if hasattr(torch, \"compile\"):\n",
    "        compiled = torch.compile(eager_model)\n",
    "        warmup(compiled, lambda: eager_model.zero_grad(set_to_none=True), x)\n",
    "        results['torch.compile'] = peak_memory_mb(compiled, lambda: eager_model.zero_grad(set_to_none=True), x)\n",
    "    else:\n",
    "        print(\"torch.compile not available in this PyTorch build\")\n",
    "\n",
    "    # 3) Activation checkpointing\n",
    "    warmup(ckpt_model, lambda: ckpt_model.zero_grad(set_to_none=True), x)\n",
    "    results['Checkpointing'] = peak_memory_mb(ckpt_model, lambda: ckpt_model.zero_grad(set_to_none=True), x)\n",
    "\n",
    "    print(f\"FFN: dim={dim}, batch={batch}, seq={seq}\")\n",
    "    print(\"-\" * 50)\n",
    "    for name, mem in results.items():\n",
    "        print(f\"{name:20s}: {mem:8.2f} MB\")\n",
    "    print()\n",
    "    print(\"torch.compile: better speed, moderate memory savings\")\n",
    "    print(\"Checkpointing: maximum memory savings, some speed cost\")\n",
    "else:\n",
    "    print(\"[Run on GPU for comparison]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Gets Recomputed at Each Budget Level?Based on real transformer results:| Budget | Recomputes | Saves | Memory ||--------|------------|-------|--------|| 1.0 (default) | Nothing extra | Everything | 100% || 0.7 | Pointwise ops (gelu, add, mul) | Matmuls, attention | ~85% || 0.5 | Pointwise + some matmuls | Attention (most expensive) | ~50% || 0.3 | Pointwise + most matmuls | Only attention | ~35% || 0.0 | Everything (like full AC) | Almost nothing | Minimum |**Key insight:** 50% memory reduction by recomputing only pointwise ops. Attention is expensive. Recompute it last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---# 9b. External Libraries ReferenceBeyond PyTorch's built-in checkpointing, external libraries offer additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEEPSPEED ACTIVATION CHECKPOINTING\n",
      "==================================\n",
      "pip install deepspeed\n",
      "\n",
      "Key features:\n",
      "- Automatic checkpoint placement\n",
      "- CPU offloading for extreme memory savings (offload activations to RAM!)\n",
      "- Integrated with ZeRO optimizer stages\n",
      "- Best for: very large models (billions of parameters)\n",
      "\n",
      "Config in ds_config.json:\n",
      "{\n",
      "    \"activation_checkpointing\": {\n",
      "        \"partition_activations\": true,\n",
      "        \"contiguous_memory_optimization\": true,\n",
      "        \"cpu_checkpointing\": true\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "FAIRSCALE CHECKPOINT_WRAPPER\n",
      "============================\n",
      "pip install fairscale\n",
      "\n",
      "Key features:\n",
      "- Clean API: wrap modules directly with checkpoint_wrapper()\n",
      "- Works well with FSDP (Fully Sharded Data Parallel)\n",
      "- Simpler than DeepSpeed for many use cases\n",
      "- Best for: medium-scale training\n",
      "\n",
      "Usage:\n",
      "from fairscale.nn.checkpoint import checkpoint_wrapper\n",
      "\n",
      "layer = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU())\n",
      "checkpointed_layer = checkpoint_wrapper(layer)\n",
      "\n",
      "\n",
      "TOOL SELECTION GUIDE\n",
      "====================\n",
      "- PyTorch native: Best for most cases, well-maintained\n",
      "- DeepSpeed: Very large models, need CPU offloading\n",
      "- FairScale: Using FSDP, prefer simpler API\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# External Libraries: DeepSpeed and FairScale\n",
    "\n",
    "print(\"\"\"\n",
    "DEEPSPEED ACTIVATION CHECKPOINTING\n",
    "==================================\n",
    "pip install deepspeed\n",
    "\n",
    "Key features:\n",
    "- Automatic checkpoint placement\n",
    "- CPU offloading for extreme memory savings (offload activations to RAM!)\n",
    "- Integrated with ZeRO optimizer stages\n",
    "- Best for: very large models (billions of parameters)\n",
    "\n",
    "Config in ds_config.json:\n",
    "{\n",
    "    \"activation_checkpointing\": {\n",
    "        \"partition_activations\": true,\n",
    "        \"contiguous_memory_optimization\": true,\n",
    "        \"cpu_checkpointing\": true\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "FAIRSCALE CHECKPOINT_WRAPPER\n",
    "============================\n",
    "pip install fairscale\n",
    "\n",
    "Key features:\n",
    "- Clean API: wrap modules directly with checkpoint_wrapper()\n",
    "- Works well with FSDP (Fully Sharded Data Parallel)\n",
    "- Simpler than DeepSpeed for many use cases\n",
    "- Best for: medium-scale training\n",
    "\n",
    "Usage:\n",
    "from fairscale.nn.checkpoint import checkpoint_wrapper\n",
    "\n",
    "layer = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU())\n",
    "checkpointed_layer = checkpoint_wrapper(layer)\n",
    "\n",
    "\n",
    "TOOL SELECTION GUIDE\n",
    "====================\n",
    "- PyTorch native: Best for most cases, well-maintained\n",
    "- DeepSpeed: Very large models, need CPU offloading\n",
    "- FairScale: Using FSDP, prefer simpler API\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---# 10. Case Study: Activation Checkpointing in ResNet50ResNet50 is deep enough to benefit from checkpointing. It has 4 stages with multiple bottleneck blocks. Each block stores feature maps for backward.This section shows how to apply checkpointing to a real production model. You will see:1. How to modify torchvision's ResNet502. Memory comparison across checkpointing strategies3. A complete training loop with checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 ResNet50 Architecture Overview\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    import torchvision.models as models\n",
    "except Exception as e:\n",
    "    models = None\n",
    "    print(\"torchvision is required for the ResNet50 case study.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "if models is not None:\n",
    "    # Load ResNet50 without downloading weights\n",
    "    try:\n",
    "        resnet50 = models.resnet50(weights=None)\n",
    "    except TypeError:\n",
    "        # Older torchvision\n",
    "        resnet50 = models.resnet50(pretrained=False)\n",
    "\n",
    "    print(\"ResNet50 Structure:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"conv1:  1 conv layer\")\n",
    "    print(f\"layer1: {len(resnet50.layer1)} Bottleneck blocks (64 -> 256 channels)\")\n",
    "    print(f\"layer2: {len(resnet50.layer2)} Bottleneck blocks (128 -> 512 channels)\")\n",
    "    print(f\"layer3: {len(resnet50.layer3)} Bottleneck blocks (256 -> 1024 channels)\")\n",
    "    print(f\"layer4: {len(resnet50.layer4)} Bottleneck blocks (512 -> 2048 channels)\")\n",
    "    print(f\"fc:     1 linear layer\")\n",
    "    print()\n",
    "    print(\n",
    "        f\"Total Bottleneck blocks: {len(resnet50.layer1) + len(resnet50.layer2) + len(resnet50.layer3) + len(resnet50.layer4)}\"\n",
    "    )\n",
    "    print()\n",
    "    print(\"Each Bottleneck block has 3 conv layers + skip connection.\")\n",
    "    print(\"Feature maps grow larger in early layers, then shrink spatially.\")\n",
    "    print(\"layer3 often has the largest activation memory (1024 channels, moderate spatial size).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 ResNet50 with Checkpointing: Three Strategies\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "try:\n",
    "    import torchvision.models as models\n",
    "except Exception as e:\n",
    "    models = None\n",
    "    ResNet50Checkpointed = None\n",
    "    print(\"torchvision is required for ResNet50Checkpointed.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "if models is not None:\n",
    "    class ResNet50Checkpointed(nn.Module):\n",
    "        \"\"\"ResNet50 with configurable checkpointing strategies.\"\"\"\n",
    "\n",
    "        def __init__(self, num_classes=1000, checkpoint_strategy='none'):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                checkpoint_strategy: 'none', 'per_stage', 'per_block', or 'aggressive'\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "\n",
    "            # Load base ResNet50 without downloading weights\n",
    "            try:\n",
    "                base = models.resnet50(weights=None)\n",
    "            except TypeError:\n",
    "                base = models.resnet50(pretrained=False)\n",
    "\n",
    "            # Copy all layers\n",
    "            self.conv1 = base.conv1\n",
    "            self.bn1 = base.bn1\n",
    "            self.relu = base.relu\n",
    "            self.maxpool = base.maxpool\n",
    "            self.layer1 = base.layer1\n",
    "            self.layer2 = base.layer2\n",
    "            self.layer3 = base.layer3\n",
    "            self.layer4 = base.layer4\n",
    "            self.avgpool = base.avgpool\n",
    "            self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "            self.checkpoint_strategy = checkpoint_strategy\n",
    "\n",
    "        def _forward_stage(self, stage, x):\n",
    "            \"\"\"Forward through a stage (layer1, layer2, etc.).\"\"\"\n",
    "            for block in stage:\n",
    "                x = block(x)\n",
    "            return x\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Stem\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "            # NOTE: ResNet blocks include BatchNorm, which updates running stats in train mode.\n",
    "            # Checkpointing recomputes forward in backward, which can update BN stats twice.\n",
    "            # For strict parity, consider freezing BN stats (eval for BN) or using GroupNorm.\n",
    "\n",
    "            if self.checkpoint_strategy == 'none':\n",
    "                x = self.layer1(x)\n",
    "                x = self.layer2(x)\n",
    "                x = self.layer3(x)\n",
    "                x = self.layer4(x)\n",
    "\n",
    "            elif self.checkpoint_strategy == 'per_stage':\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer1, t), x, use_reentrant=False)\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer2, t), x, use_reentrant=False)\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer3, t), x, use_reentrant=False)\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer4, t), x, use_reentrant=False)\n",
    "\n",
    "            elif self.checkpoint_strategy == 'per_block':\n",
    "                for block in self.layer1:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer2:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer3:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer4:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "\n",
    "            elif self.checkpoint_strategy == 'aggressive':\n",
    "                x = self.layer1(x)\n",
    "                x = self.layer2(x)\n",
    "                for block in self.layer3:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer4:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown checkpoint_strategy: {self.checkpoint_strategy}\")\n",
    "\n",
    "            # Head\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "    print(\"ResNet50Checkpointed created with 4 strategies:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"'none':       No checkpointing (baseline)\")\n",
    "    print(\"'per_stage':  Checkpoint each of the 4 stages\")\n",
    "    print(\"'per_block':  Checkpoint each of the 16 bottleneck blocks\")\n",
    "    print(\"'aggressive': Only checkpoint layer3 and layer4 (best ROI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3 Memory Comparison: ResNet50 Strategies\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "def benchmark_resnet50(strategy, batch_size=32, image_size=224, num_iters=5):\n",
    "    \"\"\"Benchmark memory and time for a ResNet50 checkpointing strategy.\"\"\"\n",
    "    if ResNet50Checkpointed is None:\n",
    "        raise RuntimeError(\"ResNet50Checkpointed is not available (torchvision import likely failed).\")\n",
    "\n",
    "    model = ResNet50Checkpointed(num_classes=1000, checkpoint_strategy=strategy).cuda()\n",
    "    model.train()\n",
    "\n",
    "    x = torch.randn(batch_size, 3, image_size, image_size, device=\"cuda\")\n",
    "    target = torch.randint(0, 1000, (batch_size,), device=\"cuda\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(2):\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Measure memory + time\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(num_iters):\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed = (time.time() - start) / num_iters * 1000  # ms/iter\n",
    "    peak_mem = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
    "\n",
    "    del model, x, target\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return peak_mem, elapsed\n",
    "\n",
    "if torch.cuda.is_available() and ResNet50Checkpointed is not None:\n",
    "    batch_size = 32\n",
    "    image_size = 224\n",
    "\n",
    "    print(\"ResNet50 Checkpointing Comparison\")\n",
    "    print(f\"Batch size: {batch_size}, Image size: {image_size}x{image_size}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Strategy':<15} {'Peak Memory (MB)':<20} {'Time (ms)':<15} {'Mem Savings':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    strategies = ['none', 'aggressive', 'per_stage', 'per_block']\n",
    "    results = {}\n",
    "\n",
    "    for strategy in strategies:\n",
    "        mem, time_ms = benchmark_resnet50(strategy, batch_size, image_size)\n",
    "        results[strategy] = (mem, time_ms)\n",
    "\n",
    "    baseline_mem = results['none'][0]\n",
    "    for strategy in strategies:\n",
    "        mem, time_ms = results[strategy]\n",
    "        savings = (1 - mem / baseline_mem) * 100\n",
    "        print(f\"{strategy:<15} {mem:<20.2f} {time_ms:<15.2f} {savings:>6.1f}%\")\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "    print(\"\\nObservations:\")\n",
    "    print(\"- 'aggressive' gives best memory/speed balance (checkpoints only deep layers)\")\n",
    "    print(\"- 'per_block' gives maximum memory savings but highest overhead\")\n",
    "    print(\"- 'per_stage' is a middle ground\")\n",
    "else:\n",
    "    print(\"[Run on GPU and ensure torchvision is installed for ResNet50 benchmarks]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.4 Maximum Batch Size: ResNet50\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def find_max_batch_resnet50(strategy, start=16, max_batch=256):\n",
    "    \"\"\"Find maximum batch size before OOM for ResNet50 (CUDA only).\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is required for find_max_batch_resnet50().\")\n",
    "    if ResNet50Checkpointed is None:\n",
    "        raise RuntimeError(\"ResNet50Checkpointed is not available (torchvision import likely failed).\")\n",
    "\n",
    "    start = max(1, int(start))\n",
    "    max_batch = max(start, int(max_batch))\n",
    "\n",
    "    def can_run(batch_size):\n",
    "        model = None\n",
    "        x = None\n",
    "        target = None\n",
    "        out = None\n",
    "        loss = None\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            model = ResNet50Checkpointed(checkpoint_strategy=strategy).cuda()\n",
    "            model.train()\n",
    "            x = torch.randn(batch_size, 3, 224, 224, device=\"cuda\")\n",
    "            target = torch.randint(0, 1000, (batch_size,), device=\"cuda\")\n",
    "\n",
    "            out = model(x)\n",
    "            loss = nn.CrossEntropyLoss()(out, target)\n",
    "            loss.backward()\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                return False\n",
    "            raise\n",
    "        finally:\n",
    "            try:\n",
    "                if model is not None:\n",
    "                    model.zero_grad(set_to_none=True)\n",
    "            except Exception:\n",
    "                pass\n",
    "            del model, x, target, out, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Exponential search to find an upper bound, then binary search.\n",
    "    max_working = 0\n",
    "    batch = start\n",
    "    while batch <= max_batch and can_run(batch):\n",
    "        max_working = batch\n",
    "        batch *= 2\n",
    "\n",
    "    low = max_working + 1\n",
    "    high = min(batch, max_batch)\n",
    "\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        if can_run(mid):\n",
    "            max_working = mid\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "\n",
    "    return max_working\n",
    "\n",
    "if torch.cuda.is_available() and ResNet50Checkpointed is not None:\n",
    "    print(\"Finding maximum batch size for ResNet50 (224x224 images)\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    strategies = ['none', 'aggressive', 'per_block']\n",
    "    baseline = None\n",
    "\n",
    "    for strategy in strategies:\n",
    "        max_batch = find_max_batch_resnet50(strategy)\n",
    "        if baseline is None:\n",
    "            baseline = max_batch\n",
    "        improvement = (max_batch / baseline) if baseline else float('inf')\n",
    "        print(f\"{strategy:<15}: max batch = {max_batch:>3d} ({improvement:.2f}x)\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nWith checkpointing, you can often fit larger batches.\")\n",
    "else:\n",
    "    print(\"[Run on GPU and ensure torchvision is installed to find max batch sizes]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.5 Complete Training Loop: ResNet50 with Checkpointing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_resnet50_with_checkpointing(\n",
    "    checkpoint_strategy='aggressive',\n",
    "    batch_size=32,\n",
    "    num_epochs=3,\n",
    "    num_samples=256,  # Small for demo\n",
    "    use_amp=True,\n",
    "):\n",
    "    \"\"\"Complete training loop with checkpointing and mixed precision.\"\"\"\n",
    "\n",
    "    if ResNet50Checkpointed is None:\n",
    "        raise RuntimeError(\"ResNet50Checkpointed is not available (torchvision import likely failed).\")\n",
    "\n",
    "    # Create model\n",
    "    model = ResNet50Checkpointed(\n",
    "        num_classes=10,  # Simplified for demo\n",
    "        checkpoint_strategy=checkpoint_strategy,\n",
    "    ).cuda()\n",
    "\n",
    "    # Create synthetic dataset\n",
    "    X = torch.randn(num_samples, 3, 224, 224)\n",
    "    y = torch.randint(0, 10, (num_samples,))\n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler() if (use_amp and torch.cuda.is_available()) else None\n",
    "\n",
    "    print(f\"Training ResNet50 with '{checkpoint_strategy}' checkpointing\")\n",
    "    print(f\"Batch size: {batch_size}, Mixed precision: {use_amp and scaler is not None}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.cuda()\n",
    "            batch_y = batch_y.cuda()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            if scaler is not None:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(batch_x)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: loss = {avg_loss:.4f}, peak memory = {peak_mem:.2f} MB\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "if torch.cuda.is_available() and ResNet50Checkpointed is not None:\n",
    "    model = train_resnet50_with_checkpointing(\n",
    "        checkpoint_strategy='aggressive',\n",
    "        batch_size=32,\n",
    "        num_epochs=3,\n",
    "        use_amp=True,\n",
    "    )\n",
    "else:\n",
    "    print(\"[Run on GPU and ensure torchvision is installed for training demo]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.6 Alternative: Using torchvision's Built-in Support\n",
    "\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "try:\n",
    "    import torchvision.models as models\n",
    "except Exception as e:\n",
    "    models = None\n",
    "    print(\"torchvision is required for this alternative ResNet50 example.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "\n",
    "def _checkpoint_sequential(mod, segments, x):\n",
    "    \"\"\"checkpoint_sequential signature varies across PyTorch versions.\"\"\"\n",
    "    try:\n",
    "        return checkpoint_sequential(mod, segments, x, use_reentrant=False)\n",
    "    except TypeError:\n",
    "        return checkpoint_sequential(mod, segments, x)\n",
    "\n",
    "\n",
    "def resnet50_with_sequential_checkpoint(num_segments=4):\n",
    "    \"\"\"Monkey-patch torchvision ResNet50 to checkpoint its stage blocks.\"\"\"\n",
    "    if models is None:\n",
    "        raise RuntimeError(\"torchvision is not available\")\n",
    "\n",
    "    try:\n",
    "        model = models.resnet50(weights=None)\n",
    "    except TypeError:\n",
    "        model = models.resnet50(pretrained=False)\n",
    "\n",
    "    def checkpointed_forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Checkpoint each stage with user-controlled granularity.\n",
    "        x = _checkpoint_sequential(self.layer1, min(num_segments, len(self.layer1)), x)\n",
    "        x = _checkpoint_sequential(self.layer2, min(num_segments, len(self.layer2)), x)\n",
    "        x = _checkpoint_sequential(self.layer3, min(num_segments, len(self.layer3)), x)\n",
    "        x = _checkpoint_sequential(self.layer4, min(num_segments, len(self.layer4)), x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    import types\n",
    "\n",
    "    model.forward = types.MethodType(checkpointed_forward, model)\n",
    "    return model\n",
    "\n",
    "# Test it\n",
    "if models is not None:\n",
    "    model = resnet50_with_sequential_checkpoint(num_segments=4)\n",
    "    print(\"Alternative approach: Monkey-patch torchvision ResNet50\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Pros:\")\n",
    "    print(\"  - Uses official torchvision model\")\n",
    "    print(\"  - Simple implementation\")\n",
    "    print(\"  - Adjustable granularity via num_segments\")\n",
    "    print()\n",
    "    print(\"Cons:\")\n",
    "    print(\"  - Less control over which blocks to checkpoint\")\n",
    "    print(\"  - Harder to switch strategies dynamically\")\n",
    "    print()\n",
    "    print(\"Use the custom class (ResNet50Checkpointed) for flexible experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (10: ResNet50 Case Study)ResNet50 is a perfect testbed for checkpointing because:- 16 bottleneck blocks across 4 stages- Deep enough to benefit, not so deep it is impractical- Widely used in productionKey findings:**Strategy Selection:**- `aggressive` (checkpoint layer3+layer4): Best ROI. These layers have the most parameters and largest feature maps.- `per_block`: Maximum memory savings (~40-50%), but highest compute overhead (~30-40%).- `per_stage`: Middle ground. Good for quick wins.**Practical Impact:**- 1.5-2x larger batch sizes possible- Combined with mixed precision: up to 3x improvement**BatchNorm Caveat:**ResNet blocks use BatchNorm. Checkpointing recomputes forward in backward, which can update BN running stats twice in `train()` mode. If you need strict parity, consider freezing BN stats (set BN layers to eval) or using GroupNorm.**Production Recommendations:**1. Start with `aggressive` strategy2. Combine with mixed precision (AMP)3. Profile before and after4. If still OOM, move to `per_block`The ResNet50Checkpointed class is a solid starting point. Copy it into your codebase and adapt it to your training setup and normalization strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---# 11. ConclusionActivation checkpointing is a memory-compute trade-off. It trades extra forward passes for reduced memory usage. The trade-off is usually worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.1 Key Takeaways\n",
    "\n",
    "print(\"\"\"\n",
    "KEY TAKEAWAYS\n",
    "=============\n",
    "\n",
    "1. ACTIVATIONS DOMINATE MEMORY\n",
    "   - Not parameters, not gradients\n",
    "   - Scales with batch_size * depth * hidden_size\n",
    "   - Peak memory occurs at start of backward pass\n",
    "\n",
    "2. CHECKPOINTING TRADES COMPUTE FOR MEMORY\n",
    "   - Save some activations (checkpoints)\n",
    "   - Recompute others during backward pass\n",
    "   - Typical: 30-50% memory savings, ~50% compute overhead\n",
    "\n",
    "3. THE SQRT(N) RULE\n",
    "   - Optimal checkpointing: O(sqrt(n)) memory for n layers\n",
    "   - 100 layers -> ~10 checkpoints -> 10x memory reduction\n",
    "\n",
    "4. PRACTICAL PATTERNS\n",
    "   - checkpoint(): Single modules/functions\n",
    "   - checkpoint_sequential(): Sequential models\n",
    "   - Always use use_reentrant=False\n",
    "\n",
    "5. ADVANCED: SELECTIVE ACTIVATION CHECKPOINT (SAC)\n",
    "   - Fine-grained control: choose what to save vs recompute\n",
    "   - Policy functions: MUST_SAVE for expensive ops, PREFER_RECOMPUTE for cheap\n",
    "   - Sweet spot: save matmuls, recompute pointwise ops\n",
    "   - Use aten.<op>.default for correct op matching\n",
    "\n",
    "6. ADVANCED: MEMORY BUDGET API (torch.compile)\n",
    "   - One line: torch._dynamo.config.activation_memory_budget = 0.5\n",
    "   - Automatic pareto-optimal recomputation strategies\n",
    "   - Budget 0 = full AC, Budget 1 = default compile\n",
    "\n",
    "7. COMBINE WITH OTHER TECHNIQUES\n",
    "   - Mixed precision: 2x memory from fp16\n",
    "   - Gradient accumulation: Effective larger batches\n",
    "   - DDP: Scale across GPUs\n",
    "   - Together: Train 4-5x larger models\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.2 Decision Framework: When to Use Checkpointing\n",
    "\n",
    "print(\"\"\"\n",
    "WHEN TO USE ACTIVATION CHECKPOINTING\n",
    "====================================\n",
    "\n",
    "USE IT WHEN:\n",
    "- You are hitting OOM errors\n",
    "- You want larger batch sizes\n",
    "- Your model has 10+ layers\n",
    "- Training time is less critical than memory\n",
    "- You are training transformers or deep CNNs\n",
    "\n",
    "SKIP IT WHEN:\n",
    "- Model fits comfortably in memory\n",
    "- Training time is the bottleneck (not memory)\n",
    "- Model is shallow (< 5 layers)\n",
    "- You need maximum training speed\n",
    "\n",
    "QUICK DECISION TREE:\n",
    "\n",
    "  OOM Error?\n",
    "     |\n",
    "     v\n",
    "  YES --> Use checkpointing\n",
    "     |\n",
    "     v\n",
    "  Want larger batches?\n",
    "     |\n",
    "     v\n",
    "  YES --> Use checkpointing\n",
    "     |\n",
    "     v\n",
    "  Model > 10 layers?\n",
    "     |\n",
    "     v\n",
    "  YES --> Consider checkpointing\n",
    "     |\n",
    "     v\n",
    "  NO --> Probably skip it\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final ImpressionsActivation checkpointing is not magic. It is a simple trade-off executed well.You now understand:- Why activations dominate memory (batch x depth x hidden)- How checkpointing works (recompute instead of store)- When to use it (memory-bound, deep models)- Basic implementation (checkpoint, checkpoint_sequential, use_reentrant=False)- Advanced control with SAC (choose exactly what to save)- Automatic optimization with Memory Budget API (one config line)The landscape of techniques:- **Eager**: Maximum speed, maximum memory- **torch.compile**: Free speedups, some automatic memory savings- **Memory Budget API**: Tunable compile-time optimization (0 to 1)- **Selective AC**: Manual control over save/recompute decisions- **Standard AC**: Maximum memory savings, ~50% compute overheadStart simple. Add complexity only when needed. Measure everything.Every large language model uses some form of activation checkpointing. Now you know exactly how it works and when to use each variant.Go train something bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1n9yptvfc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: 10 FFN blocks (4x expansion), hidden=1024, batch=256\n",
      "Segments: 5 (2 blocks each)\n",
      "\n",
      "WITHOUT checkpointing: 2171.83 MB\n",
      "WITH checkpointing:    2002.85 MB\n",
      "Memory saved: 168.98 MB (7.8%)\n",
      "\n",
      "Theoretical activation memory: 41.9 MB (10 layers x 4.2 MB)\n",
      "Expected savings: ~21.0 MB (5 layers not stored)\n",
      "\n",
      "The 4x expansion in FFN blocks is why transformer checkpointing works so well.\n",
      "Simple Linear+ReLU layers have much smaller activations.\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Deep Model Comparison: The Real Difference\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "class FFNBlock(nn.Module):\n",
    "    \"\"\"FFN block with 4x expansion - the memory-hungry part of transformers.\n",
    "    \n",
    "    The 4x expanded intermediate tensor is what makes activations large.\n",
    "    This is why transformer FFN blocks benefit most from checkpointing.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "class DeepModelNoCheckpoint(nn.Module):\n",
    "    def __init__(self, num_layers=10, hidden=1024):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class DeepModelWithCheckpoint(nn.Module):\n",
    "    \"\"\"Uses checkpoint_sequential to checkpoint segments of layers.\n",
    "    \n",
    "    Key insight: checkpoint_sequential splits the model into `segments` chunks\n",
    "    and only saves the input to each chunk. During backward, each chunk is\n",
    "    recomputed from its saved input.\n",
    "    \n",
    "    Memory: O(segments) instead of O(num_layers)\n",
    "    Compute: ~1 extra forward pass (recomputing during backward)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers=10, hidden=1024, num_segments=5):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[FFNBlock(hidden) for _ in range(num_layers)])\n",
    "        self.num_segments = num_segments\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return checkpoint_sequential(\n",
    "            self.layers, self.num_segments, x, use_reentrant=False\n",
    "        )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    batch_size = 256\n",
    "    hidden = 1024\n",
    "    num_layers = 10\n",
    "    num_segments = 5  # 5 segments of 2 layers each\n",
    "    \n",
    "    # Without checkpointing\n",
    "    torch.cuda.empty_cache()\n",
    "    model1 = DeepModelNoCheckpoint(num_layers, hidden).cuda()\n",
    "    x1 = torch.randn(batch_size, hidden, device='cuda')\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out1 = model1(x1)\n",
    "    out1.sum().backward()\n",
    "    mem_no_ckpt = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    del model1, x1, out1\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # With segment checkpointing\n",
    "    model2 = DeepModelWithCheckpoint(num_layers, hidden, num_segments).cuda()\n",
    "    x2 = torch.randn(batch_size, hidden, device='cuda')\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out2 = model2(x2)\n",
    "    out2.sum().backward()\n",
    "    mem_ckpt = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    # Calculate theoretical activation memory\n",
    "    # Each FFNBlock saves the 4x expanded intermediate: batch * hidden * 4 * 4 bytes\n",
    "    act_per_layer = batch_size * hidden * 4 * 4 / 1e6  # MB\n",
    "    total_act = act_per_layer * num_layers\n",
    "    saved_act = act_per_layer * (num_layers - num_segments)\n",
    "    \n",
    "    print(f\"Config: {num_layers} FFN blocks (4x expansion), hidden={hidden}, batch={batch_size}\")\n",
    "    print(f\"Segments: {num_segments} ({num_layers // num_segments} blocks each)\")\n",
    "    print()\n",
    "    print(f\"WITHOUT checkpointing: {mem_no_ckpt:.2f} MB\")\n",
    "    print(f\"WITH checkpointing:    {mem_ckpt:.2f} MB\")\n",
    "    print(f\"Memory saved: {mem_no_ckpt - mem_ckpt:.2f} MB ({(1 - mem_ckpt/mem_no_ckpt)*100:.1f}%)\")\n",
    "    print()\n",
    "    print(f\"Theoretical activation memory: {total_act:.1f} MB ({num_layers} layers x {act_per_layer:.1f} MB)\")\n",
    "    print(f\"Expected savings: ~{saved_act:.1f} MB ({num_layers - num_segments} layers not stored)\")\n",
    "    print()\n",
    "    print(\"The 4x expansion in FFN blocks is why transformer checkpointing works so well.\")\n",
    "    print(\"Simple Linear+ReLU layers have much smaller activations.\")\n",
    "else:\n",
    "    print(\"[Run on GPU]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ksphwc3sk2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers=10, hidden=1024, segments=5:\n",
      "  No ckpt: 2676 MB, With ckpt: 2357 MB\n",
      "  Saved: 319 MB (11.9%)\n",
      "\n",
      "layers=10, hidden=1024, segments=2:\n",
      "  No ckpt: 2003 MB, With ckpt: 2003 MB\n",
      "  Saved: 0 MB (0.0%)\n",
      "\n",
      "layers=10, hidden=2048, segments=2:\n",
      "  No ckpt: 4023 MB, With ckpt: 4023 MB\n",
      "  Saved: 0 MB (0.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try with fewer segments (more aggressive checkpointing) and larger hidden\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "class FFNBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "class DeepModelNoCheckpoint(nn.Module):\n",
    "    def __init__(self, num_layers=10, hidden=1024):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class DeepModelWithCheckpoint(nn.Module):\n",
    "    def __init__(self, num_layers=10, hidden=1024, num_segments=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(*[FFNBlock(hidden) for _ in range(num_layers)])\n",
    "        self.num_segments = num_segments\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return checkpoint_sequential(\n",
    "            self.layers, self.num_segments, x, use_reentrant=False\n",
    "        )\n",
    "\n",
    "# Test with different configs\n",
    "configs = [\n",
    "    (10, 1024, 5),  # Current: 10 layers, hidden=1024, 5 segments\n",
    "    (10, 1024, 2),  # More aggressive: 2 segments\n",
    "    (10, 2048, 2),  # Larger hidden\n",
    "]\n",
    "\n",
    "for num_layers, hidden, num_segments in configs:\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    batch_size = 256\n",
    "    \n",
    "    # Without checkpointing\n",
    "    model1 = DeepModelNoCheckpoint(num_layers, hidden).cuda()\n",
    "    x1 = torch.randn(batch_size, hidden, device='cuda')\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out1 = model1(x1)\n",
    "    out1.sum().backward()\n",
    "    mem_no_ckpt = torch.cuda.max_memory_allocated() / 1e6\n",
    "    del model1, x1, out1\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # With checkpointing\n",
    "    model2 = DeepModelWithCheckpoint(num_layers, hidden, num_segments).cuda()\n",
    "    x2 = torch.randn(batch_size, hidden, device='cuda')\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out2 = model2(x2)\n",
    "    out2.sum().backward()\n",
    "    mem_ckpt = torch.cuda.max_memory_allocated() / 1e6\n",
    "    del model2, x2, out2\n",
    "    \n",
    "    saved = mem_no_ckpt - mem_ckpt\n",
    "    pct = (1 - mem_ckpt/mem_no_ckpt)*100\n",
    "    print(f\"layers={num_layers}, hidden={hidden}, segments={num_segments}:\")\n",
    "    print(f\"  No ckpt: {mem_no_ckpt:.0f} MB, With ckpt: {mem_ckpt:.0f} MB\")\n",
    "    print(f\"  Saved: {saved:.0f} MB ({pct:.1f}%)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bldnq8bw7yf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpointing:\n",
      "  After forward: 1689.1 MB\n",
      "  Peak memory:   1935.7 MB\n",
      "\n",
      "With checkpointing (2 segments):\n",
      "  After forward: 1710.1 MB\n",
      "  Peak memory:   2002.9 MB\n",
      "\n",
      "Savings after forward: -21.0 MB\n",
      "Savings peak:          -67.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check if checkpoint_sequential is working with 2 segments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "class FFNBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "batch_size = 256\n",
    "hidden = 1024\n",
    "num_layers = 10\n",
    "\n",
    "# Fresh start\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Without checkpointing - measure at different points\n",
    "layers_no_ckpt = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n",
    "x = torch.randn(batch_size, hidden, device='cuda')\n",
    "\n",
    "# Forward pass\n",
    "for layer in layers_no_ckpt:\n",
    "    x = layer(x)\n",
    "mem_after_forward = torch.cuda.memory_allocated() / 1e6\n",
    "\n",
    "# Backward\n",
    "x.sum().backward()\n",
    "mem_peak = torch.cuda.max_memory_allocated() / 1e6\n",
    "\n",
    "print(f\"No checkpointing:\")\n",
    "print(f\"  After forward: {mem_after_forward:.1f} MB\")\n",
    "print(f\"  Peak memory:   {mem_peak:.1f} MB\")\n",
    "\n",
    "del layers_no_ckpt, x\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# With checkpointing (2 segments)\n",
    "layers_ckpt = nn.Sequential(*[FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n",
    "x = torch.randn(batch_size, hidden, device='cuda')\n",
    "\n",
    "# Forward with checkpoint\n",
    "out = checkpoint_sequential(layers_ckpt, 2, x, use_reentrant=False)\n",
    "mem_after_forward_ckpt = torch.cuda.memory_allocated() / 1e6\n",
    "\n",
    "# Backward\n",
    "out.sum().backward()\n",
    "mem_peak_ckpt = torch.cuda.max_memory_allocated() / 1e6\n",
    "\n",
    "print(f\"\\nWith checkpointing (2 segments):\")\n",
    "print(f\"  After forward: {mem_after_forward_ckpt:.1f} MB\")\n",
    "print(f\"  Peak memory:   {mem_peak_ckpt:.1f} MB\")\n",
    "\n",
    "print(f\"\\nSavings after forward: {mem_after_forward - mem_after_forward_ckpt:.1f} MB\")\n",
    "print(f\"Savings peak:          {mem_peak - mem_peak_ckpt:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "epe0uyxxvrh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (no checkpoint): 2608 MB\n",
      "\n",
      "Segments   Layers/Seg   Peak MB      Saved MB     Savings % \n",
      "------------------------------------------------------------\n",
      "2          5.0          2674         -66          -2.5      \n",
      "3          3.3          2674         -66          -2.5      \n",
      "4          2.5          2674         -66          -2.5      \n",
      "5          2.0          2674         -66          -2.5      \n",
      "10         1.0          2674         -66          -2.5      \n",
      "\n",
      "Observation: More segments = more memory savings (to a point).\n"
     ]
    }
   ],
   "source": [
    "# Test with varying segment counts to find the sweet spot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "class FFNBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "batch_size = 256\n",
    "hidden = 1024\n",
    "num_layers = 10\n",
    "\n",
    "# Baseline - no checkpointing\n",
    "torch.cuda.empty_cache()\n",
    "model_base = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n",
    "x = torch.randn(batch_size, hidden, device='cuda')\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "for layer in model_base:\n",
    "    x = layer(x)\n",
    "x.sum().backward()\n",
    "baseline_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "del model_base, x\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Baseline (no checkpoint): {baseline_mem:.0f} MB\")\n",
    "print()\n",
    "print(f\"{'Segments':<10} {'Layers/Seg':<12} {'Peak MB':<12} {'Saved MB':<12} {'Savings %':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for num_segments in [2, 3, 4, 5, 10]:  # 10 segments = per-layer checkpointing\n",
    "    torch.cuda.empty_cache()\n",
    "    model = nn.Sequential(*[FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n",
    "    x = torch.randn(batch_size, hidden, device='cuda')\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out = checkpoint_sequential(model, num_segments, x, use_reentrant=False)\n",
    "    out.sum().backward()\n",
    "    mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    saved = baseline_mem - mem\n",
    "    pct = (saved / baseline_mem) * 100 if baseline_mem > 0 else 0\n",
    "    layers_per_seg = num_layers / num_segments\n",
    "    \n",
    "    print(f\"{num_segments:<10} {layers_per_seg:<12.1f} {mem:<12.0f} {saved:<12.0f} {pct:<10.1f}\")\n",
    "    del model, x, out\n",
    "    \n",
    "print()\n",
    "print(\"Observation: More segments = more memory savings (to a point).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "oxq8ych1vzd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: 10 FFN blocks, hidden=1024, batch=256\n",
      "\n",
      "No checkpointing: 2674 MB\n",
      "\n",
      "2 segments: 2674 MB (saved 0 MB, 0.0%)\n",
      "5 segments: 2674 MB (saved 0 MB, 0.0%)\n",
      "10 segments: 2674 MB (saved 0 MB, 0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Clean test - completely fresh state for each measurement\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "import gc\n",
    "\n",
    "class FFNBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "def measure_no_checkpoint(num_layers, hidden, batch_size):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    model = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n",
    "    x = torch.randn(batch_size, hidden, device='cuda')\n",
    "    \n",
    "    for layer in model:\n",
    "        x = layer(x)\n",
    "    x.sum().backward()\n",
    "    \n",
    "    peak = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    del model, x\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return peak\n",
    "\n",
    "def measure_with_checkpoint(num_layers, hidden, batch_size, num_segments):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    model = nn.Sequential(*[FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n",
    "    x = torch.randn(batch_size, hidden, device='cuda')\n",
    "    \n",
    "    out = checkpoint_sequential(model, num_segments, x, use_reentrant=False)\n",
    "    out.sum().backward()\n",
    "    \n",
    "    peak = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    del model, x, out\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return peak\n",
    "\n",
    "# Config\n",
    "num_layers = 10\n",
    "hidden = 1024\n",
    "batch_size = 256\n",
    "\n",
    "print(f\"Config: {num_layers} FFN blocks, hidden={hidden}, batch={batch_size}\")\n",
    "print()\n",
    "\n",
    "# Measure baseline\n",
    "baseline = measure_no_checkpoint(num_layers, hidden, batch_size)\n",
    "print(f\"No checkpointing: {baseline:.0f} MB\")\n",
    "print()\n",
    "\n",
    "# Measure with different segment counts\n",
    "for segs in [2, 5, 10]:\n",
    "    mem = measure_with_checkpoint(num_layers, hidden, batch_size, segs)\n",
    "    saved = baseline - mem\n",
    "    pct = (saved / baseline) * 100\n",
    "    print(f\"{segs} segments: {mem:.0f} MB (saved {saved:.0f} MB, {pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "x7tj8zaaxl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WITHOUT Checkpointing ===\n",
      "Before forward:     5236.6 MB\n",
      "After forward:      5336.4 MB\n",
      "Activation memory:  99.7 MB\n",
      "\n",
      "=== WITH Checkpointing ===\n",
      "Before forward:     5357.4 MB\n",
      "After forward:      5235.7 MB\n",
      "Activation memory:  -121.7 MB\n",
      "\n",
      "=== Comparison ===\n",
      "Activation savings: 221.4 MB\n",
      "Reduction:          222.0%\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check if checkpointing is actually working\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class FFNBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "num_layers = 12\n",
    "hidden = 2048\n",
    "batch_size = 128\n",
    "\n",
    "# Test 1: Without checkpointing - measure memory after forward\n",
    "torch.cuda.empty_cache()\n",
    "blocks1 = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n",
    "x1 = torch.randn(batch_size, hidden, device='cuda', requires_grad=True)\n",
    "\n",
    "mem_before = torch.cuda.memory_allocated() / 1e6\n",
    "\n",
    "# Forward without checkpoint\n",
    "out = x1\n",
    "for block in blocks1:\n",
    "    out = block(out)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "mem_after_fwd_no_ckpt = torch.cuda.memory_allocated() / 1e6\n",
    "activations_no_ckpt = mem_after_fwd_no_ckpt - mem_before\n",
    "\n",
    "print(\"=== WITHOUT Checkpointing ===\")\n",
    "print(f\"Before forward:     {mem_before:.1f} MB\")\n",
    "print(f\"After forward:      {mem_after_fwd_no_ckpt:.1f} MB\")\n",
    "print(f\"Activation memory:  {activations_no_ckpt:.1f} MB\")\n",
    "\n",
    "del blocks1, x1, out\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Test 2: With checkpointing - measure memory after forward\n",
    "blocks2 = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n",
    "x2 = torch.randn(batch_size, hidden, device='cuda', requires_grad=True)\n",
    "\n",
    "mem_before2 = torch.cuda.memory_allocated() / 1e6\n",
    "\n",
    "# Forward WITH checkpoint\n",
    "out2 = x2\n",
    "for block in blocks2:\n",
    "    out2 = checkpoint(block, out2, use_reentrant=False)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "mem_after_fwd_ckpt = torch.cuda.memory_allocated() / 1e6\n",
    "activations_ckpt = mem_after_fwd_ckpt - mem_before2\n",
    "\n",
    "print(\"\\n=== WITH Checkpointing ===\")\n",
    "print(f\"Before forward:     {mem_before2:.1f} MB\")\n",
    "print(f\"After forward:      {mem_after_fwd_ckpt:.1f} MB\")\n",
    "print(f\"Activation memory:  {activations_ckpt:.1f} MB\")\n",
    "\n",
    "print(f\"\\n=== Comparison ===\")\n",
    "print(f\"Activation savings: {activations_no_ckpt - activations_ckpt:.1f} MB\")\n",
    "print(f\"Reduction:          {(1 - activations_ckpt/activations_no_ckpt)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a4l9buyi08r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: 12 FFN blocks (4x expansion), hidden=2048, batch=128\n",
      "=================================================================\n",
      "\n",
      "Metric                              No Ckpt    With Ckpt      Saved\n",
      "-----------------------------------------------------------------\n",
      "Activations after forward:          113.2 MB    -1880.7 MB   1993.9 MB\n",
      "Peak during backward:              8465.2 MB     6840.5 MB   1624.7 MB\n",
      "\n",
      "KEY INSIGHT:\n",
      "- Checkpointing eliminates activation storage during forward pass\n",
      "- Peak memory during backward may be similar (recomputation recreates activations)\n",
      "- The WIN is being able to run larger batches that would otherwise OOM\n",
      "\n",
      "In practice: checkpointing lets you increase batch size until you hit\n",
      "the new (lower) memory ceiling, then backward peak becomes the limit.\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Deep Model Comparison: The Real Difference\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class FFNBlock(nn.Module):\n",
    "    \"\"\"FFN block with 4x expansion - the memory-hungry part of transformers.\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_layers = 12\n",
    "    hidden = 2048\n",
    "    batch_size = 128\n",
    "    \n",
    "    # ===== Test 1: Without checkpointing =====\n",
    "    torch.cuda.empty_cache()\n",
    "    blocks1 = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n",
    "    x1 = torch.randn(batch_size, hidden, device='cuda', requires_grad=True)\n",
    "    \n",
    "    mem_before1 = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    out = x1\n",
    "    for block in blocks1:\n",
    "        out = block(out)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    mem_after_fwd1 = torch.cuda.memory_allocated() / 1e6\n",
    "    activations1 = mem_after_fwd1 - mem_before1\n",
    "    \n",
    "    # Measure peak during backward\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out.sum().backward()\n",
    "    peak1 = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    del blocks1, x1, out\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # ===== Test 2: With checkpointing =====\n",
    "    blocks2 = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n",
    "    x2 = torch.randn(batch_size, hidden, device='cuda', requires_grad=True)\n",
    "    \n",
    "    mem_before2 = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    out2 = x2\n",
    "    for block in blocks2:\n",
    "        out2 = checkpoint(block, out2, use_reentrant=False)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    mem_after_fwd2 = torch.cuda.memory_allocated() / 1e6\n",
    "    activations2 = mem_after_fwd2 - mem_before2\n",
    "    \n",
    "    # Measure peak during backward\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out2.sum().backward()\n",
    "    peak2 = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    print(f\"Config: {num_layers} FFN blocks (4x expansion), hidden={hidden}, batch={batch_size}\")\n",
    "    print(\"=\" * 65)\n",
    "    print()\n",
    "    print(f\"{'Metric':<30} {'No Ckpt':>12} {'With Ckpt':>12} {'Saved':>10}\")\n",
    "    print(\"-\" * 65)\n",
    "    print(f\"{'Activations after forward:':<30} {activations1:>10.1f} MB {activations2:>10.1f} MB {activations1-activations2:>8.1f} MB\")\n",
    "    print(f\"{'Peak during backward:':<30} {peak1:>10.1f} MB {peak2:>10.1f} MB {peak1-peak2:>8.1f} MB\")\n",
    "    print()\n",
    "    print(\"KEY INSIGHT:\")\n",
    "    print(\"- Checkpointing eliminates activation storage during forward pass\")\n",
    "    print(\"- Peak memory during backward may be similar (recomputation recreates activations)\")\n",
    "    print(\"- The WIN is being able to run larger batches that would otherwise OOM\")\n",
    "    print()\n",
    "    print(\"In practice: checkpointing lets you increase batch size until you hit\")\n",
    "    print(\"the new (lower) memory ceiling, then backward peak becomes the limit.\")\n",
    "else:\n",
    "    print(\"[Run on GPU]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b7jfns0ux5n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: 12 FFN blocks (4x expansion), hidden=2048, batch=128\n",
      "============================================================\n",
      "\n",
      "Peak memory WITHOUT checkpointing: 10,066 MB\n",
      "Peak memory WITH checkpointing:    10,066 MB\n",
      "Memory saved:                      0 MB (0.0%)\n",
      "\n",
      "WHY THIS MATTERS:\n",
      "  - You could increase batch size by ~0% before hitting OOM\n",
      "  - Or train a model ~0% larger with the same GPU\n",
      "\n",
      "TRADE-OFF: ~50% more compute (one extra forward pass per block)\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Deep Model Comparison: The Real Difference\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class FFNBlock(nn.Module):\n",
    "    \"\"\"FFN block with 4x expansion - the memory-hungry part of transformers.\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "def measure_training_memory(blocks, x, use_checkpoint=False):\n",
    "    \"\"\"Measure peak memory during a full forward+backward pass.\"\"\"\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    out = x\n",
    "    for block in blocks:\n",
    "        if use_checkpoint:\n",
    "            out = checkpoint(block, out, use_reentrant=False)\n",
    "        else:\n",
    "            out = block(out)\n",
    "    \n",
    "    out.sum().backward()\n",
    "    \n",
    "    for block in blocks:\n",
    "        block.zero_grad(set_to_none=True)\n",
    "    \n",
    "    return torch.cuda.max_memory_allocated() / 1e6\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_layers = 12\n",
    "    hidden = 2048\n",
    "    batch_size = 128\n",
    "    \n",
    "    # Create model and input (shared for fair comparison)\n",
    "    torch.cuda.empty_cache()\n",
    "    blocks = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n",
    "    \n",
    "    # Warmup\n",
    "    x = torch.randn(batch_size, hidden, device='cuda', requires_grad=True)\n",
    "    _ = measure_training_memory(blocks, x, use_checkpoint=False)\n",
    "    \n",
    "    # Measure WITHOUT checkpointing\n",
    "    torch.cuda.empty_cache()\n",
    "    x = torch.randn(batch_size, hidden, device='cuda', requires_grad=True)\n",
    "    mem_no_ckpt = measure_training_memory(blocks, x, use_checkpoint=False)\n",
    "    \n",
    "    # Measure WITH checkpointing (same model)\n",
    "    torch.cuda.empty_cache()\n",
    "    x = torch.randn(batch_size, hidden, device='cuda', requires_grad=True)\n",
    "    mem_ckpt = measure_training_memory(blocks, x, use_checkpoint=True)\n",
    "    \n",
    "    saved = mem_no_ckpt - mem_ckpt\n",
    "    pct = (saved / mem_no_ckpt) * 100\n",
    "    \n",
    "    print(f\"Config: {num_layers} FFN blocks (4x expansion), hidden={hidden}, batch={batch_size}\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(f\"Peak memory WITHOUT checkpointing: {mem_no_ckpt:,.0f} MB\")\n",
    "    print(f\"Peak memory WITH checkpointing:    {mem_ckpt:,.0f} MB\")\n",
    "    print(f\"Memory saved:                      {saved:,.0f} MB ({pct:.1f}%)\")\n",
    "    print()\n",
    "    print(\"WHY THIS MATTERS:\")\n",
    "    print(f\"  - You could increase batch size by ~{pct:.0f}% before hitting OOM\")\n",
    "    print(f\"  - Or train a model ~{pct:.0f}% larger with the same GPU\")\n",
    "    print()\n",
    "    print(\"TRADE-OFF: ~50% more compute (one extra forward pass per block)\")\n",
    "else:\n",
    "    print(\"[Run on GPU]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "p47gx0ucan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: 12 FFN blocks (4x expansion), hidden=2048, batch=128\n",
      "============================================================\n",
      "\n",
      "Peak memory WITHOUT checkpointing: 11,679 MB\n",
      "Peak memory WITH checkpointing:    10,329 MB\n",
      "Memory saved:                      1,350 MB (11.6%)\n"
     ]
    }
   ],
   "source": [
    "# Fresh models for each test - avoid CUDA memory pool interference\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import gc\n",
    "\n",
    "class FFNBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "num_layers = 12\n",
    "hidden = 2048\n",
    "batch_size = 128\n",
    "\n",
    "# ===== Test 1: WITHOUT checkpointing (fresh model) =====\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "model1 = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n",
    "x1 = torch.randn(batch_size, hidden, device='cuda', requires_grad=True)\n",
    "\n",
    "out = x1\n",
    "for block in model1:\n",
    "    out = block(out)\n",
    "out.sum().backward()\n",
    "\n",
    "mem_no_ckpt = torch.cuda.max_memory_allocated() / 1e6\n",
    "\n",
    "# Full cleanup\n",
    "del model1, x1, out\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# ===== Test 2: WITH checkpointing (fresh model) =====\n",
    "model2 = nn.ModuleList([FFNBlock(hidden) for _ in range(num_layers)]).cuda()\n",
    "x2 = torch.randn(batch_size, hidden, device='cuda', requires_grad=True)\n",
    "\n",
    "out2 = x2\n",
    "for block in model2:\n",
    "    out2 = checkpoint(block, out2, use_reentrant=False)\n",
    "out2.sum().backward()\n",
    "\n",
    "mem_ckpt = torch.cuda.max_memory_allocated() / 1e6\n",
    "\n",
    "saved = mem_no_ckpt - mem_ckpt\n",
    "pct = (saved / mem_no_ckpt) * 100 if mem_no_ckpt > 0 else 0\n",
    "\n",
    "print(f\"Config: {num_layers} FFN blocks (4x expansion), hidden={hidden}, batch={batch_size}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"Peak memory WITHOUT checkpointing: {mem_no_ckpt:,.0f} MB\")\n",
    "print(f\"Peak memory WITH checkpointing:    {mem_ckpt:,.0f} MB\")\n",
    "print(f\"Memory saved:                      {saved:,.0f} MB ({pct:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}