{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22025dfd",
   "metadata": {
    "id": "22025dfd"
   },
   "source": [
    "# Homework: Activation Checkpointing in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9479e0e",
   "metadata": {
    "id": "f9479e0e"
   },
   "source": [
    "In this notebook, you will learn how to use **activation checkpointing** in PyTorch to reduce GPU memory usage when training deep models such as language models.\n",
    "\n",
    "We will go step by step, comparing normal training vs checkpointed training, and measuring both **memory usage** and **speed trade-offs**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232873db",
   "metadata": {
    "id": "232873db"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1719942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.0+cu126\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "# Environment Check\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61ac646",
   "metadata": {
    "id": "f61ac646",
    "outputId": "2bfa58cf-0ec8-4d7e-cac2-606ceccc02a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.half\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using dtype: {dtype}\")\n",
    "\n",
    "torch.backends.cuda.enable_flash_sdp(True) # ðŸ’¡ this alone won't enable flash attention :) check the Bonus section!\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd86332",
   "metadata": {
    "id": "0bd86332"
   },
   "source": [
    "## Step 1: Define a Toy Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea56f033",
   "metadata": {
    "id": "ea56f033"
   },
   "outputs": [],
   "source": [
    "class SimpleTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=512, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, 4*d_model)\n",
    "        self.linear2 = nn.Linear(4*d_model, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def attn_path(self, x):\n",
    "        h = self.norm1(x)\n",
    "        return x + self.attn(h,h,h, need_weights=True)[0]\n",
    "\n",
    "    def mlp_path(self, x):\n",
    "        h = self.norm2(x)\n",
    "        return x + self.linear2(F.relu(self.linear1(h)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn_path(x)\n",
    "        x = self.mlp_path(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb0c255",
   "metadata": {
    "id": "dfb0c255"
   },
   "source": [
    "## Step 2: Stack Many Blocks (Without Checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a6ad4e",
   "metadata": {
    "id": "86a6ad4e"
   },
   "outputs": [],
   "source": [
    "class DeepModel(nn.Module):\n",
    "    def __init__(self, depth=12):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([SimpleTransformerBlock().to(dtype=dtype, device='cuda') for _ in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb0344b",
   "metadata": {
    "id": "2cb0344b"
   },
   "source": [
    "## Step 3: Add Activation Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1e020",
   "metadata": {
    "id": "abd1e020"
   },
   "source": [
    "Complete the forward pass so to enable activation checkpointing:\n",
    "\n",
    "- Attention sublayer is checkpointed when ckpt_attn=True.\n",
    "\n",
    "- MLP sublayer is checkpointed when ckpt_mlp=True.\n",
    "\n",
    "ðŸ’¡ The first block runs normally (no checkpoint) to avoid cold-start edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5326d164",
   "metadata": {
    "id": "5326d164"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CheckpointedModel(nn.Module):\n",
    "    def __init__(self, depth=12, ckpt_attn=True, ckpt_mlp=True):\n",
    "        super().__init__()\n",
    "        self.ckpt_attn = ckpt_attn\n",
    "        self.ckpt_mlp = ckpt_mlp\n",
    "        self.blocks = nn.ModuleList([SimpleTransformerBlock().to(dtype=dtype, device='cuda')  for _ in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            if i == 0:\n",
    "                # First block runs normally (no checkpoint), as I read it may have cold-start edge cases\n",
    "                # Probably an ugly if condition, should rethink\n",
    "                x = block(x)\n",
    "            else:\n",
    "                # Apply checkpointing based on flags\n",
    "                if self.ckpt_attn:\n",
    "                    x = checkpoint(block.attn_path, x, use_reentrant=False)\n",
    "                else:\n",
    "                    x = block.attn_path(x)\n",
    "                \n",
    "                if self.ckpt_mlp:\n",
    "                    x = checkpoint(block.mlp_path, x, use_reentrant=False)\n",
    "                else:\n",
    "                    x = block.mlp_path(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b8c8a",
   "metadata": {
    "id": "7a1b8c8a"
   },
   "source": [
    "## Step 4: Compare Memory Usage and Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806171b",
   "metadata": {
    "id": "7806171b"
   },
   "source": [
    "For the 4 configs below, check the elapsed time and peak memory usage.\n",
    "\n",
    "Why does checkpointing the attention sublayer lead to large memory savings?\n",
    "\n",
    "Why does checkpointing the MLP sublayer usually give much smaller savings compared to attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c5e0985",
   "metadata": {
    "id": "4c5e0985",
    "outputId": "ec56a82c-346f-4070-b0a5-1c1658d7aab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without checkpointing: 0.75s, 3219 MB\n",
      "With checkpointing:    1.01s, 1497 MB\n",
      "CKPT mlp:              0.85s, 3176 MB\n",
      "CKPT attn:             0.99s, 1878 MB\n"
     ]
    }
   ],
   "source": [
    "def measure_run(model, x, steps=10):\n",
    "    model.to(device)\n",
    "    x = x.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters())\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(steps):\n",
    "        opt.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = out.mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    end = time.time()\n",
    "\n",
    "    max_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "    return (end-start)/steps, max_mem\n",
    "\n",
    "\n",
    "x = torch.randn(1, 4096, 512, dtype=dtype, device=\"cuda\")  # batch, seq, hidden - adjust based on your GPU memory\n",
    "\n",
    "m1 = CheckpointedModel(depth=12, ckpt_attn=False, ckpt_mlp=False)\n",
    "m2 = CheckpointedModel(depth=12, ckpt_attn=True, ckpt_mlp=True)\n",
    "m3 = CheckpointedModel(depth=12, ckpt_attn=False, ckpt_mlp=True)\n",
    "m4 = CheckpointedModel(depth=12, ckpt_attn=True, ckpt_mlp=False)\n",
    "t1, m1_mem = measure_run(m1, x)\n",
    "t2, m2_mem = measure_run(m2, x)\n",
    "t3, m3_mem = measure_run(m3, x)\n",
    "t4, m4_mem = measure_run(m4, x)\n",
    "\n",
    "\n",
    "print(f\"Without checkpointing: {t1:.2f}s, {m1_mem:.0f} MB\")\n",
    "print(f\"With checkpointing:    {t2:.2f}s, {m2_mem:.0f} MB\")\n",
    "print(f\"CKPT mlp:              {t3:.2f}s, {m3_mem:.0f} MB\")\n",
    "print(f\"CKPT attn:             {t4:.2f}s, {m4_mem:.0f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b35b3f",
   "metadata": {
    "id": "c1b35b3f"
   },
   "source": [
    "# Bonus\n",
    "Modify the code to only checkpoint **every other block** instead of all blocks.  \n",
    "   - What trade-off do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e5dae",
   "metadata": {
    "id": "609e5dae"
   },
   "source": [
    "# Bonus\n",
    "The code above currently does not use FlashAttention, which means attention has quadratic memory complexity with respect to sequence length (O(nÂ²)).\n",
    "\n",
    "ðŸ‘‰ However, PyTorch lets you enable FlashAttention by simply toggling a single flag in your code.\n",
    "\n",
    "ðŸ’¡ Challenge: Can you find and change that flag so that your model runs with FlashAttention instead of the standard (quadratic) path?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
