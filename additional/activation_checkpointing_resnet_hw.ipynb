{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Activation Checkpointing: A First Principles Deep Dive\n",
    "\n",
    "Memory is the bottleneck in deep learning. Not compute. Not data. Memory.\n",
    "\n",
    "This notebook tears apart activation checkpointing from first principles. You will understand exactly why activations dominate memory, how checkpointing trades compute for memory, and when this trade-off makes sense.\n",
    "\n",
    "---\n",
    "\n",
    "## The Core Problem\n",
    "\n",
    "During training you do two things:\n",
    "1. **Forward pass**: compute outputs layer by layer\n",
    "2. **Backward pass**: compute gradients using the chain rule\n",
    "\n",
    "Here's what kills you: backward needs intermediate values from forward. If forward does `y = sin(x)`, backward needs `x` to compute `dy/dx = cos(x)`. PyTorch keeps `x` alive until backward reaches that node. These are \"saved tensors\" in autograd-speak.\n",
    "\n",
    "So activation memory accumulates through forward and peaks at the start of backward. You've got this pile of \"will need later\" tensors sitting in VRAM, waiting.\n",
    "\n",
    "## What Checkpointing Actually Does\n",
    "\n",
    "Think of training like hiking a trail:\n",
    "\n",
    "**Normal training**: You drop breadcrumbs at every step (store all activations). Easy to retrace (backprop), but you're carrying a giant breadcrumb bag (VRAM).\n",
    "\n",
    "**Checkpointing**: You only drop breadcrumbs at a few checkpoints. On the way back, when you need details between checkpoints, you re-walk that segment.\n",
    "\n",
    "That's literally it. Save fewer tensors in forward, recompute them on demand during backward. Memory goes down, compute goes up.\n",
    "\n",
    "## Concrete Example\n",
    "\n",
    "Suppose your forward is:\n",
    "\n",
    "```\n",
    "x --> [f1] --> a --> [f2] --> b --> [f3] --> y\n",
    "```\n",
    "\n",
    "**No checkpointing**: autograd saves `a` and `b` so backward can compute gradients.\n",
    "\n",
    "**Checkpoint after `a`**: you keep `a`, but you don't keep `b`. During backward, when you need `b` to differentiate `f3`, PyTorch reruns `f2(a)` to get it back.\n",
    "\n",
    "Memory goes down (you didn't store `b`). Compute goes up (you recomputed `b`).\n",
    "\n",
    "---\n",
    "\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.0+cu126\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "GPU: Tesla T4\n",
      "GPU Memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "# 0.1 Environment Check\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. The Memory Problem\n",
    "\n",
    "Training deep neural networks hits a wall. That wall is GPU memory.\n",
    "\n",
    "Four things consume GPU memory during training:\n",
    "1. **Model parameters** (weights and biases)\n",
    "2. **Gradients** (same size as parameters)\n",
    "3. **Optimizer states** (for Adam: 2x parameter size for m and v buffers)\n",
    "4. **Activations** (intermediate outputs from each layer)\n",
    "\n",
    "For Adam/AdamW, the first three combined are 4x the model size (params + grads + 2 momentum buffers). This is fixed cost.\n",
    "\n",
    "Here is the surprise: **activations often dominate anyway**. Not parameters. Not gradients. Not optimizer states. Activations. Why? Because activations scale with batch size. Everything else does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PART 1: THEORETICAL MEMORY BREAKDOWN\n",
      "============================================================\n",
      "Model: 10,496,000 parameters\n",
      "\n",
      "Fixed costs for training with Adam:\n",
      "  Parameter memory:  40.04 MB\n",
      "  Gradient memory:   40.04 MB\n",
      "  Optimizer memory:  80.08 MB (m + v)\n",
      "  Total fixed:       160.16 MB\n",
      "\n",
      "Activation memory by batch size (THEORETICAL):\n",
      "  Batch  32: activations =   1.25 MB (0.8% of total)\n",
      "  Batch 128: activations =   5.00 MB (3.0% of total)\n",
      "  Batch 512: activations =  20.00 MB (11.1% of total)\n",
      "\n",
      "============================================================\n",
      "PART 2: REAL GPU MEASUREMENT\n",
      "============================================================\n",
      "GPU: Tesla T4\n",
      "\n",
      "Batch  64: Peak = 4435.3 MB  (est. activations: 6.3 MB)\n",
      "Batch 256: Peak = 4442.4 MB  (est. activations: 25.2 MB)\n",
      "Batch 512: Peak = 4451.8 MB  (est. activations: 50.3 MB)\n",
      "\n",
      "============================================================\n",
      "PART 3: MEMORY TIMELINE (When Does Memory Peak?)\n",
      "============================================================\n",
      "Model: 134.3M params\n",
      "\n",
      "Memory Timeline:\n",
      "-------------------------------------------------------\n",
      "Model loaded:                         4269.3 MB\n",
      "\n",
      "During forward (activations accumulating):\n",
      "  After layer 1:                          4273.5 MB  (+4.2)\n",
      "  After layer 2:                          4277.7 MB  (+4.2)\n",
      "  After layer 3:                          4281.9 MB  (+4.2)\n",
      "  After layer 4:                          4286.0 MB  (+4.2)\n",
      "  After layer 5:                          4290.2 MB  (+4.2)\n",
      "  After layer 6:                          4294.4 MB  (+4.2)\n",
      "  After layer 7:                          4298.6 MB  (+4.2)\n",
      "  After layer 8:                          4302.8 MB  (+4.2)\n",
      "\n",
      "After forward (all activations live):   4302.8 MB\n",
      "PEAK (activations + gradients):       4814.7 MB  <-- this kills you\n",
      "After backward:                       4810.5 MB\n",
      "After zero_grad:                      4273.5 MB\n",
      "\n",
      "KEY INSIGHT: Activation memory ~33.6 MB dominates during forward pass\n",
      "Memory peaks at start of backward when both activations and gradients exist\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Memory Breakdown: Theory, Measurement, and Timing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ===== PART 1: Theoretical Breakdown =====\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def bytes_to_mb(b):\n",
    "    return b / (1024 * 1024)\n",
    "\n",
    "class DeepNetwork(nn.Module):\n",
    "    def __init__(self, input_size=1024, hidden_size=1024, num_layers=10):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_f = input_size if i == 0 else hidden_size\n",
    "            layers.append(nn.Linear(in_f, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = DeepNetwork(input_size=1024, hidden_size=1024, num_layers=10)\n",
    "num_params = count_parameters(model)\n",
    "\n",
    "# Theoretical memory breakdown for a full training setup with Adam\n",
    "param_mem = num_params * 4  # fp32 parameters\n",
    "grad_mem = param_mem        # gradients\n",
    "optimizer_mem = param_mem * 2  # Adam's m and v buffers\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 1: THEORETICAL MEMORY BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {num_params:,} parameters\")\n",
    "print()\n",
    "print(\"Fixed costs for training with Adam:\")\n",
    "print(f\"  Parameter memory:  {bytes_to_mb(param_mem):.2f} MB\")\n",
    "print(f\"  Gradient memory:   {bytes_to_mb(grad_mem):.2f} MB\")\n",
    "print(f\"  Optimizer memory:  {bytes_to_mb(optimizer_mem):.2f} MB (m + v)\")\n",
    "print(f\"  Total fixed:       {bytes_to_mb(param_mem + grad_mem + optimizer_mem):.2f} MB\")\n",
    "print()\n",
    "print(\"Activation memory by batch size (THEORETICAL):\")\n",
    "fixed_cost = param_mem + grad_mem + optimizer_mem\n",
    "for batch_size in [32, 128, 512]:\n",
    "    act_mem = batch_size * 1024 * 10 * 4\n",
    "    total = fixed_cost + act_mem\n",
    "    act_pct = (act_mem / total) * 100\n",
    "    print(f\"  Batch {batch_size:3d}: activations = {bytes_to_mb(act_mem):6.2f} MB ({act_pct:.1f}% of total)\")\n",
    "\n",
    "# ===== PART 2: Real GPU Measurement =====\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 2: REAL GPU MEASUREMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Wider network to make activations dominate\n",
    "    class WideNetwork(nn.Module):\n",
    "        def __init__(self, hidden_size=4096, num_layers=6):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            for i in range(num_layers):\n",
    "                layers.append(nn.Linear(1024 if i == 0 else hidden_size, hidden_size))\n",
    "                layers.append(nn.ReLU())\n",
    "            self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "\n",
    "    model = WideNetwork().cuda()\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print()\n",
    "    \n",
    "    for batch_size in [64, 256, 512]:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        x = torch.randn(batch_size, 1024).cuda()\n",
    "        output = model(x)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        peak = torch.cuda.max_memory_allocated() / 1e6\n",
    "        act_estimate = batch_size * 4096 * 6 * 4 / 1e6\n",
    "        print(f\"Batch {batch_size:3d}: Peak = {peak:.1f} MB  (est. activations: {act_estimate:.1f} MB)\")\n",
    "        \n",
    "        model.zero_grad()\n",
    "        del x, output, loss\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"[Run on GPU to see measurements]\")\n",
    "\n",
    "# ===== PART 3: Memory Timeline =====\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"PART 3: MEMORY TIMELINE (When Does Memory Peak?)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class InstrumentedNetwork(nn.Module):\n",
    "    def __init__(self, num_layers=5, hidden=4096):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(hidden, hidden) for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, track=False):\n",
    "        memory_log = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = torch.relu(layer(x))\n",
    "            if track and torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "                memory_log.append(torch.cuda.memory_allocated() / 1e6)\n",
    "        return x, memory_log\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = InstrumentedNetwork(num_layers=8, hidden=4096).cuda()\n",
    "    x = torch.randn(256, 4096).cuda()\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model: {num_params/1e6:.1f}M params\")\n",
    "    print()\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.empty_cache()\n",
    "    mem_model_only = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    output, fwd_mem = model(x, track=True)\n",
    "    mem_after_forward = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    peak_memory = torch.cuda.max_memory_allocated() / 1e6\n",
    "    mem_after_backward = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    model.zero_grad(set_to_none=True)\n",
    "    mem_after_zero_grad = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    print(\"Memory Timeline:\")\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"{'Model loaded:':<35} {mem_model_only:>8.1f} MB\")\n",
    "    print()\n",
    "    print(\"During forward (activations accumulating):\")\n",
    "    for i, mem in enumerate(fwd_mem):\n",
    "        delta = mem - (fwd_mem[i-1] if i > 0 else mem_model_only)\n",
    "        print(f\"  After layer {i+1}:{'':<23} {mem:>8.1f} MB  (+{delta:.1f})\")\n",
    "    print()\n",
    "    print(f\"{'After forward (all activations live):':<35} {mem_after_forward:>8.1f} MB\")\n",
    "    print(f\"{'PEAK (activations + gradients):':<35} {peak_memory:>8.1f} MB  <-- this kills you\")\n",
    "    print(f\"{'After backward:':<35} {mem_after_backward:>8.1f} MB\")\n",
    "    print(f\"{'After zero_grad:':<35} {mem_after_zero_grad:>8.1f} MB\")\n",
    "    print()\n",
    "    \n",
    "    activation_mem = mem_after_forward - mem_model_only\n",
    "    print(f\"KEY INSIGHT: Activation memory ~{activation_mem:.1f} MB dominates during forward pass\")\n",
    "    print(\"Memory peaks at start of backward when both activations and gradients exist\")\n",
    "else:\n",
    "    print(\"[Run on GPU to see memory timeline]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Understanding Activation Checkpointing\n",
    "\n",
    "Activation checkpointing is a memory-compute trade-off. Save memory by not storing activations. Pay for it by recomputing them during backprop.\n",
    "\n",
    "To understand this, you need to know what activations are and why backprop needs them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored for backprop:\n",
      "  z1: torch.Size([1, 4]) - needed to compute relu gradient\n",
      "  a1: torch.Size([1, 4]) - needed to compute W2 gradient\n",
      "\n",
      "Gradients computed:\n",
      "  W1.grad: torch.Size([4, 4])\n",
      "  W2.grad: torch.Size([4, 4])\n",
      "\n",
      "The chain rule requires intermediate values.\n",
      "PyTorch keeps them alive until backward finishes.\n",
      "This is why activations accumulate through forward pass.\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Why Backprop Needs Activations\n",
    "\n",
    "import torch\n",
    "\n",
    "# y = W2 * relu(W1 * x)\n",
    "# Gradient w.r.t. W1 needs the pre-activation W1*x to compute relu'(W1*x)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "W1 = torch.randn(4, 4, requires_grad=True)\n",
    "W2 = torch.randn(4, 4, requires_grad=True)\n",
    "x = torch.randn(1, 4)\n",
    "\n",
    "# Forward pass stores these\n",
    "z1 = x @ W1.T           # Pre-activation (needed for ReLU grad)\n",
    "a1 = torch.relu(z1)     # Activation (needed for W2 grad)\n",
    "y = a1 @ W2.T\n",
    "\n",
    "print(\"Stored for backprop:\")\n",
    "print(f\"  z1: {z1.shape} - needed to compute relu gradient\")\n",
    "print(f\"  a1: {a1.shape} - needed to compute W2 gradient\")\n",
    "\n",
    "loss = y.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradients computed:\")\n",
    "print(f\"  W1.grad: {W1.grad.shape}\")\n",
    "print(f\"  W2.grad: {W2.grad.shape}\")\n",
    "print()\n",
    "print(\"The chain rule requires intermediate values.\")\n",
    "print(\"PyTorch keeps them alive until backward finishes.\")\n",
    "print(\"This is why activations accumulate through forward pass.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Training:\n",
      "  Forward: x -> a1 -> a2 -> a3 -> loss\n",
      "           [store] [store] [store]\n",
      "  Backward: uses stored a1, a2, a3\n",
      "  Memory: O(n) activations\n",
      "\n",
      "With Checkpointing:\n",
      "  Forward: x -> a1 -> a2 -> a3 -> loss\n",
      "           [save]       [save]\n",
      "  Backward: recompute a2 from a1, then use\n",
      "  Memory: O(sqrt(n)) with optimal placement\n",
      "\n",
      "For 100 layers:\n",
      "  Standard: 100 activations stored\n",
      "  Checkpointed: ~10 activations stored (at checkpoint boundaries)\n",
      "  Memory reduction: 10x\n",
      "\n",
      "Compute analysis:\n",
      "  Without checkpointing: n forward + n backward = 2n\n",
      "  With checkpointing:    n forward + n backward + n recompute = 3n\n",
      "  Compute overhead: ~50% of total training time\n",
      "\n",
      "KEY: The sqrt(n) rule - optimal checkpointing reduces memory from O(n) to O(sqrt(n))\n"
     ]
    }
   ],
   "source": [
    "# 2.2 The Core Trade-off: Store vs Recompute\n",
    "\n",
    "import math\n",
    "\n",
    "print(\"Standard Training:\")\n",
    "print(\"  Forward: x -> a1 -> a2 -> a3 -> loss\")\n",
    "print(\"           [store] [store] [store]\")\n",
    "print(\"  Backward: uses stored a1, a2, a3\")\n",
    "print(\"  Memory: O(n) activations\")\n",
    "print()\n",
    "print(\"With Checkpointing:\")\n",
    "print(\"  Forward: x -> a1 -> a2 -> a3 -> loss\")\n",
    "print(\"           [save]       [save]\")\n",
    "print(\"  Backward: recompute a2 from a1, then use\")\n",
    "print(\"  Memory: O(sqrt(n)) with optimal placement\")\n",
    "print()\n",
    "\n",
    "num_layers = 100\n",
    "optimal = int(math.sqrt(num_layers))\n",
    "\n",
    "print(f\"For {num_layers} layers:\")\n",
    "print(f\"  Standard: {num_layers} activations stored\")\n",
    "print(f\"  Checkpointed: ~{optimal} activations stored (at checkpoint boundaries)\")\n",
    "print(f\"  Memory reduction: {num_layers/optimal:.0f}x\")\n",
    "print()\n",
    "print(\"Compute analysis:\")\n",
    "print(f\"  Without checkpointing: n forward + n backward = 2n\")\n",
    "print(f\"  With checkpointing:    n forward + n backward + n recompute = 3n\")\n",
    "print(f\"  Compute overhead: ~50% of total training time\")\n",
    "print()\n",
    "print(\"KEY: The sqrt(n) rule - optimal checkpointing reduces memory from O(n) to O(sqrt(n))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. PyTorch API - Basic Usage\n",
    "\n",
    "PyTorch provides `torch.utils.checkpoint.checkpoint()` for activation checkpointing. The basic pattern is simple: wrap the function you want to checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Checkpoint Comparison (small model)\n",
      "--------------------------------------------------\n",
      "WITHOUT Checkpointing: 4286.08 MB\n",
      "WITH Checkpointing:    4277.69 MB\n",
      "\n",
      "NOTE: Small model shows minimal difference.\n",
      "Savings become significant with deeper models (see Section 5: ResNet50).\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Basic Checkpoint: Side-by-Side Comparison\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# ===== WITHOUT Checkpointing =====\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1024, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "# ===== WITH Checkpointing =====\n",
    "class CheckpointedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1024, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 1024)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run layer1 normally (so input to checkpointed region requires grad)\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        # Checkpoint layer2: activations won't be stored, will be recomputed\n",
    "        x = checkpoint(lambda t: torch.relu(self.layer2(t)), x, use_reentrant=False)\n",
    "        return x\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Test without checkpointing\n",
    "    model1 = SimpleModel().cuda()\n",
    "    x1 = torch.randn(1, 1024).cuda()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out1 = model1(x1)\n",
    "    out1.sum().backward()\n",
    "    mem_no_ckpt = torch.cuda.max_memory_allocated() / 1e6\n",
    "    del model1, x1, out1\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Test with checkpointing\n",
    "    model2 = CheckpointedModel().cuda()\n",
    "    x2 = torch.randn(1, 1024).cuda()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out2 = model2(x2)\n",
    "    out2.sum().backward()\n",
    "    mem_with_ckpt = torch.cuda.max_memory_allocated() / 1e6\n",
    "    \n",
    "    print(\"Basic Checkpoint Comparison (small model)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"WITHOUT Checkpointing: {mem_no_ckpt:.2f} MB\")\n",
    "    print(f\"WITH Checkpointing:    {mem_with_ckpt:.2f} MB\")\n",
    "    print()\n",
    "    print(\"NOTE: Small model shows minimal difference.\")\n",
    "    print(\"Savings become significant with deeper models (see Section 5: ResNet50).\")\n",
    "else:\n",
    "    print(\"[Run on GPU to see memory comparison]\")\n",
    "    print()\n",
    "    print(\"API Usage:\")\n",
    "    print(\"  checkpoint(fn, *args, use_reentrant=False)\")\n",
    "    print(\"  - fn: callable to checkpoint\")\n",
    "    print(\"  - Always use use_reentrant=False (modern API)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. torch.compile and Memory Budget API\n",
    "\n",
    "torch.compile (PyTorch 2.0+) traces your forward and backward passes into a single joint graph and applies a \"min-cut\" partitioner that automatically decides which tensors to save and which to recompute.\n",
    "\n",
    "By default, min-cut prioritizes speed, not memory. The Memory Budget API gives you control over this trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How torch.compile handles activations:\n",
      "--------------------------------------------------\n",
      "\n",
      "1. TRACING\n",
      "   torch.compile traces forward AND backward into one graph.\n",
      "   This lets it see the whole picture.\n",
      "\n",
      "2. MIN-CUT PARTITIONING\n",
      "   The graph is split at the optimal points.\n",
      "   Algorithm minimizes: tensors crossing the cut\n",
      "   (These are the tensors saved for backward)\n",
      "\n",
      "3. AUTOMATIC RECOMPUTATION\n",
      "   Cheap ops (relu, add, mul) are recomputed automatically.\n",
      "   No user intervention needed.\n",
      "\n",
      "4. FUSION\n",
      "   Pointwise ops get fused into kernels.\n",
      "   Fused ops are fast to recompute.\n",
      "\n",
      "Result: torch.compile gives you SOME memory savings for FREE,\n",
      "        plus speed improvements from fusion.\n"
     ]
    }
   ],
   "source": [
    "# 4.1 torch.compile: The Min-Cut Partitioner\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"How torch.compile handles activations:\")\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "print(\"1. TRACING\")\n",
    "print(\"   torch.compile traces forward AND backward into one graph.\")\n",
    "print(\"   This lets it see the whole picture.\")\n",
    "print()\n",
    "print(\"2. MIN-CUT PARTITIONING\")\n",
    "print(\"   The graph is split at the optimal points.\")\n",
    "print(\"   Algorithm minimizes: tensors crossing the cut\")\n",
    "print(\"   (These are the tensors saved for backward)\")\n",
    "print()\n",
    "print(\"3. AUTOMATIC RECOMPUTATION\")\n",
    "print(\"   Cheap ops (relu, add, mul) are recomputed automatically.\")\n",
    "print(\"   No user intervention needed.\")\n",
    "print()\n",
    "print(\"4. FUSION\")\n",
    "print(\"   Pointwise ops get fused into kernels.\")\n",
    "print(\"   Fused ops are fast to recompute.\")\n",
    "print()\n",
    "print(\"Result: torch.compile gives you SOME memory savings for FREE,\")\n",
    "print(\"        plus speed improvements from fusion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN: dim=2048, batch=64, seq=128\n",
      "--------------------------------------------------\n",
      "Eager               :  2352.62 MB\n",
      "torch.compile       :  1933.14 MB\n",
      "Checkpointing       :  2201.57 MB\n",
      "\n",
      "OBSERVATION: torch.compile gives best memory efficiency here.\n",
      "\n",
      "WHY torch.compile often wins on simple models:\n",
      "  - Kernel fusion eliminates intermediate tensor allocations\n",
      "  - Min-cut partitioner automatically recomputes cheap ops\n",
      "  - Manual checkpointing has overhead without fusion benefits\n",
      "\n",
      "For deeper models (see Section 5: ResNet50), manual checkpointing\n",
      "provides more control and can achieve greater savings.\n"
     ]
    }
   ],
   "source": [
    "# 4.2 Comparing: Eager vs Compile vs Checkpointing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class SimpleFFN(nn.Module):\n",
    "    def __init__(self, dim=2048):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Wrapper(nn.Module):\n",
    "    \"\"\"Adds a small stem so the checkpointed region receives activations that require grad.\"\"\"\n",
    "    def __init__(self, dim=2048, use_checkpoint=False):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Linear(dim, dim)\n",
    "        self.ffn = SimpleFFN(dim)\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        if self.use_checkpoint:\n",
    "            x = checkpoint(self.ffn, x, use_reentrant=False)\n",
    "        else:\n",
    "            x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "def warmup(fn, zero_grad_fn, x, iters=2):\n",
    "    for _ in range(iters):\n",
    "        fn(x).sum().backward()\n",
    "        zero_grad_fn()\n",
    "\n",
    "def peak_memory_mb(fn, zero_grad_fn, x):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    out = fn(x)\n",
    "    out.sum().backward()\n",
    "    peak = torch.cuda.max_memory_allocated() / 1e6\n",
    "    zero_grad_fn()\n",
    "    return peak\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dim = 2048\n",
    "    batch = 64\n",
    "    seq = 128\n",
    "\n",
    "    x = torch.randn(batch, seq, dim, device=\"cuda\")\n",
    "\n",
    "    eager_model = Wrapper(dim, use_checkpoint=False).cuda()\n",
    "    ckpt_model = Wrapper(dim, use_checkpoint=True).cuda()\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 1) Eager mode\n",
    "    warmup(eager_model, lambda: eager_model.zero_grad(set_to_none=True), x)\n",
    "    results['Eager'] = peak_memory_mb(eager_model, lambda: eager_model.zero_grad(set_to_none=True), x)\n",
    "\n",
    "    # 2) torch.compile (if available)\n",
    "    if hasattr(torch, \"compile\"):\n",
    "        compiled = torch.compile(eager_model)\n",
    "        warmup(compiled, lambda: eager_model.zero_grad(set_to_none=True), x)\n",
    "        results['torch.compile'] = peak_memory_mb(compiled, lambda: eager_model.zero_grad(set_to_none=True), x)\n",
    "    else:\n",
    "        print(\"torch.compile not available in this PyTorch build\")\n",
    "\n",
    "    # 3) Activation checkpointing\n",
    "    warmup(ckpt_model, lambda: ckpt_model.zero_grad(set_to_none=True), x)\n",
    "    results['Checkpointing'] = peak_memory_mb(ckpt_model, lambda: ckpt_model.zero_grad(set_to_none=True), x)\n",
    "\n",
    "    print(f\"FFN: dim={dim}, batch={batch}, seq={seq}\")\n",
    "    print(\"-\" * 50)\n",
    "    for name, mem in results.items():\n",
    "        print(f\"{name:20s}: {mem:8.2f} MB\")\n",
    "    print()\n",
    "    \n",
    "    # Find best method\n",
    "    best = min(results, key=results.get)\n",
    "    print(f\"OBSERVATION: {best} gives best memory efficiency here.\")\n",
    "    print()\n",
    "    print(\"WHY torch.compile often wins on simple models:\")\n",
    "    print(\"  - Kernel fusion eliminates intermediate tensor allocations\")\n",
    "    print(\"  - Min-cut partitioner automatically recomputes cheap ops\")\n",
    "    print(\"  - Manual checkpointing has overhead without fusion benefits\")\n",
    "    print()\n",
    "    print(\"For deeper models (see Section 5: ResNet50), manual checkpointing\")\n",
    "    print(\"provides more control and can achieve greater savings.\")\n",
    "else:\n",
    "    print(\"[Run on GPU for comparison]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### What Gets Recomputed at Each Budget Level?\n",
    "\n",
    "Based on real transformer results:\n",
    "\n",
    "| Budget | Recomputes | Saves | Memory |\n",
    "|--------|------------|-------|--------|\n",
    "| 1.0 (default) | Nothing extra | Everything | 100% |\n",
    "| 0.7 | Pointwise ops (gelu, add, mul) | Matmuls, attention | ~85% |\n",
    "| 0.5 | Pointwise + some matmuls | Attention (most expensive) | ~50% |\n",
    "| 0.3 | Pointwise + most matmuls | Only attention | ~35% |\n",
    "| 0.0 | Everything (like full AC) | Almost nothing | Minimum |\n",
    "\n",
    "**Key insight:** 50% memory reduction by recomputing only pointwise ops. Attention is expensive. Recompute it last."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4b. External Libraries Reference\n",
    "\n",
    "Beyond PyTorch's built-in checkpointing, external libraries offer additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEEPSPEED ACTIVATION CHECKPOINTING\n",
      "==================================\n",
      "pip install deepspeed\n",
      "\n",
      "Key features:\n",
      "- Automatic checkpoint placement\n",
      "- CPU offloading for extreme memory savings (offload activations to RAM!)\n",
      "- Integrated with ZeRO optimizer stages\n",
      "- Best for: very large models (billions of parameters)\n",
      "\n",
      "Config in ds_config.json:\n",
      "{\n",
      "    \"activation_checkpointing\": {\n",
      "        \"partition_activations\": true,\n",
      "        \"contiguous_memory_optimization\": true,\n",
      "        \"cpu_checkpointing\": true\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "FAIRSCALE CHECKPOINT_WRAPPER\n",
      "============================\n",
      "pip install fairscale\n",
      "\n",
      "Key features:\n",
      "- Clean API: wrap modules directly with checkpoint_wrapper()\n",
      "- Works well with FSDP (Fully Sharded Data Parallel)\n",
      "- Simpler than DeepSpeed for many use cases\n",
      "- Best for: medium-scale training\n",
      "\n",
      "Usage:\n",
      "from fairscale.nn.checkpoint import checkpoint_wrapper\n",
      "\n",
      "layer = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU())\n",
      "checkpointed_layer = checkpoint_wrapper(layer)\n",
      "\n",
      "\n",
      "TOOL SELECTION GUIDE\n",
      "====================\n",
      "- PyTorch native: Best for most cases, well-maintained\n",
      "- DeepSpeed: Very large models, need CPU offloading\n",
      "- FairScale: Using FSDP, prefer simpler API\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# External Libraries: DeepSpeed and FairScale\n",
    "\n",
    "print(\"\"\"\n",
    "DEEPSPEED ACTIVATION CHECKPOINTING\n",
    "==================================\n",
    "pip install deepspeed\n",
    "\n",
    "Key features:\n",
    "- Automatic checkpoint placement\n",
    "- CPU offloading for extreme memory savings (offload activations to RAM!)\n",
    "- Integrated with ZeRO optimizer stages\n",
    "- Best for: very large models (billions of parameters)\n",
    "\n",
    "Config in ds_config.json:\n",
    "{\n",
    "    \"activation_checkpointing\": {\n",
    "        \"partition_activations\": true,\n",
    "        \"contiguous_memory_optimization\": true,\n",
    "        \"cpu_checkpointing\": true\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "FAIRSCALE CHECKPOINT_WRAPPER\n",
    "============================\n",
    "pip install fairscale\n",
    "\n",
    "Key features:\n",
    "- Clean API: wrap modules directly with checkpoint_wrapper()\n",
    "- Works well with FSDP (Fully Sharded Data Parallel)\n",
    "- Simpler than DeepSpeed for many use cases\n",
    "- Best for: medium-scale training\n",
    "\n",
    "Usage:\n",
    "from fairscale.nn.checkpoint import checkpoint_wrapper\n",
    "\n",
    "layer = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU())\n",
    "checkpointed_layer = checkpoint_wrapper(layer)\n",
    "\n",
    "\n",
    "TOOL SELECTION GUIDE\n",
    "====================\n",
    "- PyTorch native: Best for most cases, well-maintained\n",
    "- DeepSpeed: Very large models, need CPU offloading\n",
    "- FairScale: Using FSDP, prefer simpler API\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Case Study: Activation Checkpointing in ResNet50\n",
    "\n",
    "ResNet50 is deep enough to benefit from checkpointing. It has 4 stages with multiple bottleneck blocks. Each block stores feature maps for backward.\n",
    "\n",
    "This section shows how to apply checkpointing to a real production model. You will see:\n",
    "1. How to modify torchvision's ResNet50\n",
    "2. Memory comparison across checkpointing strategies\n",
    "3. A complete training loop with checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 Structure:\n",
      "------------------------------------------------------------\n",
      "conv1:  1 conv layer\n",
      "layer1: 3 Bottleneck blocks (64 -> 256 channels)\n",
      "layer2: 4 Bottleneck blocks (128 -> 512 channels)\n",
      "layer3: 6 Bottleneck blocks (256 -> 1024 channels)\n",
      "layer4: 3 Bottleneck blocks (512 -> 2048 channels)\n",
      "fc:     1 linear layer\n",
      "\n",
      "Total Bottleneck blocks: 16\n",
      "\n",
      "Each Bottleneck block has 3 conv layers + skip connection.\n",
      "Feature maps grow larger in early layers, then shrink spatially.\n",
      "layer3 often has the largest activation memory (1024 channels, moderate spatial size).\n"
     ]
    }
   ],
   "source": [
    "# 5.1 ResNet50 Architecture Overview\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    import torchvision.models as models\n",
    "except Exception as e:\n",
    "    models = None\n",
    "    print(\"torchvision is required for the ResNet50 case study.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "if models is not None:\n",
    "    # Load ResNet50 without downloading weights\n",
    "    try:\n",
    "        resnet50 = models.resnet50(weights=None)\n",
    "    except TypeError:\n",
    "        # Older torchvision\n",
    "        resnet50 = models.resnet50(pretrained=False)\n",
    "\n",
    "    print(\"ResNet50 Structure:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"conv1:  1 conv layer\")\n",
    "    print(f\"layer1: {len(resnet50.layer1)} Bottleneck blocks (64 -> 256 channels)\")\n",
    "    print(f\"layer2: {len(resnet50.layer2)} Bottleneck blocks (128 -> 512 channels)\")\n",
    "    print(f\"layer3: {len(resnet50.layer3)} Bottleneck blocks (256 -> 1024 channels)\")\n",
    "    print(f\"layer4: {len(resnet50.layer4)} Bottleneck blocks (512 -> 2048 channels)\")\n",
    "    print(f\"fc:     1 linear layer\")\n",
    "    print()\n",
    "    print(\n",
    "        f\"Total Bottleneck blocks: {len(resnet50.layer1) + len(resnet50.layer2) + len(resnet50.layer3) + len(resnet50.layer4)}\"\n",
    "    )\n",
    "    print()\n",
    "    print(\"Each Bottleneck block has 3 conv layers + skip connection.\")\n",
    "    print(\"Feature maps grow larger in early layers, then shrink spatially.\")\n",
    "    print(\"layer3 often has the largest activation memory (1024 channels, moderate spatial size).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50Checkpointed created with 4 strategies:\n",
      "------------------------------------------------------------\n",
      "'none':       No checkpointing (baseline)\n",
      "'per_stage':  Checkpoint each of the 4 stages\n",
      "'per_block':  Checkpoint each of the 16 bottleneck blocks\n",
      "'aggressive': Only checkpoint layer3 and layer4 (best ROI)\n"
     ]
    }
   ],
   "source": [
    "# 5.2 ResNet50 with Checkpointing: Three Strategies\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "try:\n",
    "    import torchvision.models as models\n",
    "except Exception as e:\n",
    "    models = None\n",
    "    ResNet50Checkpointed = None\n",
    "    print(\"torchvision is required for ResNet50Checkpointed.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "if models is not None:\n",
    "    class ResNet50Checkpointed(nn.Module):\n",
    "        \"\"\"ResNet50 with configurable checkpointing strategies.\"\"\"\n",
    "\n",
    "        def __init__(self, num_classes=1000, checkpoint_strategy='none'):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                checkpoint_strategy: 'none', 'per_stage', 'per_block', or 'aggressive'\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "\n",
    "            # Load base ResNet50 without downloading weights\n",
    "            try:\n",
    "                base = models.resnet50(weights=None)\n",
    "            except TypeError:\n",
    "                base = models.resnet50(pretrained=False)\n",
    "\n",
    "            # Copy all layers\n",
    "            self.conv1 = base.conv1\n",
    "            self.bn1 = base.bn1\n",
    "            self.relu = base.relu\n",
    "            self.maxpool = base.maxpool\n",
    "            self.layer1 = base.layer1\n",
    "            self.layer2 = base.layer2\n",
    "            self.layer3 = base.layer3\n",
    "            self.layer4 = base.layer4\n",
    "            self.avgpool = base.avgpool\n",
    "            self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "            self.checkpoint_strategy = checkpoint_strategy\n",
    "\n",
    "        def _forward_stage(self, stage, x):\n",
    "            \"\"\"Forward through a stage (layer1, layer2, etc.).\"\"\"\n",
    "            for block in stage:\n",
    "                x = block(x)\n",
    "            return x\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Stem\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "            # NOTE: ResNet blocks include BatchNorm, which updates running stats in train mode.\n",
    "            # Checkpointing recomputes forward in backward, which can update BN stats twice.\n",
    "            # For strict parity, consider freezing BN stats (eval for BN) or using GroupNorm.\n",
    "\n",
    "            if self.checkpoint_strategy == 'none':\n",
    "                x = self.layer1(x)\n",
    "                x = self.layer2(x)\n",
    "                x = self.layer3(x)\n",
    "                x = self.layer4(x)\n",
    "\n",
    "            elif self.checkpoint_strategy == 'per_stage':\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer1, t), x, use_reentrant=False)\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer2, t), x, use_reentrant=False)\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer3, t), x, use_reentrant=False)\n",
    "                x = checkpoint(lambda t: self._forward_stage(self.layer4, t), x, use_reentrant=False)\n",
    "\n",
    "            elif self.checkpoint_strategy == 'per_block':\n",
    "                for block in self.layer1:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer2:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer3:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer4:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "\n",
    "            elif self.checkpoint_strategy == 'aggressive':\n",
    "                x = self.layer1(x)\n",
    "                x = self.layer2(x)\n",
    "                for block in self.layer3:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "                for block in self.layer4:\n",
    "                    x = checkpoint(block, x, use_reentrant=False)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown checkpoint_strategy: {self.checkpoint_strategy}\")\n",
    "\n",
    "            # Head\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "\n",
    "    print(\"ResNet50Checkpointed created with 4 strategies:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"'none':       No checkpointing (baseline)\")\n",
    "    print(\"'per_stage':  Checkpoint each of the 4 stages\")\n",
    "    print(\"'per_block':  Checkpoint each of the 16 bottleneck blocks\")\n",
    "    print(\"'aggressive': Only checkpoint layer3 and layer4 (best ROI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 Checkpointing Comparison\n",
      "Batch size: 32, Image size: 224x224\n",
      "----------------------------------------------------------------------\n",
      "Strategy        Peak Memory (MB)     Time (ms)       Mem Savings    \n",
      "----------------------------------------------------------------------\n",
      "none            4143.20              279.37             0.0%\n",
      "aggressive      3737.85              318.93             9.8%\n",
      "per_stage       2873.71              369.58            30.6%\n",
      "per_block       2468.50              368.85            40.4%\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Observations:\n",
      "- 'aggressive' gives best memory/speed balance (checkpoints only deep layers)\n",
      "- 'per_block' gives maximum memory savings but highest overhead\n",
      "- 'per_stage' is a middle ground\n"
     ]
    }
   ],
   "source": [
    "# 5.3 Memory Comparison: ResNet50 Strategies\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "def benchmark_resnet50(strategy, batch_size=32, image_size=224, num_iters=5):\n",
    "    \"\"\"Benchmark memory and time for a ResNet50 checkpointing strategy.\"\"\"\n",
    "    if ResNet50Checkpointed is None:\n",
    "        raise RuntimeError(\"ResNet50Checkpointed is not available (torchvision import likely failed).\")\n",
    "\n",
    "    model = ResNet50Checkpointed(num_classes=1000, checkpoint_strategy=strategy).cuda()\n",
    "    model.train()\n",
    "\n",
    "    x = torch.randn(batch_size, 3, image_size, image_size, device=\"cuda\")\n",
    "    target = torch.randint(0, 1000, (batch_size,), device=\"cuda\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(2):\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Measure memory + time\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(num_iters):\n",
    "        out = model(x)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed = (time.time() - start) / num_iters * 1000  # ms/iter\n",
    "    peak_mem = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
    "\n",
    "    del model, x, target\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return peak_mem, elapsed\n",
    "\n",
    "if torch.cuda.is_available() and ResNet50Checkpointed is not None:\n",
    "    batch_size = 32\n",
    "    image_size = 224\n",
    "\n",
    "    print(\"ResNet50 Checkpointing Comparison\")\n",
    "    print(f\"Batch size: {batch_size}, Image size: {image_size}x{image_size}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Strategy':<15} {'Peak Memory (MB)':<20} {'Time (ms)':<15} {'Mem Savings':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    strategies = ['none', 'aggressive', 'per_stage', 'per_block']\n",
    "    results = {}\n",
    "\n",
    "    for strategy in strategies:\n",
    "        mem, time_ms = benchmark_resnet50(strategy, batch_size, image_size)\n",
    "        results[strategy] = (mem, time_ms)\n",
    "\n",
    "    baseline_mem = results['none'][0]\n",
    "    for strategy in strategies:\n",
    "        mem, time_ms = results[strategy]\n",
    "        savings = (1 - mem / baseline_mem) * 100\n",
    "        print(f\"{strategy:<15} {mem:<20.2f} {time_ms:<15.2f} {savings:>6.1f}%\")\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "    print(\"\\nObservations:\")\n",
    "    print(\"- 'aggressive' gives best memory/speed balance (checkpoints only deep layers)\")\n",
    "    print(\"- 'per_block' gives maximum memory savings but highest overhead\")\n",
    "    print(\"- 'per_stage' is a middle ground\")\n",
    "else:\n",
    "    print(\"[Run on GPU and ensure torchvision is installed for ResNet50 benchmarks]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding maximum batch size for ResNet50 (224x224 images)\n",
      "--------------------------------------------------\n",
      "none           : max batch = 164 (1.00x)\n",
      "aggressive     : max batch = 192 (1.17x)\n",
      "per_block      : max batch = 256 (1.56x)\n",
      "--------------------------------------------------\n",
      "\n",
      "With checkpointing, you can often fit larger batches.\n"
     ]
    }
   ],
   "source": [
    "# 5.4 Maximum Batch Size: ResNet50\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def find_max_batch_resnet50(strategy, start=16, max_batch=256):\n",
    "    \"\"\"Find maximum batch size before OOM for ResNet50 (CUDA only).\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is required for find_max_batch_resnet50().\")\n",
    "    if ResNet50Checkpointed is None:\n",
    "        raise RuntimeError(\"ResNet50Checkpointed is not available (torchvision import likely failed).\")\n",
    "\n",
    "    start = max(1, int(start))\n",
    "    max_batch = max(start, int(max_batch))\n",
    "\n",
    "    def can_run(batch_size):\n",
    "        model = None\n",
    "        x = None\n",
    "        target = None\n",
    "        out = None\n",
    "        loss = None\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            model = ResNet50Checkpointed(checkpoint_strategy=strategy).cuda()\n",
    "            model.train()\n",
    "            x = torch.randn(batch_size, 3, 224, 224, device=\"cuda\")\n",
    "            target = torch.randint(0, 1000, (batch_size,), device=\"cuda\")\n",
    "\n",
    "            out = model(x)\n",
    "            loss = nn.CrossEntropyLoss()(out, target)\n",
    "            loss.backward()\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                return False\n",
    "            raise\n",
    "        finally:\n",
    "            try:\n",
    "                if model is not None:\n",
    "                    model.zero_grad(set_to_none=True)\n",
    "            except Exception:\n",
    "                pass\n",
    "            del model, x, target, out, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Exponential search to find an upper bound, then binary search.\n",
    "    max_working = 0\n",
    "    batch = start\n",
    "    while batch <= max_batch and can_run(batch):\n",
    "        max_working = batch\n",
    "        batch *= 2\n",
    "\n",
    "    low = max_working + 1\n",
    "    high = min(batch, max_batch)\n",
    "\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        if can_run(mid):\n",
    "            max_working = mid\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "\n",
    "    return max_working\n",
    "\n",
    "if torch.cuda.is_available() and ResNet50Checkpointed is not None:\n",
    "    print(\"Finding maximum batch size for ResNet50 (224x224 images)\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    strategies = ['none', 'aggressive', 'per_block']\n",
    "    baseline = None\n",
    "\n",
    "    for strategy in strategies:\n",
    "        max_batch = find_max_batch_resnet50(strategy)\n",
    "        if baseline is None:\n",
    "            baseline = max_batch\n",
    "        improvement = (max_batch / baseline) if baseline else float('inf')\n",
    "        print(f\"{strategy:<15}: max batch = {max_batch:>3d} ({improvement:.2f}x)\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nWith checkpointing, you can often fit larger batches.\")\n",
    "else:\n",
    "    print(\"[Run on GPU and ensure torchvision is installed to find max batch sizes]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1592307309.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if (use_amp and torch.cuda.is_available()) else None\n",
      "/tmp/ipython-input-1592307309.py:55: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet50 with 'aggressive' checkpointing\n",
      "Batch size: 32, Mixed precision: True\n",
      "--------------------------------------------------\n",
      "Epoch 1/3: loss = 2.4102, peak memory = 2739.79 MB\n",
      "Epoch 2/3: loss = 1.9871, peak memory = 2739.79 MB\n",
      "Epoch 3/3: loss = 1.5362, peak memory = 2739.79 MB\n",
      "--------------------------------------------------\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# 5.5 Complete Training Loop: ResNet50 with Checkpointing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_resnet50_with_checkpointing(\n",
    "    checkpoint_strategy='aggressive',\n",
    "    batch_size=32,\n",
    "    num_epochs=3,\n",
    "    num_samples=256,  # Small for demo\n",
    "    use_amp=True,\n",
    "):\n",
    "    \"\"\"Complete training loop with checkpointing and mixed precision.\"\"\"\n",
    "\n",
    "    if ResNet50Checkpointed is None:\n",
    "        raise RuntimeError(\"ResNet50Checkpointed is not available (torchvision import likely failed).\")\n",
    "\n",
    "    # Create model\n",
    "    model = ResNet50Checkpointed(\n",
    "        num_classes=10,  # Simplified for demo\n",
    "        checkpoint_strategy=checkpoint_strategy,\n",
    "    ).cuda()\n",
    "\n",
    "    # Create synthetic dataset\n",
    "    X = torch.randn(num_samples, 3, 224, 224)\n",
    "    y = torch.randint(0, 10, (num_samples,))\n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler() if (use_amp and torch.cuda.is_available()) else None\n",
    "\n",
    "    print(f\"Training ResNet50 with '{checkpoint_strategy}' checkpointing\")\n",
    "    print(f\"Batch size: {batch_size}, Mixed precision: {use_amp and scaler is not None}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.cuda()\n",
    "            batch_y = batch_y.cuda()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            if scaler is not None:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(batch_x)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: loss = {avg_loss:.4f}, peak memory = {peak_mem:.2f} MB\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "if torch.cuda.is_available() and ResNet50Checkpointed is not None:\n",
    "    model = train_resnet50_with_checkpointing(\n",
    "        checkpoint_strategy='aggressive',\n",
    "        batch_size=32,\n",
    "        num_epochs=3,\n",
    "        use_amp=True,\n",
    "    )\n",
    "else:\n",
    "    print(\"[Run on GPU and ensure torchvision is installed for training demo]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell-38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative approach: Monkey-patch torchvision ResNet50\n",
      "------------------------------------------------------------\n",
      "Pros:\n",
      "  - Uses official torchvision model\n",
      "  - Simple implementation\n",
      "  - Adjustable granularity via num_segments\n",
      "\n",
      "Cons:\n",
      "  - Less control over which blocks to checkpoint\n",
      "  - Harder to switch strategies dynamically\n",
      "\n",
      "Use the custom class (ResNet50Checkpointed) for flexible experiments.\n"
     ]
    }
   ],
   "source": [
    "# 5.6 Alternative: Using torchvision's Built-in Support\n",
    "\n",
    "import torch\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "try:\n",
    "    import torchvision.models as models\n",
    "except Exception as e:\n",
    "    models = None\n",
    "    print(\"torchvision is required for this alternative ResNet50 example.\")\n",
    "    print(f\"Details: {type(e).__name__}: {e}\")\n",
    "\n",
    "\n",
    "def _checkpoint_sequential(mod, segments, x):\n",
    "    \"\"\"checkpoint_sequential signature varies across PyTorch versions.\"\"\"\n",
    "    try:\n",
    "        return checkpoint_sequential(mod, segments, x, use_reentrant=False)\n",
    "    except TypeError:\n",
    "        return checkpoint_sequential(mod, segments, x)\n",
    "\n",
    "\n",
    "def resnet50_with_sequential_checkpoint(num_segments=4):\n",
    "    \"\"\"Monkey-patch torchvision ResNet50 to checkpoint its stage blocks.\"\"\"\n",
    "    if models is None:\n",
    "        raise RuntimeError(\"torchvision is not available\")\n",
    "\n",
    "    try:\n",
    "        model = models.resnet50(weights=None)\n",
    "    except TypeError:\n",
    "        model = models.resnet50(pretrained=False)\n",
    "\n",
    "    def checkpointed_forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Checkpoint each stage with user-controlled granularity.\n",
    "        x = _checkpoint_sequential(self.layer1, min(num_segments, len(self.layer1)), x)\n",
    "        x = _checkpoint_sequential(self.layer2, min(num_segments, len(self.layer2)), x)\n",
    "        x = _checkpoint_sequential(self.layer3, min(num_segments, len(self.layer3)), x)\n",
    "        x = _checkpoint_sequential(self.layer4, min(num_segments, len(self.layer4)), x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    import types\n",
    "\n",
    "    model.forward = types.MethodType(checkpointed_forward, model)\n",
    "    return model\n",
    "\n",
    "# Test it\n",
    "if models is not None:\n",
    "    model = resnet50_with_sequential_checkpoint(num_segments=4)\n",
    "    print(\"Alternative approach: Monkey-patch torchvision ResNet50\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Pros:\")\n",
    "    print(\"  - Uses official torchvision model\")\n",
    "    print(\"  - Simple implementation\")\n",
    "    print(\"  - Adjustable granularity via num_segments\")\n",
    "    print()\n",
    "    print(\"Cons:\")\n",
    "    print(\"  - Less control over which blocks to checkpoint\")\n",
    "    print(\"  - Harder to switch strategies dynamically\")\n",
    "    print()\n",
    "    print(\"Use the custom class (ResNet50Checkpointed) for flexible experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "### Impressions/Conclusions (5: ResNet50 Case Study)\n",
    "\n",
    "ResNet50 is a perfect testbed for checkpointing because:\n",
    "- 16 bottleneck blocks across 4 stages\n",
    "- Deep enough to benefit, not so deep it is impractical\n",
    "- Widely used in production\n",
    "\n",
    "Key findings:\n",
    "\n",
    "**Strategy Selection:**\n",
    "- `aggressive` (checkpoint layer3+layer4): Best ROI. These layers have the most parameters and largest feature maps.\n",
    "- `per_block`: Maximum memory savings (~40-50%), but highest compute overhead (~30-40%).\n",
    "- `per_stage`: Middle ground. Good for quick wins.\n",
    "\n",
    "**Practical Impact:**\n",
    "- 1.5-2x larger batch sizes possible\n",
    "- Combined with mixed precision: up to 3x improvement\n",
    "\n",
    "**BatchNorm Caveat:**\n",
    "\n",
    "ResNet blocks use BatchNorm. Checkpointing recomputes forward in backward, which can update BN running stats twice in `train()` mode. If you need strict parity, consider freezing BN stats (set BN layers to eval) or using GroupNorm.\n",
    "\n",
    "**Production Recommendations:**\n",
    "1. Start with `aggressive` strategy\n",
    "2. Combine with mixed precision (AMP)\n",
    "3. Profile before and after\n",
    "4. If still OOM, move to `per_block`\n",
    "\n",
    "The ResNet50Checkpointed class is a solid starting point. Copy it into your codebase and adapt it to your training setup and normalization strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Conclusion\n",
    "\n",
    "Activation checkpointing is a memory-compute trade-off. It trades extra forward passes for reduced memory usage. The trade-off is usually worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cell-41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KEY TAKEAWAYS\n",
      "=============\n",
      "\n",
      "1. ACTIVATIONS DOMINATE MEMORY\n",
      "   - Not parameters, not gradients\n",
      "   - Scales with batch_size * depth * hidden_size\n",
      "   - Peak memory occurs at start of backward pass\n",
      "\n",
      "2. CHECKPOINTING TRADES COMPUTE FOR MEMORY\n",
      "   - Save some activations (checkpoints)\n",
      "   - Recompute others during backward pass\n",
      "   - Typical: 30-50% memory savings, ~50% compute overhead\n",
      "\n",
      "3. THE SQRT(N) RULE\n",
      "   - Optimal checkpointing: O(sqrt(n)) memory for n layers\n",
      "   - 100 layers -> ~10 checkpoints -> 10x memory reduction\n",
      "\n",
      "4. PRACTICAL PATTERNS\n",
      "   - checkpoint(): Single modules/functions\n",
      "   - checkpoint_sequential(): Sequential models\n",
      "   - Always use use_reentrant=False\n",
      "\n",
      "5. ADVANCED: SELECTIVE ACTIVATION CHECKPOINT (SAC)\n",
      "   - Fine-grained control: choose what to save vs recompute\n",
      "   - Policy functions: MUST_SAVE for expensive ops, PREFER_RECOMPUTE for cheap\n",
      "   - Sweet spot: save matmuls, recompute pointwise ops\n",
      "   - Use aten.<op>.default for correct op matching\n",
      "\n",
      "6. ADVANCED: MEMORY BUDGET API (torch.compile)\n",
      "   - One line: torch._dynamo.config.activation_memory_budget = 0.5\n",
      "   - Automatic pareto-optimal recomputation strategies\n",
      "   - Budget 0 = full AC, Budget 1 = default compile\n",
      "\n",
      "7. COMBINE WITH OTHER TECHNIQUES\n",
      "   - Mixed precision: 2x memory from fp16\n",
      "   - Gradient accumulation: Effective larger batches\n",
      "   - DDP: Scale across GPUs\n",
      "   - Together: Train 4-5x larger models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.1 Key Takeaways\n",
    "\n",
    "print(\"\"\"\n",
    "KEY TAKEAWAYS\n",
    "=============\n",
    "\n",
    "1. ACTIVATIONS DOMINATE MEMORY\n",
    "   - Not parameters, not gradients\n",
    "   - Scales with batch_size * depth * hidden_size\n",
    "   - Peak memory occurs at start of backward pass\n",
    "\n",
    "2. CHECKPOINTING TRADES COMPUTE FOR MEMORY\n",
    "   - Save some activations (checkpoints)\n",
    "   - Recompute others during backward pass\n",
    "   - Typical: 30-50% memory savings, ~50% compute overhead\n",
    "\n",
    "3. THE SQRT(N) RULE\n",
    "   - Optimal checkpointing: O(sqrt(n)) memory for n layers\n",
    "   - 100 layers -> ~10 checkpoints -> 10x memory reduction\n",
    "\n",
    "4. PRACTICAL PATTERNS\n",
    "   - checkpoint(): Single modules/functions\n",
    "   - checkpoint_sequential(): Sequential models\n",
    "   - Always use use_reentrant=False\n",
    "\n",
    "5. ADVANCED: SELECTIVE ACTIVATION CHECKPOINT (SAC)\n",
    "   - Fine-grained control: choose what to save vs recompute\n",
    "   - Policy functions: MUST_SAVE for expensive ops, PREFER_RECOMPUTE for cheap\n",
    "   - Sweet spot: save matmuls, recompute pointwise ops\n",
    "   - Use aten.<op>.default for correct op matching\n",
    "\n",
    "6. ADVANCED: MEMORY BUDGET API (torch.compile)\n",
    "   - One line: torch._dynamo.config.activation_memory_budget = 0.5\n",
    "   - Automatic pareto-optimal recomputation strategies\n",
    "   - Budget 0 = full AC, Budget 1 = default compile\n",
    "\n",
    "7. COMBINE WITH OTHER TECHNIQUES\n",
    "   - Mixed precision: 2x memory from fp16\n",
    "   - Gradient accumulation: Effective larger batches\n",
    "   - DDP: Scale across GPUs\n",
    "   - Together: Train 4-5x larger models\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cell-42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WHEN TO USE ACTIVATION CHECKPOINTING\n",
      "====================================\n",
      "\n",
      "USE IT WHEN:\n",
      "- You are hitting OOM errors\n",
      "- You want larger batch sizes\n",
      "- Your model has 10+ layers\n",
      "- Training time is less critical than memory\n",
      "- You are training transformers or deep CNNs\n",
      "\n",
      "SKIP IT WHEN:\n",
      "- Model fits comfortably in memory\n",
      "- Training time is the bottleneck (not memory)\n",
      "- Model is shallow (< 5 layers)\n",
      "- You need maximum training speed\n",
      "\n",
      "QUICK DECISION TREE:\n",
      "\n",
      "  OOM Error?\n",
      "     |\n",
      "     v\n",
      "  YES --> Use checkpointing\n",
      "     |\n",
      "     v\n",
      "  Want larger batches?\n",
      "     |\n",
      "     v\n",
      "  YES --> Use checkpointing\n",
      "     |\n",
      "     v\n",
      "  Model > 10 layers?\n",
      "     |\n",
      "     v\n",
      "  YES --> Consider checkpointing\n",
      "     |\n",
      "     v\n",
      "  NO --> Probably skip it\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Decision Framework: When to Use Checkpointing\n",
    "\n",
    "print(\"\"\"\n",
    "WHEN TO USE ACTIVATION CHECKPOINTING\n",
    "====================================\n",
    "\n",
    "USE IT WHEN:\n",
    "- You are hitting OOM errors\n",
    "- You want larger batch sizes\n",
    "- Your model has 10+ layers\n",
    "- Training time is less critical than memory\n",
    "- You are training transformers or deep CNNs\n",
    "\n",
    "SKIP IT WHEN:\n",
    "- Model fits comfortably in memory\n",
    "- Training time is the bottleneck (not memory)\n",
    "- Model is shallow (< 5 layers)\n",
    "- You need maximum training speed\n",
    "\n",
    "QUICK DECISION TREE:\n",
    "\n",
    "  OOM Error?\n",
    "     |\n",
    "     v\n",
    "  YES --> Use checkpointing\n",
    "     |\n",
    "     v\n",
    "  Want larger batches?\n",
    "     |\n",
    "     v\n",
    "  YES --> Use checkpointing\n",
    "     |\n",
    "     v\n",
    "  Model > 10 layers?\n",
    "     |\n",
    "     v\n",
    "  YES --> Consider checkpointing\n",
    "     |\n",
    "     v\n",
    "  NO --> Probably skip it\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "### Final Impressions\n",
    "\n",
    "Activation checkpointing is not magic. It is a simple trade-off executed well.\n",
    "\n",
    "You now understand:\n",
    "- Why activations dominate memory (batch x depth x hidden)\n",
    "- How checkpointing works (recompute instead of store)\n",
    "- When to use it (memory-bound, deep models)\n",
    "- Basic implementation (checkpoint, checkpoint_sequential, use_reentrant=False)\n",
    "- Advanced control with SAC (choose exactly what to save)\n",
    "- Automatic optimization with Memory Budget API (one config line)\n",
    "\n",
    "The landscape of techniques:\n",
    "- **Eager**: Maximum speed, maximum memory\n",
    "- **torch.compile**: Free speedups, some automatic memory savings\n",
    "- **Memory Budget API**: Tunable compile-time optimization (0 to 1)\n",
    "- **Selective AC**: Manual control over save/recompute decisions\n",
    "- **Standard AC**: Maximum memory savings, ~50% compute overhead\n",
    "\n",
    "Start simple. Add complexity only when needed. Measure everything.\n",
    "\n",
    "Every large language model uses some form of activation checkpointing. Now you know exactly how it works and when to use each variant.\n",
    "\n",
    "Go train something bigger."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
