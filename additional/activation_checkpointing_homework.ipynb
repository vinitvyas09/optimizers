{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22025dfd",
   "metadata": {
    "id": "22025dfd"
   },
   "source": [
    "# Homework: Activation Checkpointing in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9479e0e",
   "metadata": {
    "id": "f9479e0e"
   },
   "source": [
    "In this notebook, you will learn how to use **activation checkpointing** in PyTorch to reduce GPU memory usage when training deep models such as language models.\n",
    "\n",
    "We will go step by step, comparing normal training vs checkpointed training, and measuring both **memory usage** and **speed trade-offs**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232873db",
   "metadata": {
    "id": "232873db"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1719942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.0+cu126\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "GPU Memory: 42.47 GB\n"
     ]
    }
   ],
   "source": [
    "# Environment Check\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f61ac646",
   "metadata": {
    "id": "f61ac646",
    "outputId": "2bfa58cf-0ec8-4d7e-cac2-606ceccc02a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.half\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Using dtype: {dtype}\")\n",
    "\n",
    "torch.backends.cuda.enable_flash_sdp(True) # ðŸ’¡ this alone won't enable flash attention :) check the Bonus section!\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd86332",
   "metadata": {
    "id": "0bd86332"
   },
   "source": [
    "## Step 1: Define a Toy Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea56f033",
   "metadata": {
    "id": "ea56f033"
   },
   "outputs": [],
   "source": [
    "class SimpleTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=512, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, 4*d_model)\n",
    "        self.linear2 = nn.Linear(4*d_model, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def attn_path(self, x):\n",
    "        h = self.norm1(x)\n",
    "        return x + self.attn(h,h,h, need_weights=True)[0]\n",
    "\n",
    "    def mlp_path(self, x):\n",
    "        h = self.norm2(x)\n",
    "        return x + self.linear2(F.relu(self.linear1(h)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn_path(x)\n",
    "        x = self.mlp_path(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb0c255",
   "metadata": {
    "id": "dfb0c255"
   },
   "source": [
    "## Step 2: Stack Many Blocks (Without Checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a6ad4e",
   "metadata": {
    "id": "86a6ad4e"
   },
   "outputs": [],
   "source": [
    "class DeepModel(nn.Module):\n",
    "    def __init__(self, depth=12):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([SimpleTransformerBlock().to(dtype=dtype, device='cuda') for _ in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb0344b",
   "metadata": {
    "id": "2cb0344b"
   },
   "source": [
    "## Step 3: Add Activation Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1e020",
   "metadata": {
    "id": "abd1e020"
   },
   "source": [
    "Complete the forward pass so to enable activation checkpointing:\n",
    "\n",
    "- Attention sublayer is checkpointed when ckpt_attn=True.\n",
    "\n",
    "- MLP sublayer is checkpointed when ckpt_mlp=True.\n",
    "\n",
    "ðŸ’¡ The first block runs normally (no checkpoint) to avoid cold-start edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5326d164",
   "metadata": {
    "id": "5326d164"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CheckpointedModel(nn.Module):\n",
    "    def __init__(self, depth=12, ckpt_attn=True, ckpt_mlp=True):\n",
    "        super().__init__()\n",
    "        self.ckpt_attn = ckpt_attn\n",
    "        self.ckpt_mlp = ckpt_mlp\n",
    "        self.blocks = nn.ModuleList([SimpleTransformerBlock().to(dtype=dtype, device='cuda')  for _ in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            if i == 0:\n",
    "                # First block runs normally (no checkpoint), as I read it may have cold-start edge cases\n",
    "                # Probably an ugly if condition, should rethink\n",
    "                x = block(x)\n",
    "            else:\n",
    "                # Apply checkpointing based on flags\n",
    "                if self.ckpt_attn:\n",
    "                    x = checkpoint(block.attn_path, x, use_reentrant=False)\n",
    "                else:\n",
    "                    x = block.attn_path(x)\n",
    "                \n",
    "                if self.ckpt_mlp:\n",
    "                    x = checkpoint(block.mlp_path, x, use_reentrant=False)\n",
    "                else:\n",
    "                    x = block.mlp_path(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b8c8a",
   "metadata": {
    "id": "7a1b8c8a"
   },
   "source": [
    "## Step 4: Compare Memory Usage and Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806171b",
   "metadata": {
    "id": "7806171b"
   },
   "source": [
    "For the 4 configs below, check the elapsed time and peak memory usage.\n",
    "\n",
    "Why does checkpointing the attention sublayer lead to large memory savings?\n",
    "\n",
    "Why does checkpointing the MLP sublayer usually give much smaller savings compared to attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c5e0985",
   "metadata": {
    "id": "4c5e0985",
    "outputId": "ec56a82c-346f-4070-b0a5-1c1658d7aab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without checkpointing: 0.12s, 3140 MB\n",
      "With checkpointing:    0.06s, 1414 MB\n",
      "CKPT mlp:              0.03s, 3093 MB\n",
      "CKPT attn:             0.04s, 1787 MB\n"
     ]
    }
   ],
   "source": [
    "def measure_run(model, x, steps=10):\n",
    "    model.to(device)\n",
    "    x = x.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters())\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(steps):\n",
    "        opt.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = out.mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    end = time.time()\n",
    "\n",
    "    max_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "    return (end-start)/steps, max_mem\n",
    "\n",
    "\n",
    "x = torch.randn(1, 4096, 512, dtype=dtype, device=\"cuda\")  # batch, seq, hidden - adjust based on your GPU memory\n",
    "\n",
    "m1 = CheckpointedModel(depth=12, ckpt_attn=False, ckpt_mlp=False)\n",
    "m2 = CheckpointedModel(depth=12, ckpt_attn=True, ckpt_mlp=True)\n",
    "m3 = CheckpointedModel(depth=12, ckpt_attn=False, ckpt_mlp=True)\n",
    "m4 = CheckpointedModel(depth=12, ckpt_attn=True, ckpt_mlp=False)\n",
    "t1, m1_mem = measure_run(m1, x)\n",
    "t2, m2_mem = measure_run(m2, x)\n",
    "t3, m3_mem = measure_run(m3, x)\n",
    "t4, m4_mem = measure_run(m4, x)\n",
    "\n",
    "\n",
    "print(f\"Without checkpointing: {t1:.2f}s, {m1_mem:.0f} MB\")\n",
    "print(f\"With checkpointing:    {t2:.2f}s, {m2_mem:.0f} MB\")\n",
    "print(f\"CKPT mlp:              {t3:.2f}s, {m3_mem:.0f} MB\")\n",
    "print(f\"CKPT attn:             {t4:.2f}s, {m4_mem:.0f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vlw9ixsmbjn",
   "metadata": {},
   "source": [
    "## Answer to our above questions\n",
    "\n",
    "Let's look at our forward pass, our model has d_model=512, n_heads=4, and we're feeding it a sequence of length 4096.\n",
    "\n",
    "In attention, we compute Q, K, V from the input, then do Q @ K^T to get attention scores. That matrix multiply means that with 4096 tokens, we have 4096 Ã— 4096 = ~16.7 million entries per head for the attention score calculation of Q@K_T. We have 4 heads, so that's ~65 million numbers per layer just for the attention weights. In bfloat16 (2 bytes each), that's roughly 134 MB per layer. Multiply by 11 checkpointed layers and we have ~1.5 GB of attention matrices that need to stick around for the backward pass.\n",
    "\n",
    "Now let's look at the MLP. It takes in (batch, seq, 512) and expands it to (batch, seq, 2048) via linear1, then squashes back down. The intermediate activation we need to save is (1, 4096, 2048) = ~8 million numbers. That's about 16 MB per layer, or ~176 MB across all layers.\n",
    "\n",
    "So, attention scales with sequence length *squared* because every token talks to every other token. MLP scales *linearly* because it just processes each token position independently because it doesn't care how many other tokens exist. At seq_len=4096, the squared term completely dominates. That's why checkpointing attention alone drops memory from 3140 MB to 1787 MB (saving ~1350 MB), while checkpointing MLP alone barely moves the needle from 3140 MB to 3093 MB (saving ~47 MB).\n",
    "\n",
    "This is also why FlashAttention is awesome as it's specifically attacking that O(n^2) memory problem in attention by being clever about what or how it computes/fuses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b35b3f",
   "metadata": {
    "id": "c1b35b3f"
   },
   "source": [
    "# Bonus\n",
    "Modify the code to only checkpoint **every other block** instead of all blocks.  \n",
    "   - What trade-off do you observe?\n",
    "\n",
    "#### 'checkpoint_sequential':\n",
    "\n",
    "PyTorch provides 'torch.utils.checkpoint.checkpoint_sequential' for checkpointing sequential models. It splits a model into N contiguous segments and checkpoints each segment. However, it's less suitable here because: a) it operates at the block level, not sublayer level (can't separately control attention vs MLP checkpointing), and b) it creates contiguous chunks rather than alternating patterns. It may be a good choice when we want simple, coarse-grained checkpointing without fine control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "xm3exchmku",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full checkpointing:        0.05s, 1819 MB\n",
      "Every-other checkpointing: 0.04s, 2803 MB\n"
     ]
    }
   ],
   "source": [
    "class EveryOtherCheckpointModel(nn.Module):\n",
    "    def __init__(self, depth=12, ckpt_attn=True, ckpt_mlp=True):\n",
    "        super().__init__()\n",
    "        self.ckpt_attn = ckpt_attn\n",
    "        self.ckpt_mlp = ckpt_mlp\n",
    "        self.blocks = nn.ModuleList([SimpleTransformerBlock().to(dtype=dtype, device='cuda') for _ in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            use_ckpt = (i % 2 == 1)  # checkpoint odd-indexed blocks only\n",
    "            if use_ckpt and self.ckpt_attn:\n",
    "                x = checkpoint(block.attn_path, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = block.attn_path(x)\n",
    "            if use_ckpt and self.ckpt_mlp:\n",
    "                x = checkpoint(block.mlp_path, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = block.mlp_path(x)\n",
    "        return x\n",
    "\n",
    "# Compare: full checkpointing vs every-other checkpointing\n",
    "x = torch.randn(1, 4096, 512, dtype=dtype, device=\"cuda\")\n",
    "\n",
    "m_full = CheckpointedModel(depth=12, ckpt_attn=True, ckpt_mlp=True)\n",
    "m_half = EveryOtherCheckpointModel(depth=12, ckpt_attn=True, ckpt_mlp=True)\n",
    "\n",
    "t_full, mem_full = measure_run(m_full, x)\n",
    "t_half, mem_half = measure_run(m_half, x)\n",
    "\n",
    "print(f\"Full checkpointing:        {t_full:.2f}s, {mem_full:.0f} MB\")\n",
    "print(f\"Every-other checkpointing: {t_half:.2f}s, {mem_half:.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w046jtiz5q",
   "metadata": {},
   "source": [
    "## Trade-off\n",
    "\n",
    "\n",
    "\n",
    "No checkpointing (from above) 0.12s, 3140 MB  \n",
    "Full checkpointing:        0.05s, 1819 MB  \n",
    "Every-other checkpointing: 0.04s, 2803 MB  \n",
    "\n",
    "When it comes to mem, every-other uses ~50% more memory than full checkpointing (2803 vs 2829 MB). This makes sense since we only discard half the activations. Still saves ~10% vs no checkpointing.\n",
    "Regarding speed though, this is surprisingly similar to full checkpointing (0.05s).... \n",
    "The recomputation cost is dominated by attention O(n^2), and we're still recomputing half the attention blocks.\n",
    "\n",
    "Overall though, if we compare every-other with full checkpointing, every-other seems to provide a bad trade-off if we just look at these numbers for the A100 GPU, but in general, we would expect every-other to have more compute involvement. We get most of the speed penalty of full checkpointing but just half of the memory savings. This is likely because attention recomputation is expensive regardless of how many blocks we skip...\n",
    "\n",
    "With this limited toy example, we could make a case that perhaps full checkpointing or no checkpointing are usually better choices than partial strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e5dae",
   "metadata": {
    "id": "609e5dae"
   },
   "source": [
    "# Bonus\n",
    "The code above currently does not use FlashAttention, which means attention has quadratic memory complexity with respect to sequence length (O(nÂ²)).\n",
    "\n",
    "ðŸ‘‰ However, PyTorch lets you enable FlashAttention by simply toggling a single flag in your code.\n",
    "\n",
    "ðŸ’¡ Challenge: Can you find and change that flag so that your model runs with FlashAttention instead of the standard (quadratic) path?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9v21jsj5vh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full ckpt (no flash):     0.05s, 2216 MB\n",
      "Full ckpt (flash):        0.05s, 1691 MB\n",
      "Every-other ckpt (flash): 0.04s, 1987 MB\n"
     ]
    }
   ],
   "source": [
    "class SimpleTransformerBlockFlash(nn.Module):\n",
    "    def __init__(self, d_model=512, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, 4*d_model)\n",
    "        self.linear2 = nn.Linear(4*d_model, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def attn_path(self, x):\n",
    "        h = self.norm1(x)\n",
    "        return x + self.attn(h, h, h, need_weights=False)[0]  # 'need_weights=False' enables FlashAttention!\n",
    "\n",
    "    def mlp_path(self, x):\n",
    "        h = self.norm2(x)\n",
    "        return x + self.linear2(F.relu(self.linear1(h)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn_path(x)\n",
    "        x = self.mlp_path(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CheckpointedModelWithFlashAttn(nn.Module):\n",
    "    def __init__(self, depth=12, ckpt_attn=True, ckpt_mlp=True):\n",
    "        super().__init__()\n",
    "        self.ckpt_attn = ckpt_attn\n",
    "        self.ckpt_mlp = ckpt_mlp\n",
    "        self.blocks = nn.ModuleList([SimpleTransformerBlockFlash().to(dtype=dtype, device='cuda') for _ in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            if i == 0:\n",
    "                x = block(x)\n",
    "            else:\n",
    "                if self.ckpt_attn:\n",
    "                    x = checkpoint(block.attn_path, x, use_reentrant=False)\n",
    "                else:\n",
    "                    x = block.attn_path(x)\n",
    "                if self.ckpt_mlp:\n",
    "                    x = checkpoint(block.mlp_path, x, use_reentrant=False)\n",
    "                else:\n",
    "                    x = block.mlp_path(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EveryOtherCheckpointModelWithFlashAttn(nn.Module):\n",
    "    def __init__(self, depth=12, ckpt_attn=True, ckpt_mlp=True):\n",
    "        super().__init__()\n",
    "        self.ckpt_attn = ckpt_attn\n",
    "        self.ckpt_mlp = ckpt_mlp\n",
    "        self.blocks = nn.ModuleList([SimpleTransformerBlockFlash().to(dtype=dtype, device='cuda') for _ in range(depth)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            use_ckpt = (i % 2 == 1)\n",
    "            if use_ckpt and self.ckpt_attn:\n",
    "                x = checkpoint(block.attn_path, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = block.attn_path(x)\n",
    "            if use_ckpt and self.ckpt_mlp:\n",
    "                x = checkpoint(block.mlp_path, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = block.mlp_path(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Compare with and without FlashAttention\n",
    "x = torch.randn(1, 4096, 512, dtype=dtype, device=\"cuda\")\n",
    "\n",
    "m_no_flash = CheckpointedModel(depth=12, ckpt_attn=True, ckpt_mlp=True)\n",
    "m_flash = CheckpointedModelWithFlashAttn(depth=12, ckpt_attn=True, ckpt_mlp=True)\n",
    "m_flash_every_other = EveryOtherCheckpointModelWithFlashAttn(depth=12, ckpt_attn=True, ckpt_mlp=True)\n",
    "\n",
    "t1, mem1 = measure_run(m_no_flash, x)\n",
    "t2, mem2 = measure_run(m_flash, x)\n",
    "t3, mem3 = measure_run(m_flash_every_other, x)\n",
    "\n",
    "print(f\"Full ckpt (no flash):     {t1:.2f}s, {mem1:.0f} MB\")\n",
    "print(f\"Full ckpt (flash):        {t2:.2f}s, {mem2:.0f} MB\")\n",
    "print(f\"Every-other ckpt (flash): {t3:.2f}s, {mem3:.0f} MB\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
