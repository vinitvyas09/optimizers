{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc50cf9b",
   "metadata": {},
   "source": [
    "# Week 1 — Optimizers as State Machines: Memory, Weight Decay, Gradient Accumulation, Checkpointing\n",
    "\n",
    "**Promise:** By the end, you can  \n",
    "1) write each update rule from memory,  \n",
    "2) compute (and *measure*) optimizer memory overhead from first principles,  \n",
    "3) reproduce/verify PyTorch behavior with minimal code, and  \n",
    "4) explain the “why” verbally without hand-waving.\n",
    "\n",
    "> Scope note: This notebook intentionally **does not** re-teach “momentum” and “second-order momentum” as concepts. It treats them as known and focuses on **comparisons, memory/state, and PyTorch semantics**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76cf2de",
   "metadata": {},
   "source": [
    "## 0 — Agenda, deliverables, and “whiteboard readiness”\n",
    "\n",
    "### 0.1 Mentor checklist → notebook map\n",
    "\n",
    "| Mentor item | Where answered | Proof artifact |\n",
    "|---|---|---|\n",
    "| SGD vs. SGD with momentum (memory footprint, #params) | §2 | `optimizer.state` inspection + byte counting |\n",
    "| SGD vs. Adam (memory footprint, #params) | §3 | state inspection + byte counting |\n",
    "| Weight decay in Adam vs AdamW | §4 | scalar “unit test” + PyTorch flag semantics |\n",
    "| Gradient accumulation (what + how in PyTorch) | §5 | correctness test vs “true big batch” |\n",
    "| Activation / gradient checkpointing | §6 | peak memory + runtime comparison |\n",
    "\n",
    "### 0.2 Deliverables\n",
    "\n",
    "- **One-page cheat sheet** at the end: update rules + memory multipliers + “when-to-use”.\n",
    "- **“Prove it” appendix:** tiny scalar examples showing Adam vs AdamW decay differs.\n",
    "- **PyTorch verification suite:** code cells that inspect `optimizer.state_dict()` and (if CUDA) measure peak memory.\n",
    "\n",
    "### 0.3 Reading links (primary sources)\n",
    "\n",
    "- Lightly blog (decision heuristics, resource framing):  \n",
    "  https://www.lightly.ai/blog/which-optimizer-should-i-use-for-my-machine-learning-project\n",
    "- PyTorch docs (SGD / Adam / AdamW / checkpoint):  \n",
    "  - https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html  \n",
    "  - https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html  \n",
    "  - https://docs.pytorch.org/docs/stable/generated/torch.optim.AdamW.html  \n",
    "  - https://docs.pytorch.org/docs/stable/checkpoint.html\n",
    "- Adam paper (Kingma & Ba, 2014): https://arxiv.org/abs/1412.6980  \n",
    "- Decoupled Weight Decay Regularization (Loshchilov & Hutter, 2017): https://arxiv.org/abs/1711.05101  \n",
    "\n",
    "> Ground rule for this notebook: if “PyTorch semantics” matters, we trust **the official docs + a runnable micro-test** over folklore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26131712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os, time, math, inspect\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Iterable, Tuple, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f60dd",
   "metadata": {},
   "source": [
    "## 1 — First principles: what costs memory during training?\n",
    "\n",
    "Think of training as a small ecosystem of tensors:\n",
    "\n",
    "- **Parameters**: \\(\\theta\\) (persist across steps)\n",
    "- **Gradients**: \\(g = \\nabla_\\theta L\\) (persist unless cleared)\n",
    "- **Optimizer state**: extra persistent tensors per parameter (e.g. momentum buffers, Adam moments)\n",
    "- **Activations**: intermediate tensors saved for backward (often the *dominant* memory term)\n",
    "\n",
    "### 1.1 A clean accounting model\n",
    "\n",
    "Let\n",
    "\n",
    "- \\(P\\) = total number of scalar parameters (sum of `.numel()`)\n",
    "- \\(b\\) = bytes per scalar (fp32 → 4, bf16/fp16 → 2)\n",
    "\n",
    "Then (very roughly):\n",
    "\n",
    "- params memory \\(\\approx P \\cdot b\\)\n",
    "- grads memory \\(\\approx P \\cdot b\\) (if grads are materialized)\n",
    "- optimizer state depends on optimizer: **SGD: 0–\\(P\\)**, **Adam: \\(2P\\)**, etc.\n",
    "- activations depend on batch/sequence/depth; checkpointing targets *this* term.\n",
    "\n",
    "### 1.2 PyTorch nuance: `zero_grad(set_to_none=True)`\n",
    "\n",
    "PyTorch’s `Optimizer.zero_grad()` supports setting gradients to `None` (not zeros). This:\n",
    "- typically **reduces memory** and can be faster,\n",
    "- but changes behavior: optimizers may **skip** parameters whose grad is `None`.\n",
    "\n",
    "We’ll demo that below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b17520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper utilities: byte counting and state inspection\n",
    "\n",
    "def pretty_bytes(n: int) -> str:\n",
    "    # Human-readable byte formatting\n",
    "    suffixes = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
    "    x = float(n)\n",
    "    for s in suffixes:\n",
    "        if x < 1024 or s == suffixes[-1]:\n",
    "            return f\"{x:.2f} {s}\"\n",
    "        x /= 1024.0\n",
    "\n",
    "def tensor_nbytes(t: torch.Tensor) -> int:\n",
    "    return t.numel() * t.element_size()\n",
    "\n",
    "def count_params(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def params_nbytes(model: nn.Module) -> int:\n",
    "    return sum(tensor_nbytes(p.data) for p in model.parameters())\n",
    "\n",
    "def grads_nbytes(model: nn.Module) -> int:\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total += tensor_nbytes(p.grad)\n",
    "    return total\n",
    "\n",
    "def optimizer_state_nbytes(optim: torch.optim.Optimizer) -> int:\n",
    "    # internal state uses Parameter objects as keys\n",
    "    total = 0\n",
    "    for p, st in optim.state.items():\n",
    "        for k, v in st.items():\n",
    "            if torch.is_tensor(v):\n",
    "                total += tensor_nbytes(v)\n",
    "            elif isinstance(v, (list, tuple)):\n",
    "                for item in v:\n",
    "                    if torch.is_tensor(item):\n",
    "                        total += tensor_nbytes(item)\n",
    "    return total\n",
    "\n",
    "def optimizer_state_summary(optim: torch.optim.Optimizer, max_items: int = 12) -> List[Tuple[str, Tuple[int, ...], str]]:\n",
    "    rows = []\n",
    "    for p, st in optim.state.items():\n",
    "        for k, v in st.items():\n",
    "            if torch.is_tensor(v):\n",
    "                rows.append((k, tuple(v.shape), str(v.dtype)))\n",
    "    # Sort for stable display\n",
    "    rows.sort(key=lambda x: (x[0], x[1]))\n",
    "    return rows[:max_items]\n",
    "\n",
    "def report_memory(model: nn.Module, optim: torch.optim.Optimizer, label: str = \"\") -> None:\n",
    "    P = count_params(model)\n",
    "    print(f\"--- {label} ---\")\n",
    "    print(\"P (numel):\", P)\n",
    "    print(\"params:\", pretty_bytes(params_nbytes(model)))\n",
    "    print(\"grads :\", pretty_bytes(grads_nbytes(model)))\n",
    "    print(\"state :\", pretty_bytes(optimizer_state_nbytes(optim)))\n",
    "    print(\"state keys sample:\", optimizer_state_summary(optim))\n",
    "    print()\n",
    "\n",
    "def cuda_peak_bytes() -> Optional[int]:\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    return torch.cuda.max_memory_allocated()\n",
    "\n",
    "def measure_peak_cuda(fn, *args, **kwargs) -> Optional[int]:\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available; skipping peak-memory measurement.\")\n",
    "        return None\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    fn(*args, **kwargs)\n",
    "    torch.cuda.synchronize()\n",
    "    return torch.cuda.max_memory_allocated()\n",
    "\n",
    "print(\"Optimizer.zero_grad signature:\", inspect.signature(torch.optim.Optimizer.zero_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e025eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny demo model\n",
    "class TinyMLP(nn.Module):\n",
    "    def __init__(self, d_in=128, d_h=256, d_out=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, d_h),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_h, d_out),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TinyMLP().to(device)\n",
    "\n",
    "x = torch.randn(32, 128, device=device)\n",
    "y = torch.randint(0, 10, (32,), device=device)\n",
    "\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "# We'll use a placeholder optimizer for now\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "report_memory(model, optim, \"initial (before any backward/step)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9403fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Demo: zero_grad(set_to_none=True) vs zeros\n",
    "\n",
    "# First backward pass -> grads materialize\n",
    "optim.zero_grad(set_to_none=True)\n",
    "loss = crit(model(x), y)\n",
    "loss.backward()\n",
    "print(\"After backward: grads_nbytes =\", pretty_bytes(grads_nbytes(model)))\n",
    "\n",
    "# Set grads to None\n",
    "optim.zero_grad(set_to_none=True)\n",
    "none_count = sum(1 for p in model.parameters() if p.grad is None)\n",
    "print(\"After zero_grad(set_to_none=True): #None grads =\", none_count, \"out of\", len(list(model.parameters())))\n",
    "print(\"grads_nbytes now =\", pretty_bytes(grads_nbytes(model)))\n",
    "\n",
    "# Backward again\n",
    "loss = crit(model(x), y)\n",
    "loss.backward()\n",
    "\n",
    "# Set grads to zeros (materializes/keeps grad tensors)\n",
    "optim.zero_grad(set_to_none=False)\n",
    "zero_count = sum(1 for p in model.parameters() if (p.grad is not None and torch.all(p.grad == 0)))\n",
    "print(\"After zero_grad(set_to_none=False): grads exist for all params?\",\n",
    "      all(p.grad is not None for p in model.parameters()))\n",
    "print(\"Example: #grads that are all-zero right now (often all):\", zero_count)\n",
    "print(\"grads_nbytes now =\", pretty_bytes(grads_nbytes(model)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb44bb",
   "metadata": {},
   "source": [
    "## 2 — SGD vs SGD with momentum (comparisons, not re-teaching momentum)\n",
    "\n",
    "### 2.1 Update rule (PyTorch doc version)\n",
    "\n",
    "PyTorch’s SGD algorithm (with optional momentum) conceptually does:\n",
    "\n",
    "- optional L2 penalty: \\(g_t \\leftarrow g_t + \\lambda \\theta_{t-1}\\)\n",
    "- optional momentum buffer \\(b_t\\)\n",
    "- parameter update: \\(\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\, g_t\\) (after any momentum/Nesterov transform)\n",
    "\n",
    "(Exact ordering/notation is in the official docs; PyTorch also notes its momentum differs subtly from some textbook variants.)\n",
    "\n",
    "### 2.2 “#params” answered precisely\n",
    "\n",
    "- **Trainable parameters**: the model’s \\(\\theta\\). This number is **identical** for SGD, SGD+momentum, Adam, AdamW.\n",
    "- **Optimizer state**: extra persistent tensors (not trainable parameters) that increase:\n",
    "  - GPU memory\n",
    "  - checkpoint size\n",
    "  - optimizer step compute\n",
    "\n",
    "### 2.3 Memory footprint (theory)\n",
    "\n",
    "Let \\(P\\) be parameter scalars.\n",
    "\n",
    "Persistent scalars (very rough; ignoring activations):\n",
    "\n",
    "- SGD (no momentum): params \\(P\\) + grads \\(P\\) + state \\(0\\) → \\(\\approx 2P\\)\n",
    "- SGD + momentum: params \\(P\\) + grads \\(P\\) + momentum buffer \\(P\\) → \\(\\approx 3P\\)\n",
    "\n",
    "We’ll *verify* by inspecting `optimizer.state` after one step (state is often allocated lazily).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f4ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 Memory footprint comparison: measured via optimizer.state\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = TinyMLP().to(device)\n",
    "x = torch.randn(32, 128, device=device)\n",
    "y = torch.randint(0, 10, (32,), device=device)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "def one_step(optim: torch.optim.Optimizer, set_to_none: bool = True):\n",
    "    optim.zero_grad(set_to_none=set_to_none)\n",
    "    loss = crit(model(x), y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss.item()\n",
    "\n",
    "# SGD without momentum\n",
    "optim_sgd = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.0)\n",
    "report_memory(model, optim_sgd, \"SGD before step\")\n",
    "one_step(optim_sgd)\n",
    "report_memory(model, optim_sgd, \"SGD after 1 step\")\n",
    "\n",
    "# SGD with momentum\n",
    "model2 = TinyMLP().to(device)\n",
    "optim_mom = torch.optim.SGD(model2.parameters(), lr=1e-2, momentum=0.9)\n",
    "report_memory(model2, optim_mom, \"SGD+momentum before step\")\n",
    "# do one step\n",
    "optim_mom.zero_grad(set_to_none=True)\n",
    "loss = crit(model2(x), y)\n",
    "loss.backward()\n",
    "optim_mom.step()\n",
    "report_memory(model2, optim_mom, \"SGD+momentum after 1 step\")\n",
    "\n",
    "print(\"Note: momentum creates per-parameter state keys like 'momentum_buffer'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2eca12",
   "metadata": {},
   "source": [
    "## 3 — SGD vs Adam (memory footprint + practical framing)\n",
    "\n",
    "### 3.1 Adam as a state machine (minimal)\n",
    "\n",
    "Adam keeps two per-parameter exponential moving averages (EMAs):\n",
    "- \\(m_t\\): EMA of gradients (first moment estimate)\n",
    "- \\(v_t\\): EMA of squared gradients (second moment estimate)\n",
    "\n",
    "Update uses a normalized step roughly like:\n",
    "\\[\n",
    "\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\frac{\\hat m_t}{\\sqrt{\\hat v_t} + \\epsilon}.\n",
    "\\]\n",
    "\n",
    "### 3.2 Memory footprint (theory)\n",
    "\n",
    "- Adam state: \\(m\\) and \\(v\\) → **+2P** scalars  \n",
    "- Total persistent ≈ params \\(P\\) + grads \\(P\\) + state \\(2P\\) → \\(\\approx 4P\\)\n",
    "\n",
    "If AMSGrad is enabled, there’s an extra \\(v_{\\max}\\) buffer: **+P** more (≈ \\(5P\\)).\n",
    "\n",
    "### 3.3 Important PyTorch nuance (peak memory)\n",
    "\n",
    "PyTorch notes that `foreach` optimizer implementations can use **~sizeof(params)** extra *peak* memory due to intermediate tensorlists. This affects peak CUDA memory measurements even if persistent state sizes are unchanged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d2a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Memory footprint comparison: SGD vs Adam (measured)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model_sgd = TinyMLP().to(device)\n",
    "model_adam = TinyMLP().to(device)\n",
    "\n",
    "optim_sgd = torch.optim.SGD(model_sgd.parameters(), lr=1e-2, momentum=0.9)\n",
    "optim_adam = torch.optim.Adam(model_adam.parameters(), lr=1e-3)\n",
    "\n",
    "def train_step(model, optim):\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss = crit(model(x), y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    return loss.item()\n",
    "\n",
    "report_memory(model_sgd, optim_sgd, \"SGD+mom before\")\n",
    "train_step(model_sgd, optim_sgd)\n",
    "report_memory(model_sgd, optim_sgd, \"SGD+mom after 1 step\")\n",
    "\n",
    "report_memory(model_adam, optim_adam, \"Adam before\")\n",
    "train_step(model_adam, optim_adam)\n",
    "report_memory(model_adam, optim_adam, \"Adam after 1 step\")\n",
    "\n",
    "print(\"Adam state keys are typically: exp_avg, exp_avg_sq (and step).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c8041",
   "metadata": {},
   "source": [
    "## 4 — Weight decay in Adam vs AdamW (the “don’t mess this up” section)\n",
    "\n",
    "This is where a lot of people confidently say wrong things.\n",
    "\n",
    "### 4.1 Two concepts that people conflate\n",
    "\n",
    "**L2 regularization (penalty in the objective)**  \n",
    "Optimize:\n",
    "\\[\n",
    "L(\\theta) + \\frac{\\lambda}{2}\\|\\theta\\|^2\n",
    "\\]\n",
    "Then:\n",
    "\\[\n",
    "\\nabla_\\theta \\left( L(\\theta) + \\frac{\\lambda}{2}\\|\\theta\\|^2 \\right)\n",
    "= \\nabla_\\theta L(\\theta) + \\lambda \\theta.\n",
    "\\]\n",
    "So it **adds** \\(\\lambda \\theta\\) into the gradient pipeline.\n",
    "\n",
    "**Decoupled weight decay (shrink weights directly)**  \n",
    "Apply:\n",
    "\\[\n",
    "\\theta \\leftarrow (1 - \\gamma\\lambda)\\theta\n",
    "\\]\n",
    "*separately* from the gradient step.\n",
    "\n",
    "### 4.2 Equivalence for vanilla SGD\n",
    "\n",
    "SGD with L2:\n",
    "\\[\n",
    "\\theta \\leftarrow \\theta - \\gamma (g + \\lambda\\theta)\n",
    "= (1-\\gamma\\lambda)\\theta - \\gamma g.\n",
    "\\]\n",
    "So for *plain SGD*, “L2 penalty” and “weight decay” are effectively the same transformation.\n",
    "\n",
    "### 4.3 Non-equivalence for Adam\n",
    "\n",
    "If you inject \\(\\lambda\\theta\\) into Adam’s gradient path, it gets adapted/normalized by \\(1/\\sqrt{v}\\), so the effective shrink is **parameter-wise and history-dependent**.\n",
    "\n",
    "AdamW decouples: decay is applied directly to \\(\\theta\\), and **does not accumulate** into the moment estimates.\n",
    "\n",
    "### 4.4 PyTorch reality check\n",
    "\n",
    "In current PyTorch docs:\n",
    "\n",
    "- `torch.optim.Adam(..., weight_decay=λ, decoupled_weight_decay=False)` treats `weight_decay` as an **L2 penalty** (added into the gradient).\n",
    "- `torch.optim.Adam(..., decoupled_weight_decay=True)` is documented as **equivalent to AdamW**.\n",
    "- `torch.optim.AdamW` applies decay as a separate parameter shrink step and “does not accumulate in the momentum nor variance.”\n",
    "\n",
    "We’ll verify with a scalar micro-test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54507e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 Scalar “unit test”: show Adam(L2-style) vs AdamW(decoupled) differs\n",
    "\n",
    "def scalar_step(optim_ctor, *, lr=0.1, wd=0.1, betas=(0.0, 0.0), eps=1e-8, grad_value=0.0, decoupled=None):\n",
    "    p = torch.nn.Parameter(torch.tensor([1.0], device=device))\n",
    "    # Build optimizer with flexible signature\n",
    "    kwargs = dict(lr=lr, betas=betas, eps=eps, weight_decay=wd)\n",
    "    if decoupled is not None:\n",
    "        kwargs[\"decoupled_weight_decay\"] = decoupled\n",
    "    optim = optim_ctor([p], **kwargs)\n",
    "    # Manually assign a grad\n",
    "    p.grad = torch.tensor([grad_value], device=device)\n",
    "    optim.step()\n",
    "    return float(p.detach().cpu().item())\n",
    "\n",
    "# Adam: L2-style weight decay (decoupled_weight_decay=False)\n",
    "theta_adam_l2 = scalar_step(torch.optim.Adam, decoupled=False, grad_value=0.0)\n",
    "\n",
    "# Adam: decoupled weight decay (if supported) — should match AdamW behavior\n",
    "supports_decoupled = \"decoupled_weight_decay\" in inspect.signature(torch.optim.Adam).parameters\n",
    "theta_adam_decoupled = None\n",
    "if supports_decoupled:\n",
    "    theta_adam_decoupled = scalar_step(torch.optim.Adam, decoupled=True, grad_value=0.0)\n",
    "\n",
    "# AdamW: decoupled\n",
    "theta_adamw = scalar_step(torch.optim.AdamW, grad_value=0.0)\n",
    "\n",
    "print(\"Initial θ = 1.0, grad = 0.0, lr=0.1, wd=0.1, betas=(0,0)\")\n",
    "print(\"Adam (L2-style wd):        θ ->\", theta_adam_l2)\n",
    "if supports_decoupled:\n",
    "    print(\"Adam (decoupled wd=True): θ ->\", theta_adam_decoupled)\n",
    "print(\"AdamW (decoupled):        θ ->\", theta_adamw)\n",
    "\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(\"- Adam(L2-style): decay goes through Adam normalization; with grad=0, the first step can be ~lr in magnitude.\")\n",
    "print(\"- AdamW/decoupled: θ shrinks multiplicatively by (1 - lr*wd) when grad=0.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41074b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5b Same test but with a nonzero gradient (to see combined effects)\n",
    "\n",
    "theta_adam_l2_g = scalar_step(torch.optim.Adam, decoupled=False, grad_value=0.1)\n",
    "theta_adamw_g = scalar_step(torch.optim.AdamW, grad_value=0.1)\n",
    "print(\"Initial θ = 1.0, grad = 0.1, lr=0.1, wd=0.1, betas=(0,0)\")\n",
    "print(\"Adam (L2-style wd): θ ->\", theta_adam_l2_g)\n",
    "print(\"AdamW (decoupled):  θ ->\", theta_adamw_g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fbfe2a",
   "metadata": {},
   "source": [
    "### 4.6 Practical pattern: exclude weight decay on biases and normalization layers\n",
    "\n",
    "Common training practice (especially for Transformers) is to apply weight decay to “weight matrices” but **not** to:\n",
    "- bias terms\n",
    "- LayerNorm / BatchNorm parameters (scale/shift)\n",
    "\n",
    "Reason: those parameters often act like *calibration knobs*, and decaying them can hurt.\n",
    "\n",
    "Below is a robust PyTorch parameter-group builder that you can reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b649cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter-group utility: decay vs no-decay (bias + norm excluded)\n",
    "\n",
    "def build_param_groups_for_weight_decay(model: nn.Module, weight_decay: float) -> List[Dict[str, Any]]:\n",
    "    decay_params = []\n",
    "    no_decay_params = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        is_bias = name.endswith(\".bias\")\n",
    "        is_norm = any(nd in name.lower() for nd in [\"bn\", \"batchnorm\", \"layernorm\", \"ln\", \"norm\"])\n",
    "        if is_bias or is_norm:\n",
    "            no_decay_params.append(p)\n",
    "        else:\n",
    "            decay_params.append(p)\n",
    "    return [\n",
    "        {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "        {\"params\": no_decay_params, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "m = TinyMLP().to(device)\n",
    "groups = build_param_groups_for_weight_decay(m, weight_decay=0.01)\n",
    "print(\"# decay params:\", sum(p.numel() for p in groups[0][\"params\"]))\n",
    "print(\"# no-decay params:\", sum(p.numel() for p in groups[1][\"params\"]))\n",
    "\n",
    "opt = torch.optim.AdamW(groups, lr=1e-3)\n",
    "print(\"Param group weight_decays:\", [g[\"weight_decay\"] for g in opt.param_groups])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e589849",
   "metadata": {},
   "source": [
    "## 5 — Gradient accumulation (what problem + how in PyTorch)\n",
    "\n",
    "### 5.1 The problem it solves\n",
    "\n",
    "You want a larger **effective batch size** (for stability, variance reduction, or to match a paper), but your GPU can’t fit that batch’s activations.\n",
    "\n",
    "**Gradient accumulation** simulates a batch of size \\(B = K \\cdot b\\) by splitting it into \\(K\\) microbatches of size \\(b\\), accumulating gradients, then stepping once.\n",
    "\n",
    "### 5.2 The math you must get right\n",
    "\n",
    "If your loss is a *mean over the batch* (typical), then to match the gradient of the big batch you want:\n",
    "\n",
    "- either sum per-example losses over all microbatches and divide once,\n",
    "- or equivalently: **divide each microbatch loss by \\(K\\)** before `backward()`.\n",
    "\n",
    "### 5.3 Canonical PyTorch loop\n",
    "\n",
    "```\n",
    "optimizer.zero_grad()\n",
    "for micro_step in range(K):\n",
    "    loss = loss_fn(...) / K\n",
    "    loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "### 5.4 Verification test (must-have)\n",
    "\n",
    "Below we compare gradients from:\n",
    "1) one big batch, vs  \n",
    "2) K microbatches with accumulation\n",
    "\n",
    "on a deterministic model (no dropout/BatchNorm).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb42e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Correctness proof: big batch vs gradient accumulation\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Simple model with no stochastic layers\n",
    "model_big = nn.Sequential(nn.Linear(16, 32), nn.ReLU(), nn.Linear(32, 4)).to(device)\n",
    "model_acc = nn.Sequential(nn.Linear(16, 32), nn.ReLU(), nn.Linear(32, 4)).to(device)\n",
    "# Force identical init\n",
    "model_acc.load_state_dict(model_big.state_dict())\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "B = 64\n",
    "K = 4\n",
    "b = B // K\n",
    "\n",
    "X = torch.randn(B, 16, device=device)\n",
    "T = torch.randn(B, 4, device=device)\n",
    "\n",
    "# --- Big batch gradients ---\n",
    "opt_big = torch.optim.SGD(model_big.parameters(), lr=0.1)\n",
    "opt_big.zero_grad(set_to_none=True)\n",
    "loss_big = loss_fn(model_big(X), T)\n",
    "loss_big.backward()\n",
    "grads_big = [p.grad.detach().clone() for p in model_big.parameters()]\n",
    "\n",
    "# --- Accumulation gradients ---\n",
    "opt_acc = torch.optim.SGD(model_acc.parameters(), lr=0.1)\n",
    "opt_acc.zero_grad(set_to_none=True)\n",
    "for i in range(K):\n",
    "    xb = X[i*b:(i+1)*b]\n",
    "    tb = T[i*b:(i+1)*b]\n",
    "    loss = loss_fn(model_acc(xb), tb) / K\n",
    "    loss.backward()\n",
    "\n",
    "grads_acc = [p.grad.detach().clone() for p in model_acc.parameters()]\n",
    "\n",
    "# Compare\n",
    "max_abs = 0.0\n",
    "for g1, g2 in zip(grads_big, grads_acc):\n",
    "    max_abs = max(max_abs, (g1 - g2).abs().max().item())\n",
    "\n",
    "print(\"loss_big:\", float(loss_big.detach().cpu().item()))\n",
    "print(\"max |grad_big - grad_acc|:\", max_abs)\n",
    "print(\"Expected: ~0 (numerical noise only).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f874aa",
   "metadata": {},
   "source": [
    "### 5.5 Gotchas (short but sharp)\n",
    "\n",
    "- **BatchNorm**: accumulation does *not* make BN see the full big batch; BN stats are computed per microbatch.\n",
    "- **Dropout / randomness**: big-batch vs microbatch equivalence assumes deterministic forward; with dropout, use a fixed RNG strategy if you need strict equivalence.\n",
    "- **AMP / GradScaler**: typically you scale each micro-loss (already divided by \\(K\\)), call `scaler.scale(loss).backward()`, and only `scaler.step(optimizer)` once per accumulation cycle.\n",
    "- **Gradient clipping**: clip *after* accumulation (i.e., right before stepping).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a3996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 (Optional) AMP + accumulation sketch (runs only if CUDA)\n",
    "\n",
    "from contextlib import nullcontext\n",
    "\n",
    "def amp_accumulation_sketch(model, optimizer, X, T, K: int):\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "    autocast = torch.cuda.amp.autocast if torch.cuda.is_available() else nullcontext\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    B = X.shape[0]\n",
    "    b = B // K\n",
    "\n",
    "    for i in range(K):\n",
    "        xb = X[i*b:(i+1)*b]\n",
    "        tb = T[i*b:(i+1)*b]\n",
    "        with autocast():\n",
    "            loss = loss_fn(model(xb), tb) / K\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    # unscale before clipping (if you clip)\n",
    "    scaler.unscale_(optimizer)\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "print(\"This cell defines a sketch; it doesn't run a full training loop.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59f088f",
   "metadata": {},
   "source": [
    "## 6 — Activation / gradient checkpointing (the activation-memory lever)\n",
    "\n",
    "### 6.1 Why activations dominate\n",
    "\n",
    "Autograd needs intermediate tensors from the forward pass to compute gradients in the backward pass. Those saved activations can dominate memory, especially for:\n",
    "- deep nets\n",
    "- large batch sizes\n",
    "- long sequences (Transformers)\n",
    "- large feature maps (vision)\n",
    "\n",
    "### 6.2 What checkpointing does\n",
    "\n",
    "Activation checkpointing trades **compute for memory**:\n",
    "- forward pass in checkpointed region does **not** save intermediates\n",
    "- backward pass **recomputes** the forward for that region to recover needed activations\n",
    "\n",
    "### 6.3 PyTorch API reality (important)\n",
    "\n",
    "`torch.utils.checkpoint.checkpoint(function, *args, use_reentrant=...)` has **two implementations**:\n",
    "- `use_reentrant=True` (older “reentrant autograd” variant)\n",
    "- `use_reentrant=False` (newer non-reentrant variant)\n",
    "\n",
    "PyTorch docs recommend **`use_reentrant=False`** and warn you should pass it explicitly; newer versions will raise if you don’t.\n",
    "\n",
    "We’ll run a peak-memory + runtime comparison (if CUDA is available).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d6f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Measurement experiment: checkpointing vs no checkpointing\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, d=2048, depth=12):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(depth):\n",
    "            layers.append(nn.Linear(d, d))\n",
    "            layers.append(nn.GELU())\n",
    "        self.seq = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "def forward_backward_step(model: nn.Module, x: torch.Tensor, use_ckpt: bool):\n",
    "    model.train()\n",
    "    # Simple scalar loss\n",
    "    if use_ckpt:\n",
    "        # Wrap the sequential as a function. Note: checkpoint expects a function, not a Module.\n",
    "        def fn(inp):\n",
    "            return model.seq(inp)\n",
    "        out = checkpoint(fn, x, use_reentrant=False)\n",
    "    else:\n",
    "        out = model(x)\n",
    "    loss = out.pow(2).mean()\n",
    "    loss.backward()\n",
    "    return float(loss.detach().cpu().item())\n",
    "\n",
    "# Try to keep this runnable on both CPU and GPU.\n",
    "d = 1024 if not torch.cuda.is_available() else 2048\n",
    "depth = 10 if not torch.cuda.is_available() else 14\n",
    "\n",
    "model_plain = DeepMLP(d=d, depth=depth).to(device)\n",
    "model_ckpt = DeepMLP(d=d, depth=depth).to(device)\n",
    "model_ckpt.load_state_dict(model_plain.state_dict())\n",
    "\n",
    "x_big = torch.randn(8 if torch.cuda.is_available() else 2, d, device=device, requires_grad=True)\n",
    "\n",
    "# Warmup\n",
    "for _ in range(2):\n",
    "    model_plain.zero_grad(set_to_none=True)\n",
    "    loss = forward_backward_step(model_plain, x_big, use_ckpt=False)\n",
    "    model_plain.zero_grad(set_to_none=True)\n",
    "\n",
    "def run_plain():\n",
    "    model_plain.zero_grad(set_to_none=True)\n",
    "    forward_backward_step(model_plain, x_big, use_ckpt=False)\n",
    "\n",
    "def run_ckpt():\n",
    "    model_ckpt.zero_grad(set_to_none=True)\n",
    "    forward_backward_step(model_ckpt, x_big, use_ckpt=True)\n",
    "\n",
    "# Measure time + peak memory\n",
    "def timed(fn, n=5):\n",
    "    t0 = time.time()\n",
    "    for _ in range(n):\n",
    "        fn()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "    return (time.time() - t0) / n\n",
    "\n",
    "t_plain = timed(run_plain, n=5)\n",
    "t_ckpt = timed(run_ckpt, n=5)\n",
    "\n",
    "peak_plain = measure_peak_cuda(run_plain)\n",
    "peak_ckpt = measure_peak_cuda(run_ckpt)\n",
    "\n",
    "print(\"avg step time (plain):\", t_plain, \"s\")\n",
    "print(\"avg step time (ckpt): \", t_ckpt, \"s\")\n",
    "\n",
    "if peak_plain is not None:\n",
    "    print(\"peak CUDA (plain):\", pretty_bytes(peak_plain))\n",
    "    print(\"peak CUDA (ckpt): \", pretty_bytes(peak_ckpt))\n",
    "    print(\"Δpeak (plain-ckpt):\", pretty_bytes(peak_plain - peak_ckpt))\n",
    "else:\n",
    "    print(\"CUDA not available; memory comparison skipped. On GPU you should see lower peak with checkpointing, but slower time.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38e428a",
   "metadata": {},
   "source": [
    "### 6.5 Determinism and RNG gotcha\n",
    "\n",
    "Checkpointing may re-run forward segments during backward, which can advance RNG state differently than a non-checkpointed run.\n",
    "\n",
    "PyTorch’s checkpoint implementation stashes/restores RNG state by default (`preserve_rng_state=True`) to make dropout outputs match non-checkpointed behavior, but that can add overhead. If you don’t need that equivalence, you can set `preserve_rng_state=False` (with careful thought).\n",
    "\n",
    "Also note: if you move tensors across devices inside the checkpointed function, RNG-state juggling may not do what you expect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc90dab3",
   "metadata": {},
   "source": [
    "## 7 — Synthesis: one-page cheat sheet + meeting talk tracks\n",
    "\n",
    "### 7.1 One-page cheat sheet (memorize-ready)\n",
    "\n",
    "Below is the “state machine summary.” Print this. Tape it to your forehead (optional).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae2b5d",
   "metadata": {},
   "source": [
    "### Update rules (conceptual)\n",
    "\n",
    "Let \\(g = \\nabla_\\theta L(\\theta)\\), learning rate \\(\\gamma\\), weight decay \\(\\lambda\\).\n",
    "\n",
    "**SGD (PyTorch semantics)**  \n",
    "Optional L2 penalty:\n",
    "\\[\n",
    "g \\leftarrow g + \\lambda \\theta\n",
    "\\]\n",
    "Then:\n",
    "\\[\n",
    "\\theta \\leftarrow \\theta - \\gamma g\n",
    "\\]\n",
    "Momentum adds a buffer \\(b\\) (details in PyTorch docs).\n",
    "\n",
    "**Adam (L2-style `weight_decay` by default)**  \n",
    "Maintains \\(m\\) and \\(v\\), does:\n",
    "- optional L2: \\(g \\leftarrow g + \\lambda \\theta\\)\n",
    "- update \\(m, v\\)\n",
    "- \\(\\theta \\leftarrow \\theta - \\gamma \\hat m / (\\sqrt{\\hat v} + \\epsilon)\\)\n",
    "\n",
    "**AdamW (decoupled)**  \n",
    "First:\n",
    "\\[\n",
    "\\theta \\leftarrow \\theta - \\gamma \\lambda \\theta\n",
    "\\]\n",
    "Then the Adam normalized gradient step (without mixing decay into \\(m,v\\)).\n",
    "\n",
    "### Memory multipliers (persistent state, in units of P scalars)\n",
    "\n",
    "Ignoring activations:\n",
    "\n",
    "| Optimizer | Extra optimizer state | Total persistent (params+grads+state) |\n",
    "|---|---:|---:|\n",
    "| SGD (no momentum) | ~0P | ~2P |\n",
    "| SGD + momentum | ~1P | ~3P |\n",
    "| Adam / AdamW | ~2P | ~4P |\n",
    "| Adam/AdamW + AMSGrad | ~3P | ~5P |\n",
    "\n",
    "> Peak memory can differ from this due to activations and temporary optimizer intermediates (e.g., `foreach`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cda1bc",
   "metadata": {},
   "source": [
    "### 7.2 “Explain in 90 seconds” scripts\n",
    "\n",
    "**SGD vs SGD+momentum**  \n",
    "SGD stores just parameters and their gradients. With momentum, it also stores a per-parameter velocity/momentum buffer. That buffer costs ~1× parameter memory. It often improves optimization by smoothing noisy gradients and accelerating consistent directions, but the model parameter count is unchanged — only optimizer state grows.\n",
    "\n",
    "**Adam vs SGD**  \n",
    "Adam adds two EMAs per parameter (first and second moment). That’s +2× parameter memory in optimizer state. It adapts step sizes per parameter based on historical gradient magnitudes, often converging faster with less hyperparameter tuning, but it’s heavier in memory and sometimes needs careful regularization for good generalization.\n",
    "\n",
    "**Adam vs AdamW**  \n",
    "In Adam (default), `weight_decay` behaves like an L2 penalty injected into the gradient pipeline; in adaptive methods, this makes the effective decay depend on the adaptive normalization. AdamW decouples weight decay, shrinking weights directly without contaminating moment estimates. This difference is easy to see in a scalar test.\n",
    "\n",
    "**Gradient accumulation**  \n",
    "It simulates a larger batch by summing/averaging gradients across microbatches and stepping once. If you divide each micro-loss by K, you match the large-batch gradient (for deterministic models). It’s not identical when layers depend on batch statistics (BatchNorm) or randomness (dropout) unless you manage those effects.\n",
    "\n",
    "**Checkpointing**  \n",
    "It reduces activation memory by not saving intermediates in some forward regions. Backward recomputes those regions, so it costs extra compute but can unlock larger models/batches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cedc6",
   "metadata": {},
   "source": [
    "### 7.3 “Mentor might ask” readiness drills (do these without a laptop)\n",
    "\n",
    "- If a model has \\(P\\) parameters in fp32, what’s the optimizer-state memory for:\n",
    "  - SGD vs SGD+momentum vs AdamW?\n",
    "- Show in 1D why AdamW decay differs from Adam’s L2-style decay.\n",
    "- Does gradient accumulation perfectly simulate a large batch? If not, what breaks equivalence?\n",
    "- Why does checkpointing help activations but not optimizer state?\n",
    "\n",
    "> If you can answer those quickly and precisely, you’re in a very good place for the meeting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679193e",
   "metadata": {},
   "source": [
    "## Appendix A — Quick memory calculator\n",
    "\n",
    "The helper below gives you a *back-of-the-envelope* memory estimate from \\(P\\), dtype bytes, and optimizer choice.  \n",
    "(It ignores activation memory, which is often dominant.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8860dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appendix A: simple memory estimator\n",
    "\n",
    "@dataclass\n",
    "class MemoryEstimate:\n",
    "    P: int\n",
    "    bytes_per_scalar: int\n",
    "    params: int\n",
    "    grads: int\n",
    "    state: int\n",
    "    total: int\n",
    "\n",
    "def estimate_persistent_memory(P: int, bytes_per_scalar: int, state_multiplier: int) -> MemoryEstimate:\n",
    "    params = P * bytes_per_scalar\n",
    "    grads = P * bytes_per_scalar\n",
    "    state = state_multiplier * P * bytes_per_scalar\n",
    "    total = params + grads + state\n",
    "    return MemoryEstimate(P, bytes_per_scalar, params, grads, state, total)\n",
    "\n",
    "def print_est(name: str, est: MemoryEstimate):\n",
    "    print(f\"{name}:\")\n",
    "    print(\"  params:\", pretty_bytes(est.params))\n",
    "    print(\"  grads :\", pretty_bytes(est.grads))\n",
    "    print(\"  state :\", pretty_bytes(est.state))\n",
    "    print(\"  total :\", pretty_bytes(est.total))\n",
    "    print()\n",
    "\n",
    "# Example: 1 billion params in bf16 (2 bytes) with AdamW state stored in bf16 (best-case)\n",
    "P = 1_000_000_000\n",
    "b = 2\n",
    "\n",
    "print(\"Assuming optimizer state uses same dtype bytes as params (best-case; many fused/AMP setups use fp32 state).\")\n",
    "print_est(\"SGD\", estimate_persistent_memory(P, b, state_multiplier=0))\n",
    "print_est(\"SGD+momentum\", estimate_persistent_memory(P, b, state_multiplier=1))\n",
    "print_est(\"AdamW\", estimate_persistent_memory(P, b, state_multiplier=2))\n",
    "\n",
    "print(\"If state is fp32 while params are bf16, state bytes double relative to params.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e94e0",
   "metadata": {},
   "source": [
    "## Appendix B — Checklist: what to rehearse before the meeting\n",
    "\n",
    "- You can explain: “optimizer state vs trainable parameters” cleanly.\n",
    "- You can derive: SGD L2 ⇔ weight decay equivalence.\n",
    "- You can explain: why Adam breaks that equivalence and why AdamW decouples.\n",
    "- You can code (from memory): correct gradient accumulation loop (loss/K + step every K).\n",
    "- You can explain: checkpointing saves activations, not optimizer state — and why it costs extra compute.\n",
    "\n",
    "That’s the whole game. Define the state, write the update, compute the memory, then verify with a tiny experiment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
