{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161a0f96",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "There is a GD expression that we have all seen. And we know how backprop works with the forward, backward, and update rule. We also know how to imagine the landscape of loss where each weight is plotted on an axis with loss on the other axis and our goal is to get to the lowest loss using that weight. \n",
    "\n",
    "As we trudge along this landscape of loss, we realize that we are doing the most fundamental part of deep learning. If we think of llms as a giant mathematical expression, then the main objective of training is to tune the constants in that expression (weights). To move the weights such that the resulting expression is close enough to the data manifold used during training is the main objective of training. And in that sense, deep learning is essentially an optimization problem. \n",
    "\n",
    "And then if moving across this landscape and finding that elusive global minima in this fractured landscape is the goal, we start seeing that the key starts becoming exactly *how* we move through it. \n",
    "\n",
    "In this notebook, that is what we will cover. And for me, that is also the *hook* for optimizers. Because while it can get deeply mathematical and the explanations can get quite abstract, the truth is that at the end of the day, all we're doing is writing an algorithm to walk down a hill."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cbf5ae",
   "metadata": {},
   "source": [
    "# Things to know"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3e634",
   "metadata": {},
   "source": [
    "## Weight Update Formula\n",
    "\n",
    "Let's review the most basic/fundamental weight update formula as done through vanilla SGD. This disucssion is a bit off topic, but I went through this rabbit hole for a while so just noting it down here. Also, only writing down the high level points, deeper analysis will lead to the question of what it means to stabilize the weight update equation, and by corollary, training itself - which is a larger question and won't be analyzed here. Idea here is just to think of the intuition. \n",
    "\n",
    "Here is the equation:\n",
    "\n",
    "W_new = W_old - lr * L'(W_old)\n",
    "\n",
    "There is something about this equation that feels a bit wrong. \n",
    "\n",
    "For example, at the most funadmental level, are the units in all the terms the same? Thinking from a physics perspective, if we strictly assign physical units, W_old is a position (e.g., \"meters\") and the gradient grad(f'(W_old)) is a slope (\"energy per meter\"). Subtracting a slope from a position is just physically impossible~ This reveals that the gradient is merely a directional signal (force) indicating urgency, not a spatial displacement. The equation requires a conversion factor to translate \"steepness of the hill\" into \"distance to step.\"\n",
    "\n",
    "Perhaps the resolution to this paradox lies in the Taylor Series expansion of the loss function. The \"perfect\" update rule, which accounts for the landscape's geometry, is:\n",
    "\n",
    "W_new = W_old - lr * ( L'(W_old) / L''(W_old) )\n",
    "\n",
    "Here, the numerator (gradient, $L'$) provides the push, while the denominator (curvature/Hessian, $L''$) provides the braking mechanism. Dimensionally, it can be argued that this balances because: the units of curvature (${energy}/{{meters}^2}$) divide the units of gradient (${energy}/{{meters}}$), canceling out the \"energy\" and leaving purely \"meters.\" So it can be argued that this confirms that a valid update step requires two distinct derivatives: one to determine direction ($L'$) and one to determine scale ($L''$).\n",
    "\n",
    "Of course, calculating the Hessian ($L''$) is computationally intractable for millions of parameters. Therefore, the learning rate $lr$ acts as a constant scalar proxy for the inverse curvature (1/{L''}$). When we set a learning rate, we are effectively guessing the geometry of the error surface. A high $lr$ assumes low curvature (a wide, flat valley where big steps are safe), while a low $lr$ assumes high curvature (a sharp, narrow ravine where precision is required). Thus, $lr$, in addition to being a speed setting, is also a unit-restoring term that bridges the gap between the \"force\" of the gradient and the \"displacement\" of the weight update.\n",
    "\n",
    "Modern optimization strategies seem to implicitly acknowledge this relationship. Learning rate schedulers (decaying $lr$ over time) mimic the assumption that the loss landscape transitions from a broad basin (low curvature) to a narrow minimum (high curvature) as training progresses. More advanced optimizers like Adam take this a step further by dividing the gradient by a rolling average of squared gradients. This term ($\\sqrt{v_t}$) serves as a computationally cheap estimation of the local curvature, attempting to replicate the unit-correcting behavior of Newtonâ€™s method by normalizing the step size for each parameter individually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648dfa7a",
   "metadata": {},
   "source": [
    "## Exponentially Weighted Moving Average\n",
    "\n",
    "EWMA (exponentially weighted moving average) is the \"smooth a noisy stream without forgetting the present\" trick. You take a time series (daily temperature, stock price, whatever), and instead of a dumb uniform mean that treats last week and last year the same, you keep a running average that leans harder on recent values while older values fade out exponentially. Visually it's that clean black curve that hugs the data enough to track the trend but not enough to chase every random zig-zag. This exact \"trend extraction\" idea shows up all over: time series / financial forecasting, signal processing (it's basically a first-order low-pass filter), and in deep learning it's the core primitive behind momentum-style optimizers.\n",
    "\n",
    "The entire method is one recurrence:\n",
    "\n",
    "$$v_t = \\beta v_{t-1} + (1 - \\beta) x_t$$\n",
    "\n",
    "Here $x_t$ is the value at time $t$ (temperature, gradient, whatever), and $v_t$ is the EWMA at time $t$. $\\beta \\in [0, 1)$ controls memory. If $\\beta$ is large, you heavily trust the past state $v_{t-1}$, so the curve is stable and smooth. If $\\beta$ is small, you aggressively trust the current observation $x_t$, so the curve becomes twitchy and tracks the data closely. Two practical notes: you need some $v_0$ to start the recurrence. People often set $v_0 = 0$, or set $v_0 = x_0$ (or some reasonable constant). Early on, initialization matters because the filter \"warms up\" from that starting point.\n",
    "\n",
    "A tiny numerical walk-through makes it real. Say $\\beta = 0.9$, $v_0 = 0$, and your first observation is $x_1 = 30$. Then $v_1 = 0.9 \\cdot 0 + 0.1 \\cdot 30 = 3$. Next if $x_2 = 17$, then $v_2 = 0.9 \\cdot 3 + 0.1 \\cdot 17 = 4.4$. And so on. Every step is \"keep most of the previous smoothed value, mix in a little bit of the new measurement.\" Plotting $v_t$ against time gives you a trend line that reacts gradually instead of instantly.\n",
    "\n",
    "There's a really useful intuition for what $\\beta$ means: EWMA behaves kind of like an average over the last\n",
    "\n",
    "$$\\frac{1}{1 - \\beta}$$\n",
    "\n",
    "points (an \"effective window length\"). If $\\beta = 0.9$, that's about $1/0.1 = 10$ steps of memory. If $\\beta = 0.5$, that's $1/0.5 = 2$ steps, meaning you're basically averaging only the last couple points and your estimate will whip around. Same data, different $\\beta$, wildly different behavior: high $\\beta$ gives you a slow, stable, low-variance curve; low $\\beta$ gives you a moody curve that updates its \"belief\" based almost entirely on what just happened.\n",
    "\n",
    "If you want the \"why does it weight recent points more?\" proof, just expand the recurrence a few steps. Substitute $v_{t-1}$ into $v_t$, then substitute again, and you'll see the pattern:\n",
    "\n",
    "$$v_t = (1 - \\beta)(x_t + \\beta x_{t-1} + \\beta^2 x_{t-2} + \\cdots + \\beta^{t-1} x_1) + \\beta^t v_0$$\n",
    "\n",
    "So the contribution of $x_{t-k}$ is $(1 - \\beta)\\beta^k$. Since $\\beta < 1$, those weights decay exponentially as you go back in time. That is literally the mechanism behind the two key properties: (1) newer points matter more, (2) any fixed old point's influence shrinks as time goes on.\n",
    "\n",
    "Now connect this to deep learning optimizers: replace \"temperature\" with \"gradient\". Gradients are noisy minibatch estimates, so you don't want your update direction to flip around just because one batch was weird. Momentum and friends maintain an EWMA of gradients (or squared gradients), which is just this same filter applied to a different signal. High $\\beta$ means \"trust the long-term direction, smooth aggressively,\" low $\\beta$ means \"react quickly to new gradient information.\" The sweet spot is task-dependent, but in practice you see $\\beta \\approx 0.9$ all the time because it gives a nice stability/response tradeoff.\n",
    "\n",
    "Implementation-wise, Python makes this boring (in a good way). In pandas you can use `ewm` to compute the exponential moving average over a column. Pandas often parameterizes the recurrence with alpha instead of $\\beta$, where\n",
    "\n",
    "$$\\alpha = 1 - \\beta$$\n",
    "\n",
    "Same thing, just a different knob. So if someone says \"alpha = 0.1\", that corresponds to $\\beta = 0.9$ (slow/stable). If they say \"alpha = 0.9\", that corresponds to $\\beta = 0.1$ (fast/moody). Typical workflow: load time series (date, value), compute `ema = df['value'].ewm(alpha=alpha).mean()`, attach it as a new column, plot raw series plus EMA with matplotlib. The best exercise is to vary $\\alpha$ / $\\beta$ and actually feel how the curve transitions from smooth trend extractor to near-copy of the raw dataâ€”and then write the recurrence yourself in a loop once, so you internalize that it's just one line of state update repeated forever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104ca402",
   "metadata": {},
   "source": [
    "# SGD with momentum\n",
    "\n",
    "SGD with momentum lives inside one big picture: youâ€™re not â€œoptimizing weights,â€ youâ€™re navigating a loss landscape. You feed input â†’ get prediction \n",
    "ð‘¦\n",
    "^\n",
    "y\n",
    "^\n",
    "\tâ€‹\n",
    "\n",
    " â†’ compare to target \n",
    "ð‘¦\n",
    "y with some loss like mean squared error\n",
    "\n",
    "ð¿\n",
    "(\n",
    "ð‘¦\n",
    ",\n",
    "ð‘¦\n",
    "^\n",
    ")\n",
    "=\n",
    "(\n",
    "ð‘¦\n",
    "âˆ’\n",
    "ð‘¦\n",
    "^\n",
    ")\n",
    "2\n",
    ",\n",
    "L(y,\n",
    "y\n",
    "^\n",
    "\tâ€‹\n",
    "\n",
    ")=(yâˆ’\n",
    "y\n",
    "^\n",
    "\tâ€‹\n",
    "\n",
    ")\n",
    "2\n",
    ",\n",
    "\n",
    "and because \n",
    "ð‘¦\n",
    "^\n",
    "y\n",
    "^\n",
    "\tâ€‹\n",
    "\n",
    " depends on parameters \n",
    "ðœƒ\n",
    "Î¸ (weights and biases), your loss is ultimately \n",
    "ð¿\n",
    "(\n",
    "ðœƒ\n",
    ")\n",
    "L(Î¸). In toy land, if \n",
    "ðœƒ\n",
    "Î¸ is a single weight \n",
    "ð‘¤\n",
    "w, you can plot \n",
    "ð¿\n",
    "(\n",
    "ð‘¤\n",
    ")\n",
    "L(w) as a 2D curve. If \n",
    "ðœƒ\n",
    "=\n",
    "(\n",
    "ð‘¤\n",
    ",\n",
    "ð‘\n",
    ")\n",
    "Î¸=(w,b), you can plot \n",
    "ð¿\n",
    "(\n",
    "ð‘¤\n",
    ",\n",
    "ð‘\n",
    ")\n",
    "L(w,b) as a 3D surface. In real nets \n",
    "ðœƒ\n",
    "âˆˆ\n",
    "ð‘…\n",
    "ð‘\n",
    "Î¸âˆˆR\n",
    "N\n",
    " with \n",
    "ð‘\n",
    "N in the millions, so that surface exists but your human brain canâ€™t â€œlook at it,â€ so we constantly fall back to 1D/2D slices and their shadows.\n",
    "\n",
    "Thatâ€™s where contour plots are secretly doing a ton of work. Take a 3D surface \n",
    "ð¿\n",
    "(\n",
    "ð‘¤\n",
    ",\n",
    "ð‘\n",
    ")\n",
    "L(w,b), look at it from the top, and draw curves where the loss is constantâ€”those are level sets. Thatâ€™s the contour plot: a 2D projection of a 3D surface. You lost one dimension (height), so you encode it with color. The geometry is the key: where contour lines are close together, the slope magnitude is large (steep region). Where the lines are far apart, the surface is flat-ish (small gradient over a big region). Saddle points show up as that weird â€œup in one direction, down in the otherâ€ structureâ€”often with large flat-ish neighborhoods where gradients are tiny and progress crawls. Local minima show up as â€œbasinsâ€ that look like closed loops around a dip. You can mentally reverse-engineer the 3D surface from the 2D contours: tight rings = steep walls, spaced rings = gentle bowl, twisted pattern = saddle.\n",
    "\n",
    "Now convex vs non-convex: convex is the friendly universe where thereâ€™s one basin and every downhill path leads to the same global minimum. Non-convex is the deep learning universe: multiple basins, flats, ravines, saddles, weird curvature. The â€œwhy is optimization hard?â€ list in practice is brutally simple:\n",
    "\n",
    "Local minima: you can fall into a small dip and stop improving, even though thereâ€™s a deeper basin elsewhere (global minimum).\n",
    "\n",
    "Saddle points / plateaus: gradients are tiny across a large region, so you move at a glacial pace (even if youâ€™re not â€œstuckâ€ in a strict minimum).\n",
    "\n",
    "High curvature ravines: one direction is steep (large curvature), another direction is shallow (small curvature), so vanilla updates bounce side-to-side while making slow forward progress.\n",
    "\n",
    "Plus the â€œSGD reality taxâ€: noisy gradients (because minibatches are stochastic estimates), and inconsistent gradients (directions can vary across steps due to noise + curvature + minibatch sampling).\n",
    "\n",
    "Before momentum, the baseline update is: â€œstep opposite the gradient.â€\n",
    "For parameters \n",
    "ðœƒ\n",
    "Î¸,\n",
    "\n",
    "ðœƒ\n",
    "ð‘¡\n",
    "+\n",
    "1\n",
    "=\n",
    "ðœƒ\n",
    "ð‘¡\n",
    "âˆ’\n",
    "ð›¼\n",
    "âˆ‡\n",
    "ðœƒ\n",
    "ð¿\n",
    "(\n",
    "ðœƒ\n",
    "ð‘¡\n",
    ")\n",
    ",\n",
    "Î¸\n",
    "t+1\n",
    "\tâ€‹\n",
    "\n",
    "=Î¸\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    "âˆ’Î±âˆ‡\n",
    "Î¸\n",
    "\tâ€‹\n",
    "\n",
    "L(Î¸\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    "),\n",
    "\n",
    "where \n",
    "ð›¼\n",
    "Î± is the learning rate. If you compute \n",
    "âˆ‡\n",
    "ð¿\n",
    "âˆ‡L over the full dataset, thatâ€™s batch gradient descent: smooth, stable, often slow per step. If you compute it using one example at a time, thatâ€™s stochastic gradient descent (SGD): cheap per step, but the path is jagged because the gradient estimate is noisy. If you compute it over a small batch, thatâ€™s mini-batch GD (what people usually mean in deep learning): a practical middle groundâ€”faster iteration than batch, less chaos than pure SGD.\n",
    "\n",
    "So where does momentum enter? Momentum is basically: â€œdonâ€™t treat each gradient like itâ€™s a brand-new opinion; treat gradients like noisy measurements of a trend.â€ If the last several gradients have been pointing roughly the same way, you should build confidence and move faster in that direction. If gradients disagree (noise, oscillations across a ravine), you should damp the indecision.\n",
    "\n",
    "There are two mental models that land well:\n",
    "\n",
    "Crowd directions model: youâ€™re trying to reach point B in a city. You ask one person and they point eastâ€”maybe, maybe not. You ask four people and they all point eastâ€”now you commit and walk faster. If the crowd is split (two say east, two say west), you still move, but cautiously. Momentum is the â€œask multiple past gradients, form a consensus direction.â€\n",
    "\n",
    "Physics model: youâ€™re a ball rolling down a landscape. A ball doesnâ€™t teleport to â€œthe steepest directionâ€ and forget its previous motion every millisecond. It has velocity; it carries inertia. Momentum optimization explicitly introduces a velocity-like state. In Newtonian terms, momentum is \n",
    "ð‘\n",
    "=\n",
    "ð‘š\n",
    "ð‘£\n",
    "p=mv. We donâ€™t really have a meaningful mass here, so you can pretend \n",
    "ð‘š\n",
    "=\n",
    "1\n",
    "m=1 and focus on â€œvelocity.â€\n",
    "\n",
    "Mathematically, SGD with momentum keeps a velocity vector \n",
    "ð‘£\n",
    "ð‘¡\n",
    "v\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    " (same shape as \n",
    "ðœƒ\n",
    "Î¸). Replace the raw gradient step with an exponentially weighted moving average of past gradients (EMA), and use that to update parameters:\n",
    "\n",
    "ð‘£\n",
    "ð‘¡\n",
    "=\n",
    "ð›½\n",
    "ð‘£\n",
    "ð‘¡\n",
    "âˆ’\n",
    "1\n",
    "+\n",
    "ð›¼\n",
    "â€‰\n",
    "ð‘”\n",
    "ð‘¡\n",
    "v\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    "=Î²v\n",
    "tâˆ’1\n",
    "\tâ€‹\n",
    "\n",
    "+Î±g\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    "ðœƒ\n",
    "ð‘¡\n",
    "+\n",
    "1\n",
    "=\n",
    "ðœƒ\n",
    "ð‘¡\n",
    "âˆ’\n",
    "ð‘£\n",
    "ð‘¡\n",
    "Î¸\n",
    "t+1\n",
    "\tâ€‹\n",
    "\n",
    "=Î¸\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    "âˆ’v\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    "\n",
    "where \n",
    "ð‘”\n",
    "ð‘¡\n",
    "=\n",
    "âˆ‡\n",
    "ðœƒ\n",
    "ð¿\n",
    "(\n",
    "ðœƒ\n",
    "ð‘¡\n",
    ")\n",
    "g\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    "=âˆ‡\n",
    "Î¸\n",
    "\tâ€‹\n",
    "\n",
    "L(Î¸\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    ") (usually computed on a mini-batch), \n",
    "ð›¼\n",
    "Î± is learning rate, and \n",
    "ð›½\n",
    "âˆˆ\n",
    "[\n",
    "0\n",
    ",\n",
    "1\n",
    ")\n",
    "Î²âˆˆ[0,1) is the momentum coefficient (commonly \n",
    "ð›½\n",
    "=\n",
    "0.9\n",
    "Î²=0.9).\n",
    "\n",
    "Two important notes:\n",
    "\n",
    "This is literally the exponential moving average idea applied to gradients (or â€œupdate directionsâ€).\n",
    "\n",
    "If you unroll the recurrence, you see the EMA explicitly:\n",
    "\n",
    "ð‘£\n",
    "ð‘¡\n",
    "=\n",
    "ð›¼\n",
    "(\n",
    "ð‘”\n",
    "ð‘¡\n",
    "+\n",
    "ð›½\n",
    "ð‘”\n",
    "ð‘¡\n",
    "âˆ’\n",
    "1\n",
    "+\n",
    "ð›½\n",
    "2\n",
    "ð‘”\n",
    "ð‘¡\n",
    "âˆ’\n",
    "2\n",
    "+\n",
    "â‹¯\n",
    "â€‰\n",
    ")\n",
    "v\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    "=Î±(g\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    "+Î²g\n",
    "tâˆ’1\n",
    "\tâ€‹\n",
    "\n",
    "+Î²\n",
    "2\n",
    "g\n",
    "tâˆ’2\n",
    "\tâ€‹\n",
    "\n",
    "+â‹¯)\n",
    "\n",
    "So the current velocity is a weighted sum of recent gradients, with weights decaying exponentially into the past. Recent gradients matter more; ancient gradients fade out.\n",
    "\n",
    "The parameter \n",
    "ð›½\n",
    "Î² is the â€œmemory / decay factor.â€ It sets how long the optimizer remembers the past. A useful rule of thumb: the effective averaging window length is about\n",
    "\n",
    "window\n",
    "â‰ˆ\n",
    "1\n",
    "1\n",
    "âˆ’\n",
    "ð›½\n",
    ".\n",
    "windowâ‰ˆ\n",
    "1âˆ’Î²\n",
    "1\n",
    "\tâ€‹\n",
    "\n",
    ".\n",
    "\n",
    "So \n",
    "ð›½\n",
    "=\n",
    "0.9\n",
    "Î²=0.9 remembers roughly \n",
    "âˆ¼\n",
    "10\n",
    "âˆ¼10 steps, \n",
    "ð›½\n",
    "=\n",
    "0.99\n",
    "Î²=0.99 remembers \n",
    "âˆ¼\n",
    "100\n",
    "âˆ¼100 steps, etc. Bigger \n",
    "ð›½\n",
    "Î² = smoother, more inertia, more â€œcommitment.â€\n",
    "\n",
    "Edge cases explain the whole design:\n",
    "\n",
    "If \n",
    "ð›½\n",
    "=\n",
    "0\n",
    "Î²=0:\n",
    "\n",
    "ð‘£\n",
    "ð‘¡\n",
    "=\n",
    "ð›¼\n",
    "ð‘”\n",
    "ð‘¡\n",
    ",\n",
    "ðœƒ\n",
    "ð‘¡\n",
    "+\n",
    "1\n",
    "=\n",
    "ðœƒ\n",
    "ð‘¡\n",
    "âˆ’\n",
    "ð›¼\n",
    "ð‘”\n",
    "ð‘¡\n",
    "v\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    "=Î±g\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    ",Î¸\n",
    "t+1\n",
    "\tâ€‹\n",
    "\n",
    "=Î¸\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    "âˆ’Î±g\n",
    "t\n",
    "\tâ€‹\n",
    "\n",
    "\n",
    "Thatâ€™s just vanilla SGD. Momentum collapses to â€œno momentum.â€\n",
    "\n",
    "If \n",
    "ð›½\n",
    "â†’\n",
    "1\n",
    "Î²â†’1: you stop forgetting. Velocity becomes a long-running accumulator. This can create persistent oscillations / a kind of dynamic equilibrium where you donâ€™t settle nicely, because the system carries too much inertia and not enough damping.\n",
    "\n",
    "Now the core behavioral payoff. Picture the classic deep learning ravine: steep curvature in one direction (say vertical), shallow slope in the other (say horizontal). Vanilla SGD tends to bounce: it overshoots across the steep direction, flips gradient sign, overshoots back, repeat. You get a zig-zag trajectory that wastes steps oscillating â€œup and downâ€ while inching forward along the shallow direction. Momentum acts like a low-pass filter: the oscillatory component cancels out over time (because it keeps switching sign), while the consistent component along the shallow direction accumulates. Net effect: less vertical oscillation, more horizontal progress, faster convergence.\n",
    "\n",
    "That same mechanism also helps with:\n",
    "\n",
    "Noisy gradients (especially small batch sizes): the noise is high-frequency randomness; EMA smooths it out.\n",
    "\n",
    "Inconsistent gradients: if the direction is unstable from step to step, momentum refuses to fully commit; it averages them and produces a more stable update direction.\n",
    "\n",
    "Saddle points / flat regions: gradients can be tiny for a long time. Momentum can â€œcarryâ€ you through these regions because velocity doesnâ€™t instantly drop to zero when the instantaneous gradient is small. You keep moving due to accumulated velocity.\n",
    "\n",
    "Local minima (the small annoying ones): inertia can help you roll out of a shallow basin. If the dip is small and your velocity is high enough, you donâ€™t get trappedâ€”you pass through and continue toward a better basin.\n",
    "\n",
    "And hereâ€™s the funny twist: momentumâ€™s superpower is also its most common failure mode. Because it builds velocity, it often overshoots the optimum and then has to correct. Near the minimum, the true gradient points back toward the basin center, but your velocity may still be blasting forward from earlier steps. So you fly past the bottom, climb the other side, turn around, fly past againâ€¦ and you get oscillations around the optimum. The exponential decay (\n",
    "ð›½\n",
    "<\n",
    "1\n",
    "Î²<1) acts as damping, so those oscillations usually shrink and you eventually settle, but you can waste time â€œringingâ€ around the minimum.\n",
    "\n",
    "On a contour plot, this looks exactly like what your intuition expects: momentum trajectories cut through the landscape aggressively, often overshooting the basin center and spiraling or oscillating before stabilizing. Plain SGD looks more like a jittery random walk that eventually drifts into the basin, often slower but with less dramatic overshoot. In interactive visualizers (contour plot + click-to-start-point), this contrast is almost comically visible: SGD is the anxious squirrel; momentum is the overconfident skateboarder.\n",
    "\n",
    "So the headline claimsâ€”when someone asks â€œwhy use momentum?â€â€”are basically three:\n",
    "\n",
    "Speed: it almost always reaches a good region faster than plain SGD, especially in ravines / high curvature terrain.\n",
    "\n",
    "Escaping shallow traps: it can roll through small local minima or tiny bumps because it has inertia.\n",
    "\n",
    "Stability under noise: it smooths noisy, stochastic gradients by averaging history.\n",
    "\n",
    "And the main caution label:\n",
    "\n",
    "Overshoot + oscillation near optima: momentum can waste steps bouncing around the minimum before damping out. Itâ€™s still typically faster than vanilla SGD overall, but this is exactly why later methods try to keep the â€œfastâ€ while fixing the â€œringing.â€\n",
    "\n",
    "The cleanest way to remember whatâ€™s happening is this: SGD reacts to the present; momentum reacts to the recent past plus the present. Itâ€™s a memory-equipped optimizer. That memory is an exponentially decayed history of gradients. Set \n",
    "ð›½\n",
    "Î² too low and youâ€™re basically back to SGD. Set it too high and you get a stubborn optimizer that refuses to slow down and can oscillate for longer. In the sweet spot (often \n",
    "ð›½\n",
    "â‰ˆ\n",
    "0.9\n",
    "Î²â‰ˆ0.9), you get the practical win: faster traversal across ugly non-convex terrain where gradients are noisy, curvature is weird, and the loss surface is doing its best impression of a crumpled bedsheet.\n",
    "\n",
    "If you want one sentence that isnâ€™t lying: SGD with momentum replaces â€œtake a step downhillâ€ with â€œmaintain a velocity thatâ€™s an EMA of downhill directions, then step according to that velocity.â€ Thatâ€™s it. Everything elseâ€”faster convergence in ravines, smoothing noise, escaping shallow minima, overshooting and oscillationsâ€”is just that sentence playing out in geometry."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
