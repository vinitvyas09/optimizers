{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48f6c90d",
   "metadata": {},
   "source": [
    "# **Why Momentum Really Works**\n",
    "\n",
    "Based on Gabriel Goh's Distill article: *Why Momentum Really Works* (Distill, 2017): https://distill.pub/2017/momentum/\n",
    "\n",
    "**Design constraint:** This notebook mirrors the article’s section headings.  \n",
    "Everything extra (derivations, prerequisites, experiments, self-tests) lives under `#### Add-on:` headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3afb57b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8037fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions used throughout the notebook\n",
    "\n",
    "def make_spd_quadratic(eigs, theta=0.0):\n",
    "    # Build a 2D symmetric positive definite matrix A with eigenvalues `eigs`,\n",
    "    # rotated by angle `theta`.\n",
    "    eigs = np.asarray(eigs, dtype=float)\n",
    "    assert eigs.shape == (2,)\n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    Q = np.array([[c, -s],\n",
    "                  [s,  c]])\n",
    "    A = Q @ np.diag(eigs) @ Q.T\n",
    "    return A, Q\n",
    "\n",
    "def quad_optimum(A, b):\n",
    "    # w* = A^{-1} b for f(w)=0.5 w^T A w - b^T w\n",
    "    return np.linalg.solve(A, b)\n",
    "\n",
    "def quad_loss(w, A, b):\n",
    "    # f(w)=0.5 w^T A w - b^T w\n",
    "    return 0.5 * (w @ (A @ w)) - (b @ w)\n",
    "\n",
    "def run_gd(A, b, w0, alpha, steps):\n",
    "    # Gradient descent: w_{k+1} = w_k - alpha (A w_k - b)\n",
    "    w = np.array(w0, dtype=float)\n",
    "    traj = [w.copy()]\n",
    "    for _ in range(steps):\n",
    "        grad = A @ w - b\n",
    "        w = w - alpha * grad\n",
    "        traj.append(w.copy())\n",
    "    return np.asarray(traj)\n",
    "\n",
    "def run_momentum(A, b, w0, alpha, beta, steps):\n",
    "    # Heavy-ball / classical momentum:\n",
    "    #   z_{k+1} = beta z_k + grad(w_k)\n",
    "    #   w_{k+1} = w_k - alpha z_{k+1}\n",
    "    w = np.array(w0, dtype=float)\n",
    "    z = np.zeros_like(w)\n",
    "    traj = [w.copy()]\n",
    "    for _ in range(steps):\n",
    "        grad = A @ w - b\n",
    "        z = beta * z + grad\n",
    "        w = w - alpha * z\n",
    "        traj.append(w.copy())\n",
    "    return np.asarray(traj)\n",
    "\n",
    "def spectral_radius_2x2(M):\n",
    "    # max |eig(M)| for a 2x2 matrix\n",
    "    vals = np.linalg.eigvals(M)\n",
    "    return float(np.max(np.abs(vals)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffe9f2c",
   "metadata": {},
   "source": [
    "# Why Momentum Really Works\n",
    "\n",
    "This notebook follows the article’s structure closely so we can follow-along. The article’s opening introduces:\n",
    "\n",
    "- The usual “heavy ball” story and why it’s incomplete\n",
    "- The convex quadratic model as a microscope\n",
    "- “Weak curvature conditions” and “pathological curvature”\n",
    "- The momentum tweak: a short-term memory for gradients\n",
    "\n",
    "Everything below in `#### Add-on:` blocks is *extra* derivation + intuition + experiments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f23c08b",
   "metadata": {},
   "source": [
    "#### Add-on: Workflow that actually forces learning\n",
    "\n",
    "Suggested loop per section:\n",
    "\n",
    "1. **Write the claim** (1–2 sentences) in your own words.\n",
    "2. **Re-derive** the math without looking (even if it’s ugly).\n",
    "3. **Predict** what a plot should look like *before* you plot.\n",
    "4. **Run the experiment.** If your prediction was wrong, fix your model.\n",
    "5. **Compress** into 3 bullets: *what matters, what doesn’t, what knobs do*.\n",
    "\n",
    "Keep a running “mistakes I made” list. Momentum will exploit your mistakes mercilessly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9655798",
   "metadata": {},
   "source": [
    "#### Add-on: Notation glossary (use consistently)\n",
    "\n",
    "We’ll reuse the article’s letters:\n",
    "\n",
    "- Objective: \\(f(w)\\)\n",
    "- Iterate (“position”): \\(w_k\\)\n",
    "- Momentum buffer (“velocity-ish”): \\(z_k\\)\n",
    "- Step-size: \\(\\alpha > 0\\)\n",
    "- Momentum coefficient: \\(\\beta \\in [0,1)\\)\n",
    "\n",
    "Quadratic microscope:\n",
    "\n",
    "\\[\n",
    "f(w)=\\tfrac12 w^\\top A w - b^\\top w,\\quad A \\succ 0\n",
    "\\]\n",
    "\n",
    "- Optimum: \\(w^\\star = A^{-1} b\\)\n",
    "- Error: \\(e_k := w_k - w^\\star\\)\n",
    "\n",
    "Eigen-decomposition (symmetric \\(A\\)):\n",
    "\n",
    "\\[\n",
    "A = Q \\Lambda Q^\\top,\\quad \\Lambda=\\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n),\\; 0<\\lambda_1\\le \\dots \\le \\lambda_n\n",
    "\\]\n",
    "\n",
    "- Condition number: \\(\\kappa := \\lambda_n/\\lambda_1\\)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71547aa",
   "metadata": {},
   "source": [
    "#### Add-on: Prerequisites the article assumes\n",
    "\n",
    "##### Eigendecomposition refresher (symmetric case)\n",
    "\n",
    "If \\(A\\) is symmetric:\n",
    "\n",
    "- It has **real** eigenvalues \\(\\lambda_i\\).\n",
    "- It has an **orthonormal** eigenbasis \\(Q=[q_1,\\dots,q_n]\\) with \\(Q^\\top Q=I\\).\n",
    "- It diagonalizes: \\(A = Q\\Lambda Q^\\top\\).\n",
    "\n",
    "Why you care: in the \\(Q\\)-basis, quadratics **decouple** into independent 1D problems.\n",
    "\n",
    "##### Convexity / curvature vocabulary\n",
    "\n",
    "For twice-differentiable \\(f\\):\n",
    "\n",
    "- **\\(L\\)-smooth**: \\(\\|\\nabla f(x)-\\nabla f(y)\\|\\le L\\|x-y\\|\\) (gradient doesn’t change too violently)\n",
    "- **\\(\\mu\\)-strongly convex**: \\(f(y)\\ge f(x)+\\nabla f(x)^\\top (y-x)+\\tfrac{\\mu}{2}\\|y-x\\|^2\\) (bowl has minimum curvature)\n",
    "\n",
    "For a quadratic \\(f(w)=\\tfrac12 w^\\top A w\\):\n",
    "\n",
    "- \\(L = \\lambda_{\\max}(A)\\), \\(\\mu = \\lambda_{\\min}(A)\\)\n",
    "- \\(\\kappa = L/\\mu\\)\n",
    "\n",
    "##### Geometric convergence / “linear rate”\n",
    "\n",
    "If an error evolves as \\( \\|e_k\\|\\le \\rho^k \\|e_0\\|\\) with \\(0<\\rho<1\\), then:\n",
    "\n",
    "- “Exponential/linear convergence” = error shrinks by a constant factor per iteration.\n",
    "- Iterations to reach \\(\\varepsilon\\): \\(k \\gtrsim \\log(\\varepsilon)/\\log(\\rho)\\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a090434",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Quick eigendecomposition sanity check (symmetric 2x2)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m A = \u001b[43mnp\u001b[49m.array([[\u001b[32m4.\u001b[39m, \u001b[32m2.\u001b[39m],\n\u001b[32m      4\u001b[39m               [\u001b[32m2.\u001b[39m, \u001b[32m1.\u001b[39m]])\n\u001b[32m      6\u001b[39m evals, Q = np.linalg.eigh(A)   \u001b[38;5;66;03m# for symmetric matrices\u001b[39;00m\n\u001b[32m      7\u001b[39m Lambda = np.diag(evals)\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Quick eigendecomposition sanity check (symmetric 2x2)\n",
    "\n",
    "A = np.array([[4., 2.],\n",
    "              [2., 1.]])\n",
    "\n",
    "evals, Q = np.linalg.eigh(A)   # for symmetric matrices\n",
    "Lambda = np.diag(evals)\n",
    "\n",
    "print(\"Eigenvalues:\", evals)\n",
    "print(\"Q (columns are eigenvectors):\", Q)\n",
    "print(\"Reconstruction Q Λ Q^T:\", Q @ Lambda @ Q.T)\n",
    "print(\"Reconstruction error:\", np.linalg.norm(A - Q @ Lambda @ Q.T))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177eb71",
   "metadata": {},
   "source": [
    "#### Add-on: “Weak curvature conditions” (what does that phrase hide?)\n",
    "\n",
    "In the article’s opening, “under a few weak curvature conditions” is shorthand for:  \n",
    "there exist assumptions on \\(f\\) that are weaker than “quadratic everywhere” but still strong enough to guarantee a **linear rate**.\n",
    "\n",
    "Two common routes:\n",
    "\n",
    "1) **\\(\\mu\\)-strong convex + \\(L\\)-smooth** (classic convex setting)  \n",
    "   Gradient descent with a suitable \\(\\alpha\\) satisfies  \n",
    "   \\[\n",
    "   f(w_k)-f(w^\\star)\\le \\rho^k\\,(f(w_0)-f(w^\\star)),\\quad \\rho \\approx 1-\\mu/L = 1-1/\\kappa.\n",
    "   \\]\n",
    "   This is already “exponential,” but painfully slow when \\(\\kappa\\) is large.\n",
    "\n",
    "2) **Polyak–Łojasiewicz (PL) inequality** (often called “weakly strongly convex”)  \n",
    "   \\[\n",
    "   \\tfrac12\\|\\nabla f(w)\\|^2 \\ge \\mu\\,(f(w)-f^\\star).\n",
    "   \\]\n",
    "   PL implies a linear rate for gradient descent even when \\(f\\) is not convex.\n",
    "   (Many overparameterized least-squares-like problems satisfy PL locally or globally.)\n",
    "\n",
    "For this notebook we mostly live in the quadratic world, because it lets us solve the dynamics exactly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bf50b8",
   "metadata": {},
   "source": [
    "#### Add-on: Why quadratics are the right microscope\n",
    "\n",
    "For a twice-differentiable \\(f\\), near a point \\(w_0\\),\n",
    "\n",
    "\\[\n",
    "f(w)\\approx f(w_0) + \\nabla f(w_0)^\\top (w-w_0) + \\tfrac12 (w-w_0)^\\top H(w_0) (w-w_0)\n",
    "\\]\n",
    "\n",
    "where \\(H(w_0)=\\nabla^2 f(w_0)\\).\n",
    "\n",
    "So locally, optimization “feels like” a quadratic with \\(A \\approx H(w_0)\\).  \n",
    "This is why eigenvalues/eigenvectors of curvature keep showing up in real training curves.\n",
    "\n",
    "Caveat: in nonconvex deep nets, \\(H\\) changes with \\(w\\), and can be indefinite.  \n",
    "But locally, for small steps, the same linear-systems intuition often predicts behavior (oscillations, stiffness, etc.).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1066c",
   "metadata": {},
   "source": [
    "#### Add-on: Momentum variants (so you don’t mix algorithms)\n",
    "\n",
    "This article analyzes **classical momentum / Polyak heavy-ball**:\n",
    "\n",
    "\\[\n",
    "z_{k+1}=\\beta z_k+\\nabla f(w_k),\\qquad\n",
    "w_{k+1}=w_k-\\alpha z_{k+1}.\n",
    "\\]\n",
    "\n",
    "**Nesterov accelerated gradient (NAG)** uses a *lookahead* gradient (roughly):  \n",
    "\\(\\nabla f(w_k - \\alpha\\beta z_k)\\) instead of \\(\\nabla f(w_k)\\).\n",
    "\n",
    "Both can “accelerate,” but their stability and guarantees differ.  \n",
    "In this notebook, “momentum” = heavy-ball unless explicitly stated otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281155dc",
   "metadata": {},
   "source": [
    "## First Steps: Gradient Descent\n",
    "\n",
    "The article’s move: analyze gradient descent on a convex quadratic because it is solvable *exactly*, and still captures “stiffness” / pathological curvature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67b5c5",
   "metadata": {},
   "source": [
    "#### Add-on: Quadratic model + gradient descent update (no skipped algebra)\n",
    "\n",
    "Take the quadratic:\n",
    "\n",
    "\\[\n",
    "f(w)=\\tfrac12 w^\\top A w - b^\\top w,\\quad A\\succ 0.\n",
    "\\]\n",
    "\n",
    "Gradient:\n",
    "\n",
    "\\[\n",
    "\\nabla f(w) = A w - b.\n",
    "\\]\n",
    "\n",
    "Setting \\(\\nabla f(w^\\star)=0\\) gives the optimum:\n",
    "\n",
    "\\[\n",
    "A w^\\star - b = 0 \\Rightarrow w^\\star = A^{-1} b.\n",
    "\\]\n",
    "\n",
    "Gradient descent (GD):\n",
    "\n",
    "\\[\n",
    "w_{k+1}=w_k-\\alpha \\nabla f(w_k)=w_k-\\alpha(Aw_k-b).\n",
    "\\]\n",
    "\n",
    "Define the error \\(e_k:=w_k-w^\\star\\). Using \\(Aw^\\star=b\\),\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "e_{k+1}\n",
    "&= w_{k+1}-w^\\star \\\\\n",
    "&= \\big(w_k-\\alpha(Aw_k-b)\\big)-w^\\star \\\\\n",
    "&= (w_k-w^\\star)-\\alpha A(w_k-w^\\star) \\\\\n",
    "&= (I-\\alpha A)e_k.\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "So GD on a quadratic is a **linear dynamical system**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00974922",
   "metadata": {},
   "source": [
    "#### Add-on: The eigenspace trick (why it decouples into 1D problems)\n",
    "\n",
    "Because \\(A\\) is symmetric, write:\n",
    "\n",
    "\\[\n",
    "A = Q\\Lambda Q^\\top,\\quad Q^\\top Q = I.\n",
    "\\]\n",
    "\n",
    "Change coordinates to the eigenbasis:\n",
    "\n",
    "\\[\n",
    "x_k := Q^\\top e_k = Q^\\top (w_k-w^\\star).\n",
    "\\]\n",
    "\n",
    "Then\n",
    "\n",
    "\\[\n",
    "x_{k+1} = Q^\\top e_{k+1} = Q^\\top (I-\\alpha A)e_k\n",
    "        = (I-\\alpha \\Lambda)\\,Q^\\top e_k\n",
    "        = (I-\\alpha \\Lambda)x_k.\n",
    "\\]\n",
    "\n",
    "Since \\(\\Lambda\\) is diagonal, this is \\(n\\) independent scalar recurrences:\n",
    "\n",
    "\\[\n",
    "x^{(i)}_{k+1} = (1-\\alpha \\lambda_i)\\,x^{(i)}_k.\n",
    "\\]\n",
    "\n",
    "**Interpretation:** each eigen-direction is its own little 1D optimization problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5033659",
   "metadata": {},
   "source": [
    "#### Add-on: Stability = spectral radius (why \\(|1-\\alpha\\lambda|<1\\) is everything)\n",
    "\n",
    "In 1D, \\(x_{k+1}=r x_k\\) converges to 0 iff \\(|r|<1\\).\n",
    "\n",
    "For GD modes, \\(r_i = 1-\\alpha\\lambda_i\\).  \n",
    "So convergence for *all* modes is:\n",
    "\n",
    "\\[\n",
    "|1-\\alpha \\lambda_i|<1 \\quad \\forall i\n",
    "\\;\\;\\Longleftrightarrow\\;\\;\n",
    "0<\\alpha\\lambda_i<2 \\quad \\forall i\n",
    "\\;\\;\\Longleftrightarrow\\;\\;\n",
    "0<\\alpha<\\tfrac{2}{\\lambda_n}.\n",
    "\\]\n",
    "\n",
    "In matrix terms, GD is stable iff the **spectral radius**  \n",
    "\\(\\rho(I-\\alpha A) = \\max_i |1-\\alpha\\lambda_i|\\) is < 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7064fc13",
   "metadata": {},
   "source": [
    "#### Add-on: “Pathological curvature” = an ellipsoid with a huge aspect ratio\n",
    "\n",
    "For the centered quadratic \\(f(w)-f(w^\\star)=\\tfrac12 e^\\top A e\\), level sets are ellipsoids:\n",
    "\n",
    "\\[\n",
    "\\{e:\\; e^\\top A e = \\text{const}\\}.\n",
    "\\]\n",
    "\n",
    "- If \\(\\kappa=\\lambda_n/\\lambda_1 \\approx 1\\), the ellipsoid is close to a sphere → easy optimization.\n",
    "- If \\(\\kappa \\gg 1\\), the ellipsoid is extremely stretched (“ravine/valley”) → GD must pick \\(\\alpha\\) small enough for the steep direction, and then crawls along the flat direction.\n",
    "\n",
    "So “pathological curvature” ≈ “bad conditioning.”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uguz1oq13qo",
   "metadata": {},
   "source": [
    "#### Prediction (GD trajectory experiment)\n",
    "\n",
    "**Before running the cells below, write your predictions:**\n",
    "\n",
    "1. In original coordinates, will the trajectory go straight to $w^*$ or zig-zag?\n",
    "2. In eigenspace coordinates, which axis (mode) should converge faster — the one with $\\lambda_1=0.2$ or $\\lambda_2=2.0$?\n",
    "3. Given $\\alpha=0.5$, what are the per-mode contraction factors $|1-\\alpha\\lambda_i|$? Are both stable?\n",
    "\n",
    "---\n",
    "\n",
    "**After running, check:**\n",
    "- Did the trajectory behave as predicted?\n",
    "- If not, what was wrong in your mental model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "846c274c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_spd_quadratic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Experiment: GD trajectory in original coordinates vs eigenspace\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Build a rotated quadratic with eigenvalues (lambda1, lambda2)\u001b[39;00m\n\u001b[32m      4\u001b[39m lambda1, lambda2 = \u001b[32m0.2\u001b[39m, \u001b[32m2.0\u001b[39m  \u001b[38;5;66;03m# condition number = 10\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m A, Q = \u001b[43mmake_spd_quadratic\u001b[49m([lambda1, lambda2], theta=np.pi/\u001b[32m6\u001b[39m)\n\u001b[32m      7\u001b[39m b = np.array([\u001b[32m1.0\u001b[39m, \u001b[32m1.0\u001b[39m])\n\u001b[32m      8\u001b[39m w_star = quad_optimum(A, b)\n",
      "\u001b[31mNameError\u001b[39m: name 'make_spd_quadratic' is not defined"
     ]
    }
   ],
   "source": [
    "# Experiment: GD trajectory in original coordinates vs eigenspace\n",
    "\n",
    "# Build a rotated quadratic with eigenvalues (lambda1, lambda2)\n",
    "lambda1, lambda2 = 0.2, 2.0  # condition number = 10\n",
    "A, Q = make_spd_quadratic([lambda1, lambda2], theta=np.pi/6)\n",
    "\n",
    "b = np.array([1.0, 1.0])\n",
    "w_star = quad_optimum(A, b)\n",
    "\n",
    "w0 = np.array([0.0, 0.0])\n",
    "alpha = 0.5\n",
    "steps = 40\n",
    "\n",
    "traj_w = run_gd(A, b, w0, alpha, steps)\n",
    "traj_x = (traj_w - w_star) @ Q  # eigenspace coordinates of error (since Q orthonormal)\n",
    "\n",
    "print(f\"Condition number κ = {lambda2/lambda1:.1f}\")\n",
    "print(\"Eigenvalues:\", [lambda1, lambda2])\n",
    "print(\"alpha:\", alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6625153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: original space\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(traj_w[:, 0], traj_w[:, 1], marker='o', markersize=3)\n",
    "plt.scatter([w_star[0]], [w_star[1]], marker='*', s=150)\n",
    "plt.title(\"GD trajectory in original coordinates\")\n",
    "plt.xlabel(\"$w_1$\")\n",
    "plt.ylabel(\"$w_2$\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743a2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: eigenspace (decoupled axes)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(traj_x[:, 0], traj_x[:, 1], marker='o', markersize=3)\n",
    "plt.scatter([0.0], [0.0], marker='*', s=150)\n",
    "plt.title(\"GD trajectory in eigenspace (coordinates decouple)\")\n",
    "plt.xlabel(\"$x_1$ (mode with $\\lambda_1$)\")\n",
    "plt.ylabel(\"$x_2$ (mode with $\\lambda_2$)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8fzp1cqet",
   "metadata": {},
   "source": [
    "#### Quick self-test (First Steps: Gradient Descent)\n",
    "\n",
    "*Without scrolling up:*\n",
    "\n",
    "1. For GD on a quadratic $f(w) = \\frac{1}{2}w^\\top A w - b^\\top w$, derive the error dynamics $e_{k+1} = (?)e_k$.\n",
    "2. Why does the problem \"decouple\" in the eigenbasis of $A$?\n",
    "3. What is the per-mode contraction factor for eigenvalue $\\lambda_i$?\n",
    "4. For stability, what constraint must $\\alpha$ satisfy in terms of $\\lambda_{\\max}$?\n",
    "\n",
    "<details>\n",
    "<summary>Answers</summary>\n",
    "\n",
    "1. $e_{k+1} = (I - \\alpha A)e_k$\n",
    "2. Because $A = Q\\Lambda Q^\\top$ with orthonormal $Q$, so in coordinates $x_k = Q^\\top e_k$, we get $x_{k+1} = (I - \\alpha\\Lambda)x_k$ — a diagonal system.\n",
    "3. $|1 - \\alpha\\lambda_i|$\n",
    "4. $0 < \\alpha < 2/\\lambda_{\\max}$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4584fb90",
   "metadata": {},
   "source": [
    "### Decomposing the Error\n",
    "\n",
    "The article’s key claim: GD is fast on *high curvature* modes (large \\(\\lambda\\)), slow on *low curvature* modes (small \\(\\lambda\\)).  \n",
    "This creates the classic “fast drop, then slog.”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db92b56",
   "metadata": {},
   "source": [
    "#### Add-on: Solve each eigenmode and decompose the loss (step-by-step)\n",
    "\n",
    "From\n",
    "\n",
    "\\[\n",
    "x^{(i)}_{k+1}=(1-\\alpha\\lambda_i)\\,x^{(i)}_k\n",
    "\\]\n",
    "\n",
    "we get\n",
    "\n",
    "\\[\n",
    "x^{(i)}_k = (1-\\alpha\\lambda_i)^k\\,x^{(i)}_0.\n",
    "\\]\n",
    "\n",
    "For the quadratic **centered at the optimum**:\n",
    "\n",
    "\\[\n",
    "f(w)-f(w^\\star)=\\tfrac12 (w-w^\\star)^\\top A (w-w^\\star)\n",
    "= \\tfrac12 e^\\top A e.\n",
    "\\]\n",
    "\n",
    "In eigen-coordinates \\(e = Qx\\), so:\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "f(w)-f(w^\\star)\n",
    "&= \\tfrac12 (Qx)^\\top (Q\\Lambda Q^\\top)(Qx) \\\\\n",
    "&= \\tfrac12 x^\\top \\Lambda x \\\\\n",
    "&= \\tfrac12 \\sum_{i=1}^n \\lambda_i \\left(x^{(i)}\\right)^2.\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "Plugging \\(x^{(i)}_k\\) in:\n",
    "\n",
    "\\[\n",
    "f(w_k)-f(w^\\star)\n",
    "= \\tfrac12 \\sum_{i=1}^n \\lambda_i \\left(1-\\alpha\\lambda_i\\right)^{2k}\\left(x^{(i)}_0\\right)^2.\n",
    "\\]\n",
    "\n",
    "So each eigenmode contributes a geometric decay term with rate \\(|1-\\alpha\\lambda_i|\\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i4qh8ebfb9j",
   "metadata": {},
   "source": [
    "#### Prediction (loss decomposition experiment)\n",
    "\n",
    "**Before running:**\n",
    "\n",
    "1. With $\\lambda \\in \\{0.01, 0.1, 1.0\\}$ and $\\alpha=0.5$, which mode will hit machine precision first?\n",
    "2. What are the contraction rates $|1-\\alpha\\lambda|$ for each mode?\n",
    "3. After $k=120$ iterations, roughly how much of the total loss will be from the slowest mode?\n",
    "\n",
    "---\n",
    "\n",
    "**After running, check:** Did the \"fast drop, then slog\" pattern appear? Which mode dominated at the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: loss decomposition by eigenmode (synthetic 1D modes)\n",
    "\n",
    "lambdas = np.array([0.01, 0.1, 1.0])  # three eigenvalues\n",
    "alpha = 0.5\n",
    "x0 = np.ones_like(lambdas)\n",
    "K = 120\n",
    "\n",
    "k = np.arange(K + 1)\n",
    "mode_losses = []\n",
    "for lam in lambdas:\n",
    "    xk = x0[0] * (1 - alpha * lam) ** k\n",
    "    mode_losses.append(0.5 * lam * xk**2)\n",
    "mode_losses = np.array(mode_losses)\n",
    "total = mode_losses.sum(axis=0)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for i, lam in enumerate(lambdas):\n",
    "    plt.semilogy(k, mode_losses[i], label=f\"mode {i+1}: λ={lam}\")\n",
    "plt.semilogy(k, total, linestyle=\"--\", label=\"total\")\n",
    "plt.title(\"Loss contributions by eigenmode\")\n",
    "plt.xlabel(\"iteration k\")\n",
    "plt.ylabel(\"loss (log scale)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Rates |1-αλ|:\")\n",
    "for lam in lambdas:\n",
    "    print(f\"  λ={lam}: |1-{alpha}×{lam}| = {abs(1-alpha*lam):.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8eb81",
   "metadata": {},
   "source": [
    "### Choosing A Step-size\n",
    "\n",
    "The step-size \\(\\alpha\\) is a stability knob and a speed knob.  \n",
    "On quadratics, you can reason about it exactly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a4ef39",
   "metadata": {},
   "source": [
    "#### Add-on: Derive stability and the optimal fixed step-size\n",
    "\n",
    "**Stability** for all modes requires\n",
    "\n",
    "\\[\n",
    "|1-\\alpha\\lambda_i|<1\\quad\\forall i\n",
    "\\;\\;\\Longleftrightarrow\\;\\;\n",
    "0<\\alpha<\\tfrac{2}{\\lambda_n}.\n",
    "\\]\n",
    "\n",
    "**Best fixed \\(\\alpha\\)** (minimize worst-case contraction):\n",
    "\n",
    "Overall rate is:\n",
    "\n",
    "\\[\n",
    "\\rho(\\alpha)=\\max_i |1-\\alpha\\lambda_i|\n",
    "           = \\max\\left\\{|1-\\alpha\\lambda_1|,\\;|1-\\alpha\\lambda_n|\\right\\}\n",
    "\\]\n",
    "\n",
    "since endpoints dominate.\n",
    "\n",
    "To minimize the max, balance the endpoints:\n",
    "\n",
    "\\[\n",
    "1-\\alpha\\lambda_1 = -(1-\\alpha\\lambda_n)\n",
    "\\Rightarrow\n",
    "2=\\alpha(\\lambda_1+\\lambda_n)\n",
    "\\Rightarrow\n",
    "\\alpha^\\star = \\frac{2}{\\lambda_1+\\lambda_n}.\n",
    "\\]\n",
    "\n",
    "Then the optimal rate is\n",
    "\n",
    "\\[\n",
    "\\rho^\\star\n",
    "= |1-\\alpha^\\star \\lambda_n|\n",
    "= \\frac{\\lambda_n-\\lambda_1}{\\lambda_n+\\lambda_1}\n",
    "= \\frac{\\kappa-1}{\\kappa+1}.\n",
    "\\]\n",
    "\n",
    "Key point: as \\(\\kappa\\to\\infty\\), \\(\\rho^\\star\\approx 1-\\frac{2}{\\kappa}\\) → slow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6wqy6e8nipc",
   "metadata": {},
   "source": [
    "#### Prediction (step-size vs convergence rate)\n",
    "\n",
    "**Before running:**\n",
    "\n",
    "1. The plot shows $|1-\\alpha\\lambda_1|$ and $|1-\\alpha\\lambda_n|$. At what $\\alpha$ does each curve equal 1?\n",
    "2. For the optimal $\\alpha^* = 2/(\\lambda_1+\\lambda_n)$, the two curves should intersect. What's the y-value at intersection?\n",
    "3. Is the optimal rate closer to 0 or to 1 when $\\kappa=100$?\n",
    "\n",
    "---\n",
    "\n",
    "**After running, check:** Did the curves intersect where you expected? How many iterations for $10^{-6}$ reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: visualize GD convergence factor vs step-size α\n",
    "\n",
    "lambda1, lambda_n = 0.1, 10.0\n",
    "kappa = lambda_n / lambda1\n",
    "\n",
    "alpha_vals = np.linspace(1e-4, 2/lambda_n - 1e-4, 400)\n",
    "rate1 = np.abs(1 - alpha_vals * lambda1)\n",
    "raten = np.abs(1 - alpha_vals * lambda_n)\n",
    "overall = np.maximum(rate1, raten)\n",
    "\n",
    "alpha_star = 2 / (lambda1 + lambda_n)\n",
    "rho_star = (kappa - 1) / (kappa + 1)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(alpha_vals, rate1, label=r\"$|1-\\alpha\\lambda_1|$\")\n",
    "plt.plot(alpha_vals, raten, label=r\"$|1-\\alpha\\lambda_n|$\")\n",
    "plt.plot(alpha_vals, overall, linestyle=\"--\", label=\"max (overall)\")\n",
    "plt.axvline(alpha_star, linestyle=\":\", label=r\"$\\alpha^*$\")\n",
    "plt.axhline(rho_star, linestyle=\":\", label=r\"$\\rho^*$\")\n",
    "plt.title(f\"GD rate vs step-size (κ={kappa:.0f})\")\n",
    "plt.xlabel(\"α\")\n",
    "plt.ylabel(\"convergence factor per step\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"alpha* = {alpha_star:.6f}\")\n",
    "print(f\"rho*   = {rho_star:.6f}\")\n",
    "print(f\"iters for 1e-6 reduction: {np.log(1e-6)/np.log(rho_star):.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8640406xj",
   "metadata": {},
   "source": [
    "#### Quick self-test (Choosing A Step-size)\n",
    "\n",
    "*Without scrolling up:*\n",
    "\n",
    "1. Why does the optimal fixed step-size balance $|1-\\alpha\\lambda_1| = |1-\\alpha\\lambda_n|$?\n",
    "2. What is $\\alpha^*$ for optimal GD?\n",
    "3. What is the resulting convergence rate $\\rho^*$ in terms of $\\kappa$?\n",
    "4. As $\\kappa \\to \\infty$, how does $\\rho^*$ behave?\n",
    "\n",
    "<details>\n",
    "<summary>Answers</summary>\n",
    "\n",
    "1. The overall rate is $\\max(|1-\\alpha\\lambda_1|, |1-\\alpha\\lambda_n|)$. To minimize a max, balance the two terms.\n",
    "2. $\\alpha^* = 2/(\\lambda_1 + \\lambda_n)$\n",
    "3. $\\rho^* = (\\kappa - 1)/(\\kappa + 1)$\n",
    "4. $\\rho^* \\approx 1 - 2/\\kappa \\to 1$ (slow convergence)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ede349",
   "metadata": {},
   "source": [
    "## Example: Polynomial Regression\n",
    "\n",
    "The article’s point here: eigenvectors are not abstract — in real problems they correspond to meaningful “features,” and GD learns some features much faster than others.\n",
    "\n",
    "This example uses polynomial regression as a concrete “eigenfeatures” story.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5aacc6",
   "metadata": {},
   "source": [
    "#### Add-on: Vandermonde matrix → normal equations (show the quadratic)\n",
    "\n",
    "Given data \\((\\xi_i, d_i)\\) and a degree-\\((n-1)\\) polynomial model\n",
    "\n",
    "\\[\n",
    "\\text{model}(\\xi)=\\sum_{j=0}^{n-1} w_{j}\\,\\xi^{j},\n",
    "\\]\n",
    "\n",
    "define the Vandermonde/design matrix \\(Z\\in\\mathbb{R}^{m\\times n}\\):\n",
    "\n",
    "\\[\n",
    "Z_{i,j} = \\xi_i^{\\,j}.\n",
    "\\]\n",
    "\n",
    "Then the least squares objective is\n",
    "\n",
    "\\[\n",
    "\\min_w \\tfrac12\\|Zw-d\\|^2\n",
    "= \\tfrac12 w^\\top (Z^\\top Z) w - (Z^\\top d)^\\top w + \\text{const}.\n",
    "\\]\n",
    "\n",
    "So it’s exactly our quadratic microscope with:\n",
    "\n",
    "- \\(A = Z^\\top Z\\)\n",
    "- \\(b = Z^\\top d\\)\n",
    "\n",
    "GD dynamics are governed by eigenvalues/eigenvectors of \\(Z^\\top Z\\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf34e4d",
   "metadata": {},
   "source": [
    "#### Add-on: Eigenfeatures, SVD, and “sensitivity to noise”\n",
    "\n",
    "Let \\(Z = U\\Sigma V^\\top\\) be the SVD.\n",
    "\n",
    "Then:\n",
    "\n",
    "- \\(Z^\\top Z = V \\Sigma^2 V^\\top\\)\n",
    "- eigenvectors of \\(Z^\\top Z\\) are columns of \\(V\\)\n",
    "- eigenvalues are \\(\\sigma_i^2\\)\n",
    "\n",
    "Small \\(\\sigma_i\\) (small eigenvalues) correspond to directions where:\n",
    "\n",
    "- the data has little support / leverage\n",
    "- the inverse problem is ill-conditioned\n",
    "- tiny noise in \\(d\\) causes large changes in the fitted \\(w\\)\n",
    "\n",
    "That’s why high-degree polynomial fits are famously unstable unless you regularize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70vebwhfik",
   "metadata": {},
   "source": [
    "#### Prediction (polynomial regression experiments)\n",
    "\n",
    "**Before running:**\n",
    "\n",
    "1. The eigenvalues of $Z^\\top Z$ span many orders of magnitude. Should the condition number be large or small?\n",
    "2. \"Eigenfeatures\" corresponding to small eigenvalues are typically high-frequency or low-frequency?\n",
    "3. In early stopping: at iteration 1, will the fit be underfit, overfit, or just right?\n",
    "4. As iterations increase, does the fit first improve then worsen, or monotonically improve?\n",
    "\n",
    "---\n",
    "\n",
    "**After running, check:** Did the eigenvalue spread match your intuition? What happened to the fit at different iterations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bbfb5d",
   "metadata": {},
   "source": [
    "#### Add-on: Early stopping as a spectral filter (compare to Tikhonov)\n",
    "\n",
    "Tikhonov (ridge) solves:\n",
    "\n",
    "\\[\n",
    "\\min_w \\tfrac12\\|Zw-d\\|^2 + \\tfrac{\\eta}{2}\\|w\\|^2\n",
    "\\Rightarrow\n",
    "w_\\eta = (Z^\\top Z + \\eta I)^{-1}Z^\\top d.\n",
    "\\]\n",
    "\n",
    "In the eigenbasis of \\(Z^\\top Z\\), each mode is shrunk by:\n",
    "\n",
    "\\[\n",
    "\\text{ridge filter:}\\;\\;\\; g_\\eta(\\lambda)=\\frac{\\lambda}{\\lambda+\\eta}.\n",
    "\\]\n",
    "\n",
    "GD starting at \\(w_0=0\\) has closed form:\n",
    "\n",
    "\\[\n",
    "w_k = \\sum_{t=0}^{k-1} (I-\\alpha A)^t \\alpha b\n",
    "\\]\n",
    "\n",
    "and in each eigenmode \\(\\lambda\\):\n",
    "\n",
    "\\[\n",
    "\\text{early-stop filter:}\\;\\;\\; g_k(\\lambda)=1-(1-\\alpha\\lambda)^k.\n",
    "\\]\n",
    "\n",
    "Both suppress small-\\(\\lambda\\) directions early — that’s why early stopping acts like regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8k7grje0wxl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: compare Tikhonov (ridge) vs early-stopping spectral filters\n",
    "\n",
    "def ridge_filter(lam, eta):\n",
    "    \"\"\"Ridge/Tikhonov spectral filter: g_η(λ) = λ / (λ + η)\"\"\"\n",
    "    return lam / (lam + eta)\n",
    "\n",
    "def gd_early_stop_filter(lam, alpha, k):\n",
    "    \"\"\"Early-stopped GD spectral filter: g_k(λ) = 1 - (1 - αλ)^k\"\"\"\n",
    "    return 1 - (1 - alpha * lam) ** k\n",
    "\n",
    "# Log-spaced eigenvalues to see behavior across scales\n",
    "lambda_vals = np.logspace(-3, 1, 500)  # λ from 0.001 to 10\n",
    "\n",
    "# Parameters\n",
    "eta = 0.1      # Ridge regularization strength\n",
    "alpha = 0.1   # GD step size\n",
    "k_vals = [10, 50, 200]  # Different stopping times\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Ridge filter\n",
    "ridge = ridge_filter(lambda_vals, eta)\n",
    "plt.semilogx(lambda_vals, ridge, 'k--', linewidth=2.5, label=f'Tikhonov (η={eta})')\n",
    "\n",
    "# Early-stopped GD filters\n",
    "colors = ['C0', 'C1', 'C2']\n",
    "for k, color in zip(k_vals, colors):\n",
    "    gd_filter = gd_early_stop_filter(lambda_vals, alpha, k)\n",
    "    plt.semilogx(lambda_vals, gd_filter, color=color, linewidth=1.5, \n",
    "                 label=f'Early-stopped GD (k={k})')\n",
    "\n",
    "plt.axhline(1.0, linestyle=':', alpha=0.5, label='full recovery')\n",
    "plt.xlabel('Eigenvalue λ (log scale)')\n",
    "plt.ylabel('Filter response g(λ)')\n",
    "plt.title('Spectral Filters: Tikhonov vs Early-Stopped GD')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.ylim(-0.05, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"• Both filters suppress small-λ directions (regularization effect)\")\n",
    "print(\"• Ridge: smooth transition, controlled by η\")\n",
    "print(\"• Early-stopping: discrete approximation that improves with more iterations\")\n",
    "print(\"• Large λ: both approach 1 (full learning)\")\n",
    "print(\"• Small λ: both stay near 0 (suppressed/regularized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee0c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment setup: polynomial regression and spectrum of Z^T Z\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "m = 50          # number of data points\n",
    "degree = 10     # number of polynomial coefficients\n",
    "xi = np.linspace(-1, 1, m)\n",
    "\n",
    "true_f = lambda x: np.sin(2*np.pi*x)\n",
    "d = true_f(xi) + 0.2*np.random.randn(m)\n",
    "\n",
    "# Vandermonde matrix (increasing powers)\n",
    "Z = np.vander(xi, degree, increasing=True)\n",
    "\n",
    "A = Z.T @ Z\n",
    "b = Z.T @ d\n",
    "\n",
    "evals, V = np.linalg.eigh(A)  # evals sorted ascending\n",
    "kappa = evals[-1] / evals[0]\n",
    "\n",
    "print(\"Eigenvalues of Z^T Z (ascending):\")\n",
    "print(evals)\n",
    "print(f\"Condition number κ ≈ {kappa:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59d0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a few eigenfeatures (eigenvectors interpreted as polynomial coefficient vectors)\n",
    "\n",
    "xi_plot = np.linspace(-1, 1, 400)\n",
    "Z_plot = np.vander(xi_plot, degree, increasing=True)\n",
    "\n",
    "# Choose a few indices: smallest, mid, largest\n",
    "idxs = [0, 1, 2, -3, -2, -1]\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for idx in idxs:\n",
    "    feature = Z_plot @ V[:, idx]\n",
    "    plt.plot(xi_plot, feature, label=f\"idx {idx} (λ={evals[idx]:.1e})\")\n",
    "plt.axhline(0, linewidth=1)\n",
    "plt.title(\"Eigenfeatures (functions) corresponding to eigenvectors of ZᵀZ\")\n",
    "plt.xlabel(\"ξ\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6244365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping experiment: gradient descent on least squares\n",
    "\n",
    "w0 = np.zeros(degree)\n",
    "\n",
    "alpha = 1.0 / evals[-1]   # conservative (<= 1/L)\n",
    "steps = 5000\n",
    "\n",
    "traj_w = run_gd(A, b, w0, alpha, steps)\n",
    "\n",
    "# Evaluate fits at a few iterations\n",
    "iters = [1, 10, 100, 1000, 5000]\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(xi, d, s=15, alpha=0.6, label=\"data\")\n",
    "plt.plot(xi_plot, true_f(xi_plot), linestyle=\"--\", label=\"true f\")\n",
    "\n",
    "for k in iters:\n",
    "    wk = traj_w[k]\n",
    "    fit = Z_plot @ wk\n",
    "    plt.plot(xi_plot, fit, label=f\"GD iter {k}\", alpha=0.8)\n",
    "\n",
    "plt.title(\"Early stopping: underfit → fit → overfit (can happen)\")\n",
    "plt.xlabel(\"ξ\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "print(f\"alpha used: {alpha:.3e} (1/λ_max)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rbye8qkiezp",
   "metadata": {},
   "source": [
    "#### Quick self-test (Polynomial Regression)\n",
    "\n",
    "*Without scrolling up:*\n",
    "\n",
    "1. For least squares $\\min_w \\|Zw - d\\|^2$, what is the Hessian $A$ in quadratic form?\n",
    "2. How are the eigenvalues of $Z^\\top Z$ related to the singular values of $Z$?\n",
    "3. Why does early stopping act like regularization?\n",
    "4. What is the \"spectral filter\" for GD after $k$ steps?\n",
    "\n",
    "<details>\n",
    "<summary>Answers</summary>\n",
    "\n",
    "1. $A = Z^\\top Z$\n",
    "2. If $Z = U\\Sigma V^\\top$, then eigenvalues of $Z^\\top Z$ are $\\sigma_i^2$.\n",
    "3. Early stopping suppresses small-eigenvalue directions, similar to how ridge regression does.\n",
    "4. $g_k(\\lambda) = 1 - (1-\\alpha\\lambda)^k$ — this starts at 0 and approaches 1, suppressing small $\\lambda$ early.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ca1d0a",
   "metadata": {},
   "source": [
    "## The Dynamics of Momentum\n",
    "\n",
    "Momentum is not “mystical.” On quadratics it is a **2nd‑order linear system** per eigenmode.  \n",
    "That means: eigenvalues of a 2×2 matrix tell you everything (rates, oscillations, stability).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea64df3",
   "metadata": {},
   "source": [
    "#### Add-on: From the update rule to a scalar 2nd‑order recurrence\n",
    "\n",
    "Momentum (heavy-ball):\n",
    "\n",
    "\\[\n",
    "z_{k+1}=\\beta z_k + \\nabla f(w_k),\\qquad\n",
    "w_{k+1}=w_k-\\alpha z_{k+1}.\n",
    "\\]\n",
    "\n",
    "On the quadratic, \\(\\nabla f(w_k)=A(w_k-w^\\star)=Ae_k\\).\n",
    "\n",
    "In eigenmode \\(i\\), define scalar error \\(x_k := x^{(i)}_k\\). Then:\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "y_{k+1} &= \\beta y_k + \\lambda x_k \\\\\n",
    "x_{k+1} &= x_k - \\alpha y_{k+1}\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "Eliminate \\(y\\). From \\(x_{k}=x_{k-1}-\\alpha y_{k}\\) we get \\(y_k=(x_{k-1}-x_k)/\\alpha\\). Substitute into the first:\n",
    "\n",
    "\\[\n",
    "\\frac{x_k-x_{k+1}}{\\alpha} = \\beta\\frac{x_{k-1}-x_k}{\\alpha} + \\lambda x_k\n",
    "\\]\n",
    "\n",
    "Multiply by \\(\\alpha\\) and rearrange:\n",
    "\n",
    "\\[\n",
    "x_{k+1} = (1+\\beta-\\alpha\\lambda)\\,x_k - \\beta x_{k-1}.\n",
    "\\]\n",
    "\n",
    "That is a classic discrete-time 2nd-order system. Oscillations happen when the characteristic roots are complex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6cca65",
   "metadata": {},
   "source": [
    "#### Add-on: 2×2 matrix form and characteristic polynomial\n",
    "\n",
    "Write the single-mode state as \\([y_k,\\;x_k]^\\top\\). Then:\n",
    "\n",
    "\\[\n",
    "\\begin{bmatrix} y_{k+1} \\\\ x_{k+1}\\end{bmatrix}\n",
    "=\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "\\beta & \\lambda \\\\\n",
    "-\\alpha\\beta & 1-\\alpha\\lambda\n",
    "\\end{bmatrix}}_{R(\\alpha,\\beta,\\lambda)}\n",
    "\\begin{bmatrix} y_{k} \\\\ x_{k}\\end{bmatrix}.\n",
    "\\]\n",
    "\n",
    "Convergence in mode \\(\\lambda\\) is governed by the spectral radius \\(\\rho(R)\\).\n",
    "\n",
    "The characteristic polynomial:\n",
    "\n",
    "\\[\n",
    "\\det(R-\\sigma I)=0\n",
    "\\;\\Rightarrow\\;\n",
    "\\sigma^2 - (1+\\beta-\\alpha\\lambda)\\sigma + \\beta = 0.\n",
    "\\]\n",
    "\n",
    "Roots:\n",
    "\n",
    "\\[\n",
    "\\sigma_{1,2}=\\frac{(1+\\beta-\\alpha\\lambda)\\pm\\sqrt{(1+\\beta-\\alpha\\lambda)^2-4\\beta}}{2}.\n",
    "\\]\n",
    "\n",
    "- **Complex roots** when \\((1+\\beta-\\alpha\\lambda)^2<4\\beta\\) → oscillatory “ringing”\n",
    "- **Real roots** otherwise → overdamped / unstable depending on magnitude\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad4146d",
   "metadata": {},
   "source": [
    "#### Add-on: Stability region \\(0<\\alpha\\lambda<2+2\\beta\\) (derive, don’t memorize)\n",
    "\n",
    "We want both characteristic roots inside the unit circle: \\(|\\sigma_{1,2}|<1\\).\n",
    "\n",
    "For a 2nd-order polynomial\n",
    "\\[\n",
    "\\sigma^2 + p\\sigma + q = 0\n",
    "\\]\n",
    "a standard discrete-time stability test (Jury) says the roots satisfy \\(|\\sigma|<1\\) iff:\n",
    "\n",
    "1) \\(|q|<1\\)\n",
    "2) \\(1+p+q>0\\)\n",
    "3) \\(1-p+q>0\\)\n",
    "\n",
    "Our polynomial is:\n",
    "\n",
    "\\[\n",
    "\\sigma^2 - a\\sigma + \\beta = 0\n",
    "\\quad\\text{with}\\quad\n",
    "a := 1+\\beta-\\alpha\\lambda.\n",
    "\\]\n",
    "\n",
    "So \\(p=-a\\), \\(q=\\beta\\). Assume \\(0\\le \\beta <1\\). Then:\n",
    "\n",
    "1) \\(|q|<1 \\Rightarrow \\beta<1\\) ✅\n",
    "2) \\(1+p+q>0 \\Rightarrow 1-a+\\beta>0\\)\n",
    "\n",
    "\\[\n",
    "1-(1+\\beta-\\alpha\\lambda)+\\beta = \\alpha\\lambda > 0\n",
    "\\]\n",
    "\n",
    "3) \\(1-p+q>0 \\Rightarrow 1+a+\\beta>0\\)\n",
    "\n",
    "\\[\n",
    "1+(1+\\beta-\\alpha\\lambda)+\\beta = 2+2\\beta-\\alpha\\lambda > 0\n",
    "\\Rightarrow \\alpha\\lambda < 2+2\\beta\n",
    "\\]\n",
    "\n",
    "So, for \\(0\\le\\beta<1\\):\n",
    "\n",
    "\\[\n",
    "\\boxed{0 < \\alpha\\lambda < 2+2\\beta}\n",
    "\\]\n",
    "\n",
    "This cleanly recovers GD when \\(\\beta=0\\): \\(0<\\alpha\\lambda<2\\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0axq2wh73gp",
   "metadata": {},
   "source": [
    "#### Prediction (momentum convergence factor heatmap)\n",
    "\n",
    "**Before running:**\n",
    "\n",
    "1. For $\\beta=0$ (GD), the stable region is $\\alpha\\lambda < 2$. With momentum, should it be larger or smaller?\n",
    "2. Where on the $(\\alpha, \\beta)$ plane is the convergence factor minimized — high $\\beta$ or low $\\beta$?\n",
    "3. The critical damping curve $\\beta = (1-\\sqrt{\\alpha\\lambda})^2$ — does it pass through the region of fastest convergence?\n",
    "\n",
    "---\n",
    "\n",
    "**After running, check:** Can you identify the \"sweet spot\" visually? Is it on the critical damping curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3449850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: convergence factor over (α, β) for one eigenvalue λ\n",
    "\n",
    "def momentum_mode_rate(alpha: float, beta: float, lam: float) -> float:\n",
    "    \"\"\"Compute the per-iteration convergence factor for momentum on a single eigenmode.\n",
    "    \n",
    "    For the 2nd-order recurrence x_{k+1} = (1+β-αλ)x_k - βx_{k-1}, this returns\n",
    "    the spectral radius of the iteration matrix, i.e., max|σ| where σ are the\n",
    "    roots of the characteristic polynomial σ² - (1+β-αλ)σ + β = 0.\n",
    "    \n",
    "    Returns:\n",
    "        Convergence factor ρ ∈ [0, ∞). Values < 1 indicate convergence.\n",
    "        For complex roots (underdamped), ρ = √β.\n",
    "        For real roots, ρ = max(|σ₁|, |σ₂|).\n",
    "    \"\"\"\n",
    "    a = 1 + beta - alpha * lam\n",
    "    disc = a*a - 4*beta\n",
    "    if disc >= 0:\n",
    "        s1 = (a + np.sqrt(disc)) / 2\n",
    "        s2 = (a - np.sqrt(disc)) / 2\n",
    "        return max(abs(s1), abs(s2))\n",
    "    else:\n",
    "        # complex conjugates with product beta -> |σ| = sqrt(beta)\n",
    "        return np.sqrt(beta)\n",
    "\n",
    "lam = 1.0\n",
    "alpha_vals = np.linspace(0.001, 2.8, 300)\n",
    "beta_vals = np.linspace(0.0, 0.99, 250)\n",
    "\n",
    "rates = np.zeros((len(beta_vals), len(alpha_vals)))\n",
    "for i, beta in enumerate(beta_vals):\n",
    "    for j, alpha in enumerate(alpha_vals):\n",
    "        rates[i, j] = momentum_mode_rate(alpha, beta, lam)\n",
    "\n",
    "# mask unstable\n",
    "rates_masked = rates.copy()\n",
    "rates_masked[rates_masked >= 1] = np.nan\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(\n",
    "    rates_masked,\n",
    "    extent=[alpha_vals[0], alpha_vals[-1], beta_vals[0], beta_vals[-1]],\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    ")\n",
    "plt.colorbar(label=\"max |σ| (white = diverges)\")\n",
    "plt.title(f\"Momentum convergence factor for a single mode (λ={lam})\")\n",
    "plt.xlabel(\"α\")\n",
    "plt.ylabel(\"β\")\n",
    "\n",
    "# Critical damping curve: β = (1 - sqrt(αλ))^2 for αλ in [0,1]\n",
    "alpha_crit = np.linspace(0.001, 1/lam, 200)\n",
    "beta_crit = (1 - np.sqrt(alpha_crit * lam))**2\n",
    "plt.plot(alpha_crit, beta_crit, linewidth=2, label=\"critical damping\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5q44gp3kytt",
   "metadata": {},
   "source": [
    "#### Quick self-test (The Dynamics of Momentum)\n",
    "\n",
    "*Without scrolling up:*\n",
    "\n",
    "1. Write the 2nd-order recurrence for momentum in one eigenmode: $x_{k+1} = (?)x_k - (?)x_{k-1}$.\n",
    "2. What is the characteristic polynomial?\n",
    "3. When are the roots complex (oscillatory regime)?\n",
    "4. What is the stability condition for momentum in terms of $\\alpha\\lambda$ and $\\beta$?\n",
    "\n",
    "<details>\n",
    "<summary>Answers</summary>\n",
    "\n",
    "1. $x_{k+1} = (1 + \\beta - \\alpha\\lambda)x_k - \\beta x_{k-1}$\n",
    "2. $\\sigma^2 - (1+\\beta-\\alpha\\lambda)\\sigma + \\beta = 0$\n",
    "3. When discriminant $< 0$: $(1+\\beta-\\alpha\\lambda)^2 < 4\\beta$\n",
    "4. $0 < \\alpha\\lambda < 2 + 2\\beta$ (for $0 \\le \\beta < 1$)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014813e5",
   "metadata": {},
   "source": [
    "### The Critical Damping Coefficient\n",
    "\n",
    "The article frames a single eigenmode as a damped spring.  \n",
    "Critical damping = fastest return to equilibrium without oscillation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d7e65",
   "metadata": {},
   "source": [
    "#### Add-on: Critical damping via discriminant \\(=0\\)\n",
    "\n",
    "For one mode \\(\\lambda\\), characteristic polynomial is:\n",
    "\n",
    "\\[\n",
    "\\sigma^2 - (1+\\beta-\\alpha\\lambda)\\sigma + \\beta = 0.\n",
    "\\]\n",
    "\n",
    "Critical damping means **repeated root**, i.e. discriminant \\(=0\\):\n",
    "\n",
    "\\[\n",
    "(1+\\beta-\\alpha\\lambda)^2 - 4\\beta = 0.\n",
    "\\]\n",
    "\n",
    "Let \\(u=\\sqrt{\\beta}\\) (so \\(\\beta=u^2\\), \\(u\\ge 0\\)):\n",
    "\n",
    "\\[\n",
    "(1+u^2-\\alpha\\lambda)^2 = (2u)^2.\n",
    "\\]\n",
    "\n",
    "Take the physically relevant branch \\(u\\le 1\\):\n",
    "\n",
    "\\[\n",
    "1+u^2-\\alpha\\lambda = 2u\n",
    "\\;\\Rightarrow\\;\n",
    "(1-u)^2 = \\alpha\\lambda\n",
    "\\;\\Rightarrow\\;\n",
    "u = 1-\\sqrt{\\alpha\\lambda}.\n",
    "\\]\n",
    "\n",
    "So\n",
    "\n",
    "\\[\n",
    "\\boxed{\\beta^\\star = (1-\\sqrt{\\alpha\\lambda})^2}\n",
    "\\]\n",
    "\n",
    "and the repeated root is \\(\\sigma = u = 1-\\sqrt{\\alpha\\lambda}\\).  \n",
    "That gives the per-mode convergence factor:\n",
    "\n",
    "\\[\n",
    "|x_k| \\approx (1-\\sqrt{\\alpha\\lambda})^k |x_0|.\n",
    "\\]\n",
    "\n",
    "Compare GD’s factor \\(1-\\alpha\\lambda\\): the square root is the whole “acceleration” story.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e545bd",
   "metadata": {},
   "source": [
    "#### Add-on: (Optional) “damped spring” mapping in one paragraph\n",
    "\n",
    "For a quadratic \\(f(x)=\\tfrac12\\lambda x^2\\), the continuous-time damped oscillator is:\n",
    "\n",
    "\\[\n",
    "\\ddot x(t) + c\\,\\dot x(t) + \\lambda x(t)=0.\n",
    "\\]\n",
    "\n",
    "Momentum behaves like a simple discrete-time simulation of this ODE when \\(\\alpha\\) is small and \\(\\beta\\) is close to 1:\n",
    "- the “spring force” is \\(\\lambda x\\)\n",
    "- \\(\\beta\\) controls damping (how much velocity is retained)\n",
    "- critical damping is the boundary between oscillation and monotone return\n",
    "\n",
    "You don’t need the ODE to do the discrete math — but the picture helps you predict ringing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z71n45i761l",
   "metadata": {},
   "source": [
    "#### Prediction (phase portraits: under/critical/overdamped)\n",
    "\n",
    "**Before running:**\n",
    "\n",
    "1. **Underdamped** ($\\beta=0.95$, high): should the trajectory spiral inward or go straight?\n",
    "2. **Critical** ($\\beta = (1-\\sqrt{\\alpha\\lambda})^2$): spiral or straight approach?\n",
    "3. **Overdamped** ($\\beta=0.2$, low): spiral or straight?\n",
    "4. Which damping regime reaches the origin fastest?\n",
    "\n",
    "---\n",
    "\n",
    "**After running, check:** Match each phase portrait to your prediction. Which had oscillations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: phase portrait for one mode (y vs x) in under/critical/over damping\n",
    "\n",
    "def run_mode(alpha: float, beta: float, lam: float, \n",
    "             x0: float = 1.0, y0: float = 0.0, steps: int = 80) -> np.ndarray:\n",
    "    \"\"\"Simulate one eigenmode of momentum dynamics and return the phase trajectory.\n",
    "    \n",
    "    State variables:\n",
    "        y: \"velocity-like\" — the momentum buffer (accumulated gradient)\n",
    "        x: \"position\" — the error in this eigenmode (distance from optimum)\n",
    "    \n",
    "    Update equations:\n",
    "        y_{k+1} = β y_k + λ x_k\n",
    "        x_{k+1} = x_k - α y_{k+1}\n",
    "    \n",
    "    Returns:\n",
    "        Array of shape (steps+1, 2) where each row is (y, x).\n",
    "    \"\"\"\n",
    "    x, y = float(x0), float(y0)\n",
    "    traj = [(y, x)]\n",
    "    for _ in range(steps):\n",
    "        y_new = beta * y + lam * x\n",
    "        x_new = x - alpha * y_new\n",
    "        x, y = x_new, y_new\n",
    "        traj.append((y, x))\n",
    "    return np.array(traj)\n",
    "\n",
    "lam = 1.0\n",
    "alpha = 0.1\n",
    "beta_crit = (1 - np.sqrt(alpha * lam))**2\n",
    "\n",
    "cases = [\n",
    "    (\"underdamped\", 0.95),\n",
    "    (\"critical\", beta_crit),\n",
    "    (\"overdamped\", 0.2),\n",
    "]\n",
    "\n",
    "for name, beta in cases:\n",
    "    traj = run_mode(alpha, beta, lam, steps=120)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(traj[:, 0], traj[:, 1], marker='o', markersize=2)\n",
    "    plt.scatter([0.0], [0.0], marker='*', s=150)\n",
    "    plt.title(f\"{name}: β={beta:.4f} (α={alpha}, λ={lam})\")\n",
    "    plt.xlabel(\"y (velocity-like)\")\n",
    "    plt.ylabel(\"x (position/error)\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2a40d",
   "metadata": {},
   "source": [
    "### Optimal parameters\n",
    "\n",
    "Critical damping is a *per-eigenvalue* sweet spot.  \n",
    "But real problems have a range \\(\\lambda \\in [\\lambda_1,\\lambda_n]\\). So the real question is: how do we pick one \\(\\alpha,\\beta\\) that works well for *all* modes?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe828f8",
   "metadata": {},
   "source": [
    "#### Add-on: The optimal heavy-ball parameters on \\([\\lambda_1,\\lambda_n]\\)\n",
    "\n",
    "For strongly convex quadratics with spectrum in \\([\\lambda_1,\\lambda_n]\\), the classical optimal heavy-ball choice is:\n",
    "\n",
    "\\[\n",
    "\\boxed{\n",
    "\\alpha^\\star=\\left(\\frac{2}{\\sqrt{\\lambda_1}+\\sqrt{\\lambda_n}}\\right)^2,\\qquad\n",
    "\\beta^\\star=\\left(\\frac{\\sqrt{\\lambda_n}-\\sqrt{\\lambda_1}}{\\sqrt{\\lambda_n}+\\sqrt{\\lambda_1}}\\right)^2\n",
    "}\n",
    "\\]\n",
    "\n",
    "and the worst-case per-iteration contraction factor becomes:\n",
    "\n",
    "\\[\n",
    "\\boxed{\n",
    "\\rho_\\text{mom} = \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\n",
    "}\n",
    "\\qquad(\\kappa=\\lambda_n/\\lambda_1).\n",
    "\\]\n",
    "\n",
    "Compare GD’s best factor:\n",
    "\n",
    "\\[\n",
    "\\rho_\\text{gd} = \\frac{\\kappa-1}{\\kappa+1}.\n",
    "\\]\n",
    "\n",
    "Asymptotically:\n",
    "- \\(1-\\rho_\\text{gd} = \\Theta(1/\\kappa)\\)\n",
    "- \\(1-\\rho_\\text{mom} = \\Theta(1/\\sqrt{\\kappa})\\)\n",
    "\n",
    "That’s the “square-rooting the condition number” slogan, made literal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zb4xuyp1kgh",
   "metadata": {},
   "source": [
    "#### Prediction (GD vs momentum comparison)\n",
    "\n",
    "**Before running:**\n",
    "\n",
    "1. For $\\kappa=100$: $\\rho_\\text{GD} = (\\kappa-1)/(\\kappa+1) \\approx ?$ and $\\rho_\\text{mom} = (\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1) \\approx ?$\n",
    "2. How many iterations does GD need for $10^{-6}$ reduction? Momentum?\n",
    "3. What's the approximate speedup factor?\n",
    "\n",
    "---\n",
    "\n",
    "**After running, check:** Did your mental arithmetic match the computed values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d4455",
   "metadata": {},
   "source": [
    "#### Add-on: Why \\(\\sqrt{\\kappa}\\) shows up (polynomial / Chebyshev viewpoint)\n",
    "\n",
    "After \\(k\\) steps, many first-order methods produce an iterate whose error in each eigenmode \\(\\lambda\\) looks like:\n",
    "\n",
    "\\[\n",
    "x_k(\\lambda)=p_k(\\lambda)\\,x_0(\\lambda)\n",
    "\\]\n",
    "\n",
    "for some degree-\\(k\\) polynomial \\(p_k\\) with \\(p_k(0)=1\\).\n",
    "\n",
    "So the “best method” in a class becomes:\n",
    "\n",
    "\\[\n",
    "\\min_{p_k} \\max_{\\lambda\\in[\\lambda_1,\\lambda_n]} |p_k(\\lambda)|.\n",
    "\\]\n",
    "\n",
    "The minimax solution is governed by **Chebyshev polynomials**, and that optimization produces the \\(\\sqrt{\\kappa}\\) rate.\n",
    "\n",
    "You don’t need to memorize Chebyshev details — just remember:  \n",
    "**acceleration = choosing a smarter polynomial filter over the spectrum.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nh10prm90eh",
   "metadata": {},
   "source": [
    "#### Add-on: The Chebyshev connection (why $\\sqrt{\\kappa}$ is inevitable)\n",
    "\n",
    "This is the deepest \"why\" for the $\\sqrt{\\kappa}$ improvement. Let's make it concrete.\n",
    "\n",
    "##### The residual polynomial perspective\n",
    "\n",
    "After $k$ steps of any first-order method, the error in eigenmode $\\lambda$ has the form:\n",
    "\n",
    "$$\n",
    "x_k(\\lambda) = p_k(\\lambda) \\cdot x_0(\\lambda)\n",
    "$$\n",
    "\n",
    "where $p_k$ is a degree-$k$ polynomial satisfying $p_k(0) = 1$ (because at $\\lambda=0$ there's no curvature, so error doesn't shrink).\n",
    "\n",
    "**For GD:**\n",
    "$$p_k^{\\text{GD}}(\\lambda) = (1 - \\alpha\\lambda)^k$$\n",
    "\n",
    "**For momentum:** It can be shown that the optimal choice gives a polynomial related to **Chebyshev polynomials**.\n",
    "\n",
    "##### Chebyshev polynomials $T_k$\n",
    "\n",
    "The Chebyshev polynomial $T_k(x)$ is defined on $[-1, 1]$ by:\n",
    "$$T_k(\\cos\\theta) = \\cos(k\\theta)$$\n",
    "\n",
    "**Key property (Chebyshev's theorem):** Among all monic polynomials of degree $k$, the one with smallest max-norm on $[-1, 1]$ is $T_k(x)/2^{k-1}$, with max value $1/2^{k-1}$.\n",
    "\n",
    "##### Why this gives $\\sqrt{\\kappa}$\n",
    "\n",
    "We want to minimize $\\max_{\\lambda \\in [\\lambda_1, \\lambda_n]} |p_k(\\lambda)|$ subject to $p_k(0) = 1$.\n",
    "\n",
    "**Linear transformation:** Map $[\\lambda_1, \\lambda_n]$ to $[-1, 1]$:\n",
    "$$\\lambda \\mapsto t(\\lambda) = \\frac{2\\lambda - (\\lambda_1 + \\lambda_n)}{\\lambda_n - \\lambda_1}$$\n",
    "\n",
    "Then $\\lambda = 0$ maps to:\n",
    "$$t(0) = \\frac{-(\\lambda_1 + \\lambda_n)}{\\lambda_n - \\lambda_1} = -\\frac{\\kappa + 1}{\\kappa - 1}$$\n",
    "\n",
    "The optimal residual polynomial is:\n",
    "$$p_k(\\lambda) = \\frac{T_k(t(\\lambda))}{T_k(t(0))}$$\n",
    "\n",
    "This satisfies $p_k(0) = 1$ and minimizes the max-norm on $[\\lambda_1, \\lambda_n]$.\n",
    "\n",
    "**The rate:** Using Chebyshev asymptotics, for large $|t(0)|$:\n",
    "$$T_k(t(0)) \\approx \\frac{1}{2}\\left(t(0) + \\sqrt{t(0)^2 - 1}\\right)^k$$\n",
    "\n",
    "After simplification (the algebra is in standard references), this yields:\n",
    "$$\\max_{\\lambda \\in [\\lambda_1, \\lambda_n]} |p_k(\\lambda)| \\approx 2 \\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^k$$\n",
    "\n",
    "**The $\\sqrt{\\kappa}$ emerges from the location of $t(0)$ relative to $[-1, 1]$.**\n",
    "\n",
    "##### Connection to momentum\n",
    "\n",
    "Heavy-ball momentum with optimal parameters produces iterates whose residual polynomial approximates the optimal Chebyshev-based polynomial.\n",
    "\n",
    "This is why:\n",
    "- Momentum's rate is $(\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1)$\n",
    "- You can't do better with any polynomial of the same degree\n",
    "- The $\\sqrt{\\kappa}$ improvement over GD is fundamental, not a trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fgc8923ael4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: visualize residual polynomials and the Chebyshev advantage\n",
    "\n",
    "def chebyshev_T(k, x):\n",
    "    \"\"\"Chebyshev polynomial of the first kind, T_k(x).\"\"\"\n",
    "    # Use the recurrence: T_0=1, T_1=x, T_{k+1} = 2x T_k - T_{k-1}\n",
    "    if k == 0:\n",
    "        return np.ones_like(x)\n",
    "    elif k == 1:\n",
    "        return x.copy()\n",
    "    T_prev, T_curr = np.ones_like(x), x.copy()\n",
    "    for _ in range(k - 1):\n",
    "        T_next = 2 * x * T_curr - T_prev\n",
    "        T_prev, T_curr = T_curr, T_next\n",
    "    return T_curr\n",
    "\n",
    "def optimal_residual_polynomial(k, lam, lambda1, lambda_n):\n",
    "    \"\"\"Optimal residual polynomial p_k(λ) based on Chebyshev.\"\"\"\n",
    "    # Map [λ1, λn] to [-1, 1]\n",
    "    t = (2 * lam - (lambda1 + lambda_n)) / (lambda_n - lambda1)\n",
    "    t0 = -(lambda1 + lambda_n) / (lambda_n - lambda1)\n",
    "    return chebyshev_T(k, t) / chebyshev_T(k, np.array([t0]))[0]\n",
    "\n",
    "def gd_residual_polynomial(k, lam, alpha):\n",
    "    \"\"\"GD residual polynomial: (1 - αλ)^k\"\"\"\n",
    "    return (1 - alpha * lam) ** k\n",
    "\n",
    "# Setup\n",
    "lambda1, lambda_n = 1.0, 100.0\n",
    "kappa = lambda_n / lambda1\n",
    "alpha_gd_opt = 2 / (lambda1 + lambda_n)\n",
    "\n",
    "lam_vals = np.linspace(lambda1, lambda_n, 500)\n",
    "k_vals = [5, 10, 20]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for ax, k in zip(axes, k_vals):\n",
    "    # GD residual polynomial\n",
    "    p_gd = gd_residual_polynomial(k, lam_vals, alpha_gd_opt)\n",
    "    \n",
    "    # Optimal (Chebyshev-based) residual polynomial\n",
    "    p_opt = optimal_residual_polynomial(k, lam_vals, lambda1, lambda_n)\n",
    "    \n",
    "    ax.plot(lam_vals, np.abs(p_gd), label=f\"GD: max={np.max(np.abs(p_gd)):.2e}\", linewidth=2)\n",
    "    ax.plot(lam_vals, np.abs(p_opt), label=f\"Chebyshev: max={np.max(np.abs(p_opt)):.2e}\", linewidth=2)\n",
    "    ax.set_xlabel(\"λ\")\n",
    "    ax.set_ylabel(\"|p_k(λ)|\")\n",
    "    ax.set_title(f\"k={k} iterations\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim(1e-12, 2)\n",
    "\n",
    "plt.suptitle(f\"Residual polynomials: GD vs Chebyshev-optimal (κ={kappa:.0f})\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the rates\n",
    "rho_gd = (kappa - 1) / (kappa + 1)\n",
    "rho_cheb = (np.sqrt(kappa) - 1) / (np.sqrt(kappa) + 1)\n",
    "print(f\"κ = {kappa:.0f}\")\n",
    "print(f\"GD rate:        ρ_GD = {rho_gd:.6f}\")\n",
    "print(f\"Chebyshev rate: ρ_Cheb = {rho_cheb:.6f}\")\n",
    "print(f\"Speedup factor: {np.log(rho_gd) / np.log(rho_cheb):.1f}x fewer iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gm41grr6hn5",
   "metadata": {},
   "source": [
    "#### Prediction (2D quadratic trajectory comparison)\n",
    "\n",
    "**Before running:**\n",
    "\n",
    "1. With $\\kappa=25$, should momentum show visible speedup or only marginal improvement?\n",
    "2. Will the momentum trajectory oscillate or go more directly to $w^*$?\n",
    "3. On a log-scale loss plot, should momentum's slope be steeper or shallower than GD's?\n",
    "\n",
    "---\n",
    "\n",
    "**After running, check:** Count iterations until each method's loss drops below $10^{-4}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: compare optimal GD vs optimal momentum rates as κ grows\n",
    "\n",
    "def optimal_gd(lambda1, lambda_n):\n",
    "    alpha = 2 / (lambda1 + lambda_n)\n",
    "    rho = (lambda_n - lambda1) / (lambda_n + lambda1)\n",
    "    return alpha, rho\n",
    "\n",
    "def optimal_momentum(lambda1, lambda_n):\n",
    "    s1, sn = np.sqrt(lambda1), np.sqrt(lambda_n)\n",
    "    alpha = (2 / (s1 + sn))**2\n",
    "    beta = ((sn - s1) / (sn + s1))**2\n",
    "    rho = (sn - s1) / (sn + s1)\n",
    "    return alpha, beta, rho\n",
    "\n",
    "kappas = [10, 100, 1000, 10000]\n",
    "lambda1 = 1.0\n",
    "\n",
    "print(f\"{'κ':>8} | {'rho_gd':>10} | {'rho_mom':>10} | {'iters_gd':>10} | {'iters_mom':>10} | {'speedup':>8}\")\n",
    "print(\"-\"*70)\n",
    "for kappa in kappas:\n",
    "    lambda_n = kappa * lambda1\n",
    "    _, rho_gd = optimal_gd(lambda1, lambda_n)\n",
    "    _, _, rho_mom = optimal_momentum(lambda1, lambda_n)\n",
    "    it_gd = np.log(1e-6) / np.log(rho_gd)\n",
    "    it_mom = np.log(1e-6) / np.log(rho_mom)\n",
    "    print(f\"{kappa:8d} | {rho_gd:10.6f} | {rho_mom:10.6f} | {it_gd:10.0f} | {it_mom:10.0f} | {it_gd/it_mom:8.1f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: GD vs momentum on a 2D quadratic\n",
    "\n",
    "lambda1, lambda2 = 0.2, 5.0   # κ = 25\n",
    "A, Q = make_spd_quadratic([lambda1, lambda2], theta=np.pi/5)\n",
    "b = np.array([1.0, 1.0])\n",
    "w_star = quad_optimum(A, b)\n",
    "\n",
    "w0 = np.array([-1.0, -1.0])\n",
    "steps = 80\n",
    "\n",
    "alpha_gd, _ = optimal_gd(lambda1, lambda2)\n",
    "alpha_m, beta_m, _ = optimal_momentum(lambda1, lambda2)\n",
    "\n",
    "traj_gd = run_gd(A, b, w0, alpha_gd, steps)\n",
    "traj_m = run_momentum(A, b, w0, alpha_m, beta_m, steps)\n",
    "\n",
    "f_star = quad_loss(w_star, A, b)\n",
    "loss_gd = np.array([quad_loss(w, A, b) - f_star for w in traj_gd])\n",
    "loss_m = np.array([quad_loss(w, A, b) - f_star for w in traj_m])\n",
    "\n",
    "print(f\"GD: alpha={alpha_gd:.4f}\")\n",
    "print(f\"Momentum: alpha={alpha_m:.4f}, beta={beta_m:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8851efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectory (original space)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(traj_gd[:, 0], traj_gd[:, 1], marker='o', markersize=3, label=\"GD\")\n",
    "plt.plot(traj_m[:, 0], traj_m[:, 1], marker='o', markersize=3, label=\"Momentum\")\n",
    "plt.scatter([w_star[0]], [w_star[1]], marker='*', s=150, label=\"$w^*$\")\n",
    "plt.title(\"Trajectories on a 2D quadratic\")\n",
    "plt.xlabel(\"$w_1$\")\n",
    "plt.ylabel(\"$w_2$\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf23c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves (log scale)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.semilogy(loss_gd, label=\"GD\")\n",
    "plt.semilogy(loss_m, label=\"Momentum\")\n",
    "plt.title(\"Convergence: GD vs Momentum\")\n",
    "plt.xlabel(\"iteration k\")\n",
    "plt.ylabel(r\"$f(w_k)-f(w^*)$\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1mb0xdqud",
   "metadata": {},
   "source": [
    "#### Quick self-test (Critical Damping and Optimal Parameters)\n",
    "\n",
    "*Without scrolling up:*\n",
    "\n",
    "1. What is critical damping in terms of discriminant?\n",
    "2. Derive $\\beta^* = (1 - \\sqrt{\\alpha\\lambda})^2$ for one mode.\n",
    "3. For spectrum $[\\lambda_1, \\lambda_n]$, what are optimal $\\alpha^*$ and $\\beta^*$?\n",
    "4. What is the optimal momentum rate $\\rho_\\text{mom}$ in terms of $\\kappa$?\n",
    "5. How does this compare to GD's rate asymptotically?\n",
    "\n",
    "<details>\n",
    "<summary>Answers</summary>\n",
    "\n",
    "1. Discriminant $= 0$ (repeated root)\n",
    "2. Set $(1+\\beta-\\alpha\\lambda)^2 = 4\\beta$. Let $u=\\sqrt{\\beta}$, get $(1-u)^2 = \\alpha\\lambda$, so $u = 1-\\sqrt{\\alpha\\lambda}$.\n",
    "3. $\\alpha^* = (2/(\\sqrt{\\lambda_1}+\\sqrt{\\lambda_n}))^2$, $\\beta^* = ((\\sqrt{\\lambda_n}-\\sqrt{\\lambda_1})/(\\sqrt{\\lambda_n}+\\sqrt{\\lambda_1}))^2$\n",
    "4. $\\rho_\\text{mom} = (\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1)$\n",
    "5. GD: $1 - \\rho \\sim 1/\\kappa$. Momentum: $1 - \\rho \\sim 1/\\sqrt{\\kappa}$. Momentum is $O(\\sqrt{\\kappa})$ faster.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68915880",
   "metadata": {},
   "source": [
    "## Example: The Colorization Problem\n",
    "\n",
    "The article reframes the optimization as “information diffusion on a graph.”\n",
    "\n",
    "Key takeaways:\n",
    "- GD updates are **local averaging**\n",
    "- Global structure emerges only through repeated local steps (diffusion is slow)\n",
    "- Graph connectivity ↔ spectrum of Laplacian ↔ condition number ↔ optimization speed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659cbc8a",
   "metadata": {},
   "source": [
    "#### Add-on: Graph Laplacian basics (PSD, zero eigenvalue, why \\(\\lambda_2\\) matters)\n",
    "\n",
    "For an undirected graph \\(G=(V,E)\\), the (combinatorial) Laplacian is:\n",
    "\n",
    "\\[\n",
    "L = D - A_G\n",
    "\\]\n",
    "\n",
    "where \\(D\\) is the degree matrix and \\(A_G\\) is adjacency.\n",
    "\n",
    "Facts:\n",
    "\n",
    "- \\(L\\) is symmetric and **positive semidefinite**:\n",
    "  \\[\n",
    "  x^\\top L x = \\sum_{(i,j)\\in E} (x_i-x_j)^2 \\ge 0.\n",
    "  \\]\n",
    "- The all-ones vector is always an eigenvector with eigenvalue \\(0\\): \\(L\\mathbf{1}=0\\).\n",
    "- \\(\\lambda_2\\) (second-smallest eigenvalue) is the **algebraic connectivity** (Fiedler value):\n",
    "  - small \\(\\lambda_2\\) → graph has bottlenecks → diffusion is slow\n",
    "  - larger \\(\\lambda_2\\) → well-connected → diffusion is fast\n",
    "\n",
    "In these Laplacian-dominated objectives, the “effective condition number” often looks like \\(\\lambda_n/\\lambda_2\\) (since \\(\\lambda_1=0\\) is trivial).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c3252",
   "metadata": {},
   "source": [
    "#### Add-on: Conditioning as connectivity (diffusion intuition made explicit)\n",
    "\n",
    "If the update is local averaging, then information spreads like diffusion:\n",
    "- each step mixes each node with its neighbors\n",
    "- long-range agreement takes many steps unless the graph has shortcuts\n",
    "\n",
    "Spectrally:\n",
    "- “slow modes” correspond to small eigenvalues (low-frequency graph signals)\n",
    "- GD kills high-frequency errors quickly, but low-frequency errors linger\n",
    "- momentum accelerates the slow modes (that’s the whole point)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1lrideva5hj",
   "metadata": {},
   "source": [
    "#### Prediction (graph Laplacian spectra)\n",
    "\n",
    "**Before running:**\n",
    "\n",
    "1. For a path graph of $n$ nodes, the condition number $\\lambda_n/\\lambda_2$ should scale like $O(?)$ in $n$.\n",
    "2. For a grid graph ($m \\times m$), how does the condition number scale with $m$?\n",
    "3. Which graph family is more ill-conditioned for the same number of nodes — path or grid?\n",
    "\n",
    "---\n",
    "\n",
    "**After running, check:** Compare the actual condition numbers. Does the path graph confirm the $O(n^2)$ scaling intuition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf54a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: Laplacian spectra for different graph families\n",
    "\n",
    "def path_laplacian(n):\n",
    "    L = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        if i > 0:\n",
    "            L[i, i] += 1\n",
    "            L[i, i-1] -= 1\n",
    "        if i < n-1:\n",
    "            L[i, i] += 1\n",
    "            L[i, i+1] -= 1\n",
    "    return L\n",
    "\n",
    "def grid_laplacian(m):\n",
    "    # m x m grid, n=m^2 nodes, 4-neighborhood\n",
    "    n = m*m\n",
    "    L = np.zeros((n, n))\n",
    "    def idx(i, j): return i*m + j\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            u = idx(i, j)\n",
    "            for di, dj in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "                ni, nj = i+di, j+dj\n",
    "                if 0 <= ni < m and 0 <= nj < m:\n",
    "                    v = idx(ni, nj)\n",
    "                    L[u, u] += 1\n",
    "                    L[u, v] -= 1\n",
    "    return L\n",
    "\n",
    "def laplacian_condition(L):\n",
    "    evals = np.linalg.eigvalsh(L)\n",
    "    evals = np.sort(evals)\n",
    "    # skip evals[0]=0; use λ2 and λn\n",
    "    return evals[-1] / evals[1], evals\n",
    "\n",
    "for n in [10, 30, 100]:\n",
    "    Lp = path_laplacian(n)\n",
    "    kappa_p, evals_p = laplacian_condition(Lp)\n",
    "    print(f\"Path n={n:3d}: λ2={evals_p[1]:.4e}, λn={evals_p[-1]:.4e},  λn/λ2={kappa_p:.2e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "for m in [4, 8, 12]:\n",
    "    Lg = grid_laplacian(m)\n",
    "    kappa_g, evals_g = laplacian_condition(Lg)\n",
    "    print(f\"Grid {m}x{m} (n={m*m:3d}): λ2={evals_g[1]:.4e}, λn={evals_g[-1]:.4e},  λn/λ2={kappa_g:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sjhtnucyr6j",
   "metadata": {},
   "source": [
    "#### Prediction (path graph colorization diffusion)\n",
    "\n",
    "**Before running:**\n",
    "\n",
    "1. With one pinned node at the left, how does the \"color\" spread along the chain?\n",
    "2. At iteration $k$, roughly how far along the chain has information propagated?\n",
    "3. Should momentum's snapshots show the color reaching the right side faster than GD?\n",
    "\n",
    "---\n",
    "\n",
    "**After running, check:** Does momentum visibly accelerate the diffusion? Compare the $k=20$ and $k=60$ snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0babb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional experiment: \"colorization\" diffusion on a path graph (one pinned node)\n",
    "\n",
    "n = 60\n",
    "L = path_laplacian(n)\n",
    "e1 = np.zeros(n); e1[0] = 1.0\n",
    "\n",
    "A = L + np.outer(e1, e1)   # pin node 1 to break the constant-mode nullspace\n",
    "b = e1\n",
    "\n",
    "# Spectrum for tuning\n",
    "evals = np.linalg.eigvalsh(A)\n",
    "lambda1, lambda_n = evals[0], evals[-1]\n",
    "kappa = lambda_n / lambda1\n",
    "\n",
    "alpha_gd, _ = optimal_gd(lambda1, lambda_n)\n",
    "alpha_m, beta_m, _ = optimal_momentum(lambda1, lambda_n)\n",
    "\n",
    "w0 = np.zeros(n)\n",
    "steps = 200\n",
    "\n",
    "traj_gd = run_gd(A, b, w0, alpha_gd, steps)\n",
    "traj_m = run_momentum(A, b, w0, alpha_m, beta_m, steps)\n",
    "\n",
    "# Plot snapshots of w along the chain\n",
    "snapshots = [0, 5, 20, 60, 200]\n",
    "x = np.arange(n)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for k in snapshots:\n",
    "    plt.plot(x, traj_gd[k], label=f\"GD k={k}\", alpha=0.8)\n",
    "plt.title(f\"Path graph: GD diffusion snapshots (n={n}, κ≈{kappa:.1e})\")\n",
    "plt.xlabel(\"node index\")\n",
    "plt.ylabel(\"w\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for k in snapshots:\n",
    "    plt.plot(x, traj_m[k], label=f\"Mom k={k}\", alpha=0.8)\n",
    "plt.title(f\"Path graph: Momentum diffusion snapshots (n={n}, κ≈{kappa:.1e})\")\n",
    "plt.xlabel(\"node index\")\n",
    "plt.ylabel(\"w\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "print(f\"A spectrum: λ_min={lambda1:.3e}, λ_max={lambda_n:.3e}, κ≈{kappa:.2e}\")\n",
    "print(f\"GD alpha={alpha_gd:.3e}\")\n",
    "print(f\"Momentum alpha={alpha_m:.3e}, beta={beta_m:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x8pextvzbp",
   "metadata": {},
   "source": [
    "#### Quick self-test (The Colorization Problem / Graph Laplacian)\n",
    "\n",
    "*Without scrolling up:*\n",
    "\n",
    "1. What is the graph Laplacian $L$ in terms of degree matrix $D$ and adjacency $A_G$?\n",
    "2. Why is $L$ positive semidefinite?\n",
    "3. What is the eigenvalue associated with the all-ones vector?\n",
    "4. What does $\\lambda_2$ (algebraic connectivity) tell you about the graph?\n",
    "5. Why does diffusion on a path graph take $O(n^2)$ iterations?\n",
    "\n",
    "<details>\n",
    "<summary>Answers</summary>\n",
    "\n",
    "1. $L = D - A_G$\n",
    "2. $x^\\top L x = \\sum_{(i,j) \\in E} (x_i - x_j)^2 \\ge 0$\n",
    "3. $\\lambda_1 = 0$ (constant vector is in nullspace)\n",
    "4. Small $\\lambda_2$ means bottlenecks/poor connectivity; large $\\lambda_2$ means well-connected.\n",
    "5. Condition number $\\lambda_n/\\lambda_2 \\sim O(n^2)$, so GD needs $O(\\kappa) = O(n^2)$ iterations.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf92a1a6",
   "metadata": {},
   "source": [
    "## The Limits of Descent\n",
    "\n",
    "After seeing a quadratic speedup, a natural question is: can we do even better with cleverer first-order tricks?\n",
    "\n",
    "The article’s answer: within a broad class of **linear first-order methods**, there’s a lower bound — and momentum essentially meets it on a worst-case function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a1566d",
   "metadata": {},
   "source": [
    "### Adventures in Algorithmic Space\n",
    "\n",
    "The article “unrolls” algorithms to describe a broad search space of first-order methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7dba10",
   "metadata": {},
   "source": [
    "#### Add-on: Unrolling GD and momentum (explicit gradient weights)\n",
    "\n",
    "GD:\n",
    "\n",
    "\\[\n",
    "w_{k+1}=w_k-\\alpha \\nabla f(w_k)\n",
    "\\]\n",
    "\n",
    "Unroll:\n",
    "\n",
    "\\[\n",
    "w_{k+1}=w_0 - \\alpha\\sum_{i=0}^k \\nabla f(w_i).\n",
    "\\]\n",
    "\n",
    "Momentum (heavy-ball) can also be unrolled to:\n",
    "\n",
    "\\[\n",
    "w_{k+1}=w_0 - \\alpha\\sum_{i=0}^k \\underbrace{\\frac{1-\\beta^{k+1-i}}{1-\\beta}}_{\\text{increasing weight for recent grads}} \\nabla f(w_i)\n",
    "\\]\n",
    "\n",
    "(up to the exact indexing convention — verify by induction in the exercise below).\n",
    "\n",
    "This makes momentum look like GD with a particular **non-uniform weighting** of past gradients.\n",
    "\n",
    "##### What “Linear First-Order Methods” means here\n",
    "\n",
    "Roughly: algorithms whose iterates lie in the affine span of past gradients, with linear (often diagonal) coefficients.\n",
    "\n",
    "It typically *includes*: GD, heavy-ball, Nesterov-style methods, conjugate gradient (on quadratics), etc.  \n",
    "It typically *excludes*: full Newton, quasi-Newton with curvature updates, methods that query Hessian-vector products, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3678e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise check: verify the unrolled form of momentum on an arbitrary gradient sequence\n",
    "\n",
    "np.random.seed(0)\n",
    "k = 8\n",
    "d = 5\n",
    "alpha = 0.1\n",
    "beta = 0.9\n",
    "\n",
    "# Fake gradients g_0..g_k (not coming from any function)\n",
    "g = [np.random.randn(d) for _ in range(k+1)]\n",
    "\n",
    "# Momentum recursion\n",
    "w = np.zeros(d)\n",
    "z = np.zeros(d)\n",
    "w_hist = [w.copy()]\n",
    "for t in range(k+1):\n",
    "    z = beta * z + g[t]\n",
    "    w = w - alpha * z\n",
    "    w_hist.append(w.copy())\n",
    "\n",
    "w_direct = np.zeros(d)\n",
    "for t in range(k+1):\n",
    "    weight = (1 - beta**(k+1 - t)) / (1 - beta)\n",
    "    w_direct = w_direct - alpha * weight * g[t]\n",
    "\n",
    "print(\"Recursive w_{k+1}:\", w_hist[-1])\n",
    "print(\"Unrolled   w_{k+1}:\", w_direct)\n",
    "print(\"Difference norm:\", np.linalg.norm(w_hist[-1] - w_direct))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b00f05",
   "metadata": {},
   "source": [
    "### The Resisting Oracle\n",
    "\n",
    "The article constructs a worst-case “local interaction” objective (a path graph) where information can only propagate one step per iteration. This creates a **light cone**: coordinate \\(i\\) cannot change before iteration \\(i\\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf1f06",
   "metadata": {},
   "source": [
    "#### Add-on: Convex Rosenbrock (a “worst function”) and the light cone proof\n",
    "\n",
    "A simplified version (path graph + one anchored node + regularization) has the form:\n",
    "\n",
    "\\[\n",
    "f(w)=\\tfrac12(w_1-1)^2 + \\tfrac12\\sum_{i=1}^{n-1}(w_i-w_{i+1})^2 + \\tfrac{2}{\\kappa-1}\\|w\\|^2.\n",
    "\\]\n",
    "\n",
    "Key structural fact:\n",
    "\n",
    "- \\(\\nabla f(w)_i\\) depends only on \\(w_{i-1}, w_i, w_{i+1}\\) (local coupling).\n",
    "\n",
    "If you start at \\(w_0=0\\), then by induction:\n",
    "- at step 1, only coordinate 1 can move\n",
    "- at step 2, only coordinates 1–2 can move\n",
    "- …\n",
    "- at step \\(k\\), coordinates \\(>k\\) are still exactly 0\n",
    "\n",
    "That is the “light cone”: information travels at speed 1 coordinate per iteration.\n",
    "\n",
    "Lower bound idea:\n",
    "\n",
    "If the true optimum has nonzero mass in coordinate \\(k+1\\), then any linear first-order method must have nontrivial error at time \\(k\\), because it physically cannot “reach” that coordinate yet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfei4k871oo",
   "metadata": {},
   "source": [
    "#### Prediction (light cone visualization)\n",
    "\n",
    "**Before running:**\n",
    "\n",
    "1. Starting at $w_0 = 0$, which coordinate can change at iteration 1? At iteration $k$?\n",
    "2. The plot shows $w_i^k$ as a heatmap. What shape should the \"active\" region form?\n",
    "3. The boundary $i = k$ is the \"light cone.\" Should any values appear above this line?\n",
    "\n",
    "---\n",
    "\n",
    "**After running, check:** Is the light cone boundary sharp? Why does this structure exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: visualize the light cone on Convex Rosenbrock (path-graph objective)\n",
    "\n",
    "def convex_rosenbrock_grad(w, kappa):\n",
    "    n = len(w)\n",
    "    grad = np.zeros(n)\n",
    "\n",
    "    # colorizer term: 0.5 (w1-1)^2\n",
    "    grad[0] += (w[0] - 1.0)\n",
    "\n",
    "    # smoother: 0.5 sum (w_i - w_{i+1})^2\n",
    "    for i in range(n-1):\n",
    "        diff = w[i] - w[i+1]\n",
    "        grad[i]   += diff\n",
    "        grad[i+1] -= diff\n",
    "\n",
    "    # regularization: (2/(kappa-1)) ||w||^2  -> grad = (4/(kappa-1)) w\n",
    "    grad += (4.0 / (kappa - 1.0)) * w\n",
    "    return grad\n",
    "\n",
    "def run_momentum_custom_grad(grad_fn, w0, alpha, beta, steps, **grad_kwargs):\n",
    "    w = np.array(w0, dtype=float)\n",
    "    z = np.zeros_like(w)\n",
    "    hist = [w.copy()]\n",
    "    for _ in range(steps):\n",
    "        g = grad_fn(w, **grad_kwargs)\n",
    "        z = beta * z + g\n",
    "        w = w - alpha * z\n",
    "        hist.append(w.copy())\n",
    "    return np.asarray(hist)\n",
    "\n",
    "n = 30\n",
    "kappa = 100.0\n",
    "w0 = np.zeros(n)\n",
    "\n",
    "# Heuristic params: use alpha small and beta close to 1\n",
    "alpha = 0.15\n",
    "beta = 0.9\n",
    "steps = 50\n",
    "\n",
    "hist = run_momentum_custom_grad(convex_rosenbrock_grad, w0, alpha, beta, steps, kappa=kappa)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(hist.T, aspect='auto', origin='lower')\n",
    "plt.colorbar(label=r\"$w_i^k$\")\n",
    "plt.title(\"Light cone: coordinate i cannot change before iteration i (structure-driven)\")\n",
    "plt.xlabel(\"iteration k\")\n",
    "plt.ylabel(\"coordinate i\")\n",
    "plt.plot(np.arange(steps+1), np.arange(steps+1), linestyle=\"--\", linewidth=2, label=\"light cone boundary\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3gjlbqe0ynj",
   "metadata": {},
   "source": [
    "#### Quick self-test (The Limits of Descent)\n",
    "\n",
    "*Without scrolling up:*\n",
    "\n",
    "1. What is the \"light cone\" constraint for linear first-order methods starting at $w_0 = 0$?\n",
    "2. If the optimum has $w_i^* = r^i$ for some $r < 1$, what is the minimum error at iteration $k$?\n",
    "3. What value of $r$ appears in the lower bound for the Convex Rosenbrock?\n",
    "4. In what sense does momentum \"match\" this lower bound?\n",
    "\n",
    "<details>\n",
    "<summary>Answers</summary>\n",
    "\n",
    "1. Coordinate $i$ cannot change before iteration $i$: $w_i^k = 0$ for $i > k$.\n",
    "2. $\\|w^k - w^*\\|_\\infty \\ge |w^*_{k+1}| = r^{k+1}$\n",
    "3. $r = (\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1)$\n",
    "4. Momentum's convergence rate matches this $r$ (up to constants), so it's essentially optimal for this function class.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c96188",
   "metadata": {},
   "source": [
    "#### Add-on: The lower bound rate (how the “light cone” turns into \\((\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1})^k\\))\n",
    "\n",
    "The article states the optimum has the form:\n",
    "\n",
    "\\[\n",
    "w_i^\\star = \\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^i.\n",
    "\\]\n",
    "\n",
    "But for any linear first-order method started at 0, the light cone implies:\n",
    "\n",
    "\\[\n",
    "w_i^k = 0\\quad \\text{for } i>k.\n",
    "\\]\n",
    "\n",
    "So at time \\(k\\), coordinate \\(k+1\\) still has error:\n",
    "\n",
    "\\[\n",
    "\\|w^k-w^\\star\\|_\\infty \\ge |w^\\star_{k+1}|\n",
    "= \\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^{k+1}.\n",
    "\\]\n",
    "\n",
    "That gives a hard lower bound on achievable contraction per iteration in this algorithmic class — and momentum matches it (up to constants / asymptotics).\n",
    "\n",
    "Spiritual interpretation: you can’t beat physics if the objective only lets information move locally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c2e98a",
   "metadata": {},
   "source": [
    "## Momentum with Stochastic Gradients\n",
    "\n",
    "Real ML rarely gives the true gradient. You get noisy estimates (minibatches).  \n",
    "Momentum changes the *noise* dynamics too — not just the mean dynamics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d8c60",
   "metadata": {},
   "source": [
    "#### Add-on: A minimal noise model and the deterministic/stochastic split\n",
    "\n",
    "Model minibatch gradient as:\n",
    "\n",
    "\\[\n",
    "g(w_k) = \\nabla f(w_k) + \\epsilon_k,\\quad \\mathbb{E}[\\epsilon_k]=0.\n",
    "\\]\n",
    "\n",
    "Then momentum becomes a **linear system with additive noise**.\n",
    "\n",
    "On a quadratic and in one eigenmode:\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "y_{k+1} &= \\beta y_k + \\lambda x_k + \\epsilon_k \\\\\n",
    "x_{k+1} &= x_k - \\alpha y_{k+1}.\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "Solution = (noiseless dynamics) + (filtered noise):\n",
    "\n",
    "\\[\n",
    "\\begin{bmatrix}y_k\\\\x_k\\end{bmatrix}\n",
    "= R^k\\begin{bmatrix}y_0\\\\x_0\\end{bmatrix}\n",
    "+ \\sum_{t=0}^{k-1} R^{k-1-t}\\begin{bmatrix}1\\\\-\\alpha\\end{bmatrix}\\epsilon_t.\n",
    "\\]\n",
    "\n",
    "So noise is passed through a geometric filter \\(R^{\\cdot}\\). If \\(R\\) is near the stability boundary, that filter has long memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754801e5",
   "metadata": {},
   "source": [
    "#### Add-on: Why increasing \\(\\beta\\) can *increase* noise (despite “averaging”)\n",
    "\n",
    "If we ignore the signal for a moment and look at the momentum buffer under pure noise:\n",
    "\n",
    "\\[\n",
    "y_{k+1} = \\beta y_k + \\epsilon_k,\n",
    "\\quad \\epsilon_k \\sim (0,\\sigma^2).\n",
    "\\]\n",
    "\n",
    "This is an AR(1) process with stationary variance:\n",
    "\n",
    "\\[\n",
    "\\mathrm{Var}(y)=\\frac{\\sigma^2}{1-\\beta^2}.\n",
    "\\]\n",
    "\n",
    "As \\(\\beta\\to 1\\), the variance blows up like \\(\\approx \\sigma^2/(2(1-\\beta))\\).\n",
    "\n",
    "So momentum *does* average, but it also **stores** noise for a long time when \\(\\beta\\) is close to 1.  \n",
    "In late training, when gradients are small, this stored noise can dominate → higher variance “jitter” around the optimum.\n",
    "\n",
    "That’s the article’s “two phases” story:\n",
    "- **transient phase:** gradient signal ≫ noise → momentum helps\n",
    "- **stochastic phase:** noise ≫ gradient signal → momentum can hurt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0za5j27jh65",
   "metadata": {},
   "source": [
    "#### Prediction (stochastic gradient experiment)\n",
    "\n",
    "**Before running:**\n",
    "\n",
    "1. **Transient phase:** When the gradient signal is large, should momentum help or hurt?\n",
    "2. **Late phase:** When near the optimum (small gradients), should high $\\beta$ increase or decrease variance?\n",
    "3. Will the mean loss curve for momentum eventually plateau *higher* or *lower* than SGD?\n",
    "4. In the late-phase histogram, which method should have a wider spread?\n",
    "\n",
    "---\n",
    "\n",
    "**After running, check:** Do the two phases appear in the mean loss curves? Compare the late-phase distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33461f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: stochastic gradients on a quadratic (SGD vs SGD+momentum)\n",
    "\n",
    "def run_sgd_momentum(A, b, w0, alpha, beta, steps, noise_std, seed=None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    w = np.array(w0, dtype=float)\n",
    "    z = np.zeros_like(w)\n",
    "    traj = [w.copy()]\n",
    "    for _ in range(steps):\n",
    "        grad = A @ w - b\n",
    "        grad_noisy = grad + noise_std * rng.standard_normal(size=w.shape)\n",
    "        z = beta * z + grad_noisy\n",
    "        w = w - alpha * z\n",
    "        traj.append(w.copy())\n",
    "    return np.asarray(traj)\n",
    "\n",
    "# 2D quadratic\n",
    "lambda1, lambda2 = 0.1, 10.0\n",
    "A, Q = make_spd_quadratic([lambda1, lambda2], theta=np.pi/8)\n",
    "b = np.array([1.0, 1.0])\n",
    "w_star = quad_optimum(A, b)\n",
    "\n",
    "evals = np.linalg.eigvalsh(A)\n",
    "alpha_m, beta_m, _ = optimal_momentum(evals[0], evals[-1])\n",
    "\n",
    "# Make step-size smaller to handle noise\n",
    "alpha = 0.2 * alpha_m\n",
    "noise_std = 0.5\n",
    "steps = 250\n",
    "trials = 40\n",
    "\n",
    "def loss_centered(w):\n",
    "    e = w - w_star\n",
    "    return 0.5 * (e @ (A @ e))\n",
    "\n",
    "losses_sgd = []\n",
    "losses_mom = []\n",
    "\n",
    "for t in range(trials):\n",
    "    traj0 = run_sgd_momentum(A, b, np.zeros(2), alpha, 0.0, steps, noise_std, seed=t)\n",
    "    trajm = run_sgd_momentum(A, b, np.zeros(2), alpha, 0.9, steps, noise_std, seed=10_000+t)\n",
    "    losses_sgd.append([loss_centered(w) for w in traj0])\n",
    "    losses_mom.append([loss_centered(w) for w in trajm])\n",
    "\n",
    "losses_sgd = np.asarray(losses_sgd)\n",
    "losses_mom = np.asarray(losses_mom)\n",
    "\n",
    "mean_sgd = losses_sgd.mean(axis=0)\n",
    "mean_mom = losses_mom.mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.semilogy(mean_sgd, label=\"SGD (β=0) mean\")\n",
    "plt.semilogy(mean_mom, label=\"SGD+Momentum (β=0.9) mean\")\n",
    "plt.title(\"Stochastic optimization: mean loss curves\")\n",
    "plt.xlabel(\"iteration k\")\n",
    "plt.ylabel(r\"$\\mathbb{E}[f(w_k)-f(w^*)]$\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Late-phase distribution\n",
    "late = 50\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(losses_sgd[:, -late:].reshape(-1), bins=30, alpha=0.6, label=\"SGD\")\n",
    "plt.hist(losses_mom[:, -late:].reshape(-1), bins=30, alpha=0.6, label=\"Momentum\")\n",
    "plt.title(f\"Late-phase loss distribution (last {late} steps × {trials} trials)\")\n",
    "plt.xlabel(\"loss\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"alpha used: {alpha:.3e}, noise_std: {noise_std}, trials: {trials}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x3ut1xezqyd",
   "metadata": {},
   "source": [
    "#### Quick self-test (Stochastic Gradients)\n",
    "\n",
    "*Without scrolling up:*\n",
    "\n",
    "1. Model the noisy gradient: $g(w_k) = \\nabla f(w_k) + \\epsilon_k$. How does momentum transform this noise?\n",
    "2. For the AR(1) process $y_{k+1} = \\beta y_k + \\epsilon_k$, what is the stationary variance?\n",
    "3. Why does high $\\beta$ hurt in late training?\n",
    "4. What are the \"two phases\" of stochastic optimization with momentum?\n",
    "\n",
    "<details>\n",
    "<summary>Answers</summary>\n",
    "\n",
    "1. Noise is passed through a geometric filter $R^k$ where $R$ is the momentum iteration matrix.\n",
    "2. $\\text{Var}(y) = \\sigma^2/(1-\\beta^2)$\n",
    "3. As $\\beta \\to 1$, variance $\\approx \\sigma^2/(2(1-\\beta)) \\to \\infty$. Noise is stored for a long time.\n",
    "4. **Transient phase:** gradient signal dominates noise, momentum helps. **Stochastic phase:** noise dominates, momentum can hurt by amplifying variance.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656baed3",
   "metadata": {},
   "source": [
    "## Onwards and Downwards\n",
    "\n",
    "The article ends by pointing to other lenses on acceleration:\n",
    "- differential equations\n",
    "- polynomial approximation\n",
    "- geometry\n",
    "- duality\n",
    "\n",
    "Below is a compact “what to remember” + a self-test you can do from memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53325b09",
   "metadata": {},
   "source": [
    "#### Add-on: The 12-line compressed mental model\n",
    "\n",
    "1. Near any point, smooth objectives look like a quadratic: curvature \\(A\\) matters.\n",
    "2. Symmetric \\(A\\) diagonalizes → optimization decomposes into eigenmodes.\n",
    "3. GD on mode \\(\\lambda\\): \\(x_{k+1}=(1-\\alpha\\lambda)x_k\\).\n",
    "4. Large \\(\\lambda\\) modes die quickly; small \\(\\lambda\\) modes linger → “plateau.”\n",
    "5. Best fixed GD step uses endpoints \\(\\lambda_1,\\lambda_n\\): rate \\((\\kappa-1)/(\\kappa+1)\\).\n",
    "6. Momentum introduces a second state → 2nd-order recurrence per mode.\n",
    "7. That recurrence has roots \\(\\sigma_{1,2}\\); \\(|\\sigma|\\) controls speed, complex roots cause oscillations.\n",
    "8. Critical damping makes roots repeated → fastest monotone return for one mode.\n",
    "9. Globally tuned momentum makes worst-case rate \\((\\sqrt{\\kappa}-1)/(\\sqrt{\\kappa}+1)\\).\n",
    "10. Graph/Laplacian problems: slow modes = global structure; momentum accelerates diffusion.\n",
    "11. There’s a lower bound for linear first-order methods; momentum essentially hits it on worst cases.\n",
    "12. With noise, momentum can amplify variance late; it’s a transient-phase weapon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b4514",
   "metadata": {},
   "source": [
    "#### Add-on: Final self-test (close everything, do this on paper)\n",
    "\n",
    "1) Starting from \\(w_{k+1}=w_k-\\alpha(Aw_k-b)\\), derive \\(e_{k+1}=(I-\\alpha A)e_k\\).\n",
    "\n",
    "2) Show that in the eigenbasis, GD decouples: \\(x^{(i)}_{k+1}=(1-\\alpha\\lambda_i)x^{(i)}_k\\).\n",
    "\n",
    "3) For momentum, derive the 2nd-order recurrence:\n",
    "\\[\n",
    "x_{k+1} = (1+\\beta-\\alpha\\lambda)x_k - \\beta x_{k-1}.\n",
    "\\]\n",
    "\n",
    "4) From the characteristic polynomial, state the condition for oscillations (complex roots).\n",
    "\n",
    "5) Derive the critical damping \\(\\beta^\\star=(1-\\sqrt{\\alpha\\lambda})^2\\).\n",
    "\n",
    "6) State the optimal \\(\\alpha^\\star,\\beta^\\star\\) for \\([\\lambda_1,\\lambda_n]\\) and the rate in terms of \\(\\kappa\\).\n",
    "\n",
    "7) Explain the “light cone” lower bound in 2 sentences.\n",
    "\n",
    "8) In the noise-only model \\(y_{k+1}=\\beta y_k+\\epsilon_k\\), compute \\(\\mathrm{Var}(y)\\) at stationarity.\n",
    "\n",
    "If you can do all 8 without looking, you own the core.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db36325",
   "metadata": {},
   "source": [
    "### Acknowledgments\n",
    "\n",
    "See the original article for full acknowledgments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb85f05c",
   "metadata": {},
   "source": [
    "#### Discussion and Review\n",
    "\n",
    "The Distill page links reviewer notes and discussion threads. Skim these after you’ve done the derivations — they often reveal where people get confused.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9073251",
   "metadata": {},
   "source": [
    "### Footnotes\n",
    "\n",
    "If you’re going for “own it,” the footnotes are not optional — they contain important caveats (e.g., momentum can diverge on carefully constructed convex examples).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4f0834",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "A short “next stack” (also see the article’s full reference list):\n",
    "\n",
    "- Polyak (heavy-ball momentum)\n",
    "- Nesterov (accelerated methods + lower bounds)\n",
    "- Su, Boyd, Candès (ODE viewpoint of acceleration)\n",
    "- O’Donoghue & Candès (adaptive restart for acceleration)\n",
    "- Sutskever et al. (momentum in deep learning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebb02e",
   "metadata": {},
   "source": [
    "### Updates and Corrections\n",
    "\n",
    "Distill maintains a changelog for the article. If something here seems off, check whether the article was updated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5c86b7",
   "metadata": {},
   "source": [
    "### Citations and Reuse\n",
    "\n",
    "The Distill page provides citation info and licensing details (CC-BY for text/diagrams unless otherwise noted).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
